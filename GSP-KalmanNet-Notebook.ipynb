{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlzEwBDZKQpG",
        "outputId": "624b9a22-b3f2-4951-9fff-c60564b19a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting PYPOWER\n",
            "  Downloading PYPOWER-5.1.16-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.0/347.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PYPOWER\n",
            "Successfully installed PYPOWER-5.1.16\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import torch.nn.functional as func\n",
        "import time\n",
        "import numpy as np\n",
        "torch.pi = torch.acos(torch.zeros(1)).item() * 2  # which is 3.1415927410125732\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from torch import autograd\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# Aid libraries and function for the extraction of IEEE-118 admitance matrix\n",
        "!pip install PYPOWER\n",
        "# !apt install PYPOWER\n",
        "from pypower.api import case118, makeYbus, case14\n",
        "from pypower.idx_brch import F_BUS, T_BUS\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sXX96Jmx5Mc"
      },
      "source": [
        "# KalmanFilter #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tICIF3GdzwkT"
      },
      "outputs": [],
      "source": [
        "class KalmanFilter:\n",
        "\n",
        "    def __init__(self, SystemModel):\n",
        "        self.F = SystemModel.F;\n",
        "        self.F_T = torch.transpose(self.F, 0, 1);\n",
        "        self.m = SystemModel.m\n",
        "\n",
        "        self.Q = SystemModel.Q;\n",
        "\n",
        "        self.H = SystemModel.H;\n",
        "        self.H_T = torch.transpose(self.H, 0, 1);\n",
        "        self.n = SystemModel.n\n",
        "\n",
        "        self.R = SystemModel.R;\n",
        "\n",
        "        self.T = SystemModel.T;\n",
        "        self.T_test = SystemModel.T_test;\n",
        "\n",
        "    # Predict\n",
        "\n",
        "    def Predict(self):\n",
        "        # Predict the 1-st moment of x\n",
        "        self.m1x_prior = torch.matmul(self.F, self.m1x_posterior);\n",
        "\n",
        "        # Predict the 2-nd moment of x\n",
        "        self.m2x_prior = torch.matmul(self.F, self.m2x_posterior);\n",
        "        self.m2x_prior = torch.matmul(self.m2x_prior, self.F_T) + self.Q;\n",
        "\n",
        "        # Predict the 1-st moment of y\n",
        "        self.m1y = torch.matmul(self.H, self.m1x_prior);\n",
        "\n",
        "        # Predict the 2-nd moment of y\n",
        "        self.m2y = torch.matmul(self.H, self.m2x_prior);\n",
        "        self.m2y = torch.matmul(self.m2y, self.H_T) + self.R;\n",
        "\n",
        "    # Compute the Kalman Gain\n",
        "    def KGain(self):\n",
        "        self.KG = torch.matmul(self.m2x_prior, self.H_T)\n",
        "        self.KG = torch.matmul(self.KG, torch.inverse(self.m2y))\n",
        "\n",
        "    # Innovation\n",
        "    def Innovation(self, y):\n",
        "        self.dy = y - self.m1y;\n",
        "\n",
        "    # Compute Posterior\n",
        "    def Correct(self):\n",
        "        # Compute the 1-st posterior moment\n",
        "        self.m1x_posterior = self.m1x_prior + torch.matmul(self.KG, self.dy);\n",
        "\n",
        "        # Compute the 2-nd posterior moment\n",
        "        self.m2x_posterior = torch.matmul(self.m2y, torch.transpose(self.KG, 0, 1))\n",
        "        self.m2x_posterior = self.m2x_prior - torch.matmul(self.KG, self.m2x_posterior)\n",
        "\n",
        "    def Update(self, y):\n",
        "        self.Predict();\n",
        "        self.KGain();\n",
        "        self.Innovation(y);\n",
        "        self.Correct();\n",
        "\n",
        "        return self.m1x_posterior,self.m2x_posterior;\n",
        "\n",
        "    def InitSequence(self, m1x_0, m2x_0):\n",
        "        self.m1x_0 = m1x_0\n",
        "        self.m2x_0 = m2x_0\n",
        "\n",
        "        #########################\n",
        "\n",
        "    ### Generate Sequence ###\n",
        "    #########################\n",
        "    def GenerateSequence(self, y, T):\n",
        "        # Pre allocate an array for predicted state and variance\n",
        "        self.x = torch.empty(size=[self.m, T])\n",
        "        self.sigma = torch.empty(size=[self.m, self.m, T])\n",
        "\n",
        "        self.m1x_posterior = self.m1x_0\n",
        "        self.m2x_posterior = self.m2x_0\n",
        "\n",
        "        for t in range(0, T):\n",
        "            yt = torch.unsqueeze(y[:, t], 1);\n",
        "            xt,sigmat = self.Update(yt);\n",
        "            self.x[:, t] = torch.squeeze(xt)\n",
        "            self.sigma[:, :, t] = torch.squeeze(sigmat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60O4QIPxx8N6"
      },
      "source": [
        "# Linear SystemModel #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ku_YoKRBwBmT"
      },
      "outputs": [],
      "source": [
        "class SystemModel_lin:\n",
        "\n",
        "    def __init__(self, F, q, H, r, T, T_test, outlier_p=0,rayleigh_sigma=10000):\n",
        "\n",
        "        self.outlier_p = outlier_p\n",
        "        self.rayleigh_sigma = rayleigh_sigma\n",
        "        ####################\n",
        "        ### Motion Model ###\n",
        "        ####################\n",
        "        self.F = F\n",
        "        self.m = self.F.size()[0]\n",
        "\n",
        "        self.q = q\n",
        "        self.Q = q * q * torch.eye(self.m)\n",
        "\n",
        "        #########################\n",
        "        ### Observation Model ###\n",
        "        #########################\n",
        "        self.H = H\n",
        "        self.n = self.H.size()[0]\n",
        "\n",
        "        self.r = r\n",
        "        self.R = r * r * torch.eye(self.n)\n",
        "\n",
        "        #Assign T and T_test\n",
        "        self.T = T\n",
        "        self.T_test = T_test\n",
        "\n",
        "    #####################\n",
        "    ### Init Sequence ###\n",
        "    #####################\n",
        "    def InitSequence(self, m1x_0, m2x_0):\n",
        "\n",
        "        self.m1x_0 = m1x_0\n",
        "        self.m2x_0 = m2x_0\n",
        "\n",
        "\n",
        "    #########################\n",
        "    ### Update Covariance ###\n",
        "    #########################\n",
        "    def UpdateCovariance_Gain(self, q, r):\n",
        "\n",
        "        self.q = q\n",
        "        self.Q = q * q * torch.eye(self.m)\n",
        "\n",
        "        self.r = r\n",
        "        self.R = r * r * torch.eye(self.n)\n",
        "\n",
        "    def UpdateCovariance_Matrix(self, Q, R):\n",
        "\n",
        "        self.Q = Q\n",
        "\n",
        "        self.R = R\n",
        "\n",
        "\n",
        "    #########################\n",
        "    ### Generate Sequence ###\n",
        "    #########################\n",
        "    def GenerateSequence(self, Q_gen, R_gen, T):\n",
        "        # Pre allocate an array for current state\n",
        "        self.x = torch.empty(size=[self.m, T])\n",
        "        # Pre allocate an array for current observation\n",
        "        self.y = torch.empty(size=[self.n, T])\n",
        "        # Set x0 to be x previous\n",
        "        self.x_prev = self.m1x_0\n",
        "\n",
        "        # Outliers\n",
        "        if self.outlier_p > 0:\n",
        "            b_matrix = torch.bernoulli(self.outlier_p *torch.ones(T))\n",
        "\n",
        "        # Generate Sequence Iteratively\n",
        "        for t in range(0, T):\n",
        "            ########################\n",
        "            #### State Evolution ###\n",
        "            ########################\n",
        "            # Process Noise\n",
        "            if self.q == 0:\n",
        "                xt = self.F.matmul(self.x_prev)\n",
        "            else:\n",
        "                xt = self.F.matmul(self.x_prev)\n",
        "                mean = torch.zeros([self.m])\n",
        "                distrib = MultivariateNormal(loc=mean, covariance_matrix=Q_gen)\n",
        "                eq = distrib.rsample()\n",
        "                # eq = torch.normal(mean, self.q)\n",
        "                eq = torch.reshape(eq[:],[self.m,1])\n",
        "                # Additive Process Noise\n",
        "                xt = torch.add(xt,eq)\n",
        "\n",
        "            ################\n",
        "            ### Emission ###\n",
        "            ################\n",
        "            # Observation Noise\n",
        "            if self.r == 0:\n",
        "                yt = self.H.matmul(xt)\n",
        "            else:\n",
        "                yt = self.H.matmul(xt)\n",
        "                mean = torch.zeros([self.n])\n",
        "                distrib = MultivariateNormal(loc=mean, covariance_matrix=R_gen)\n",
        "                er = distrib.rsample()\n",
        "                er = torch.reshape(er[:],[self.n,1])\n",
        "                # mean = torch.zeros([self.n,1])\n",
        "                # er = torch.normal(mean, self.r)\n",
        "\n",
        "                # Additive Observation Noise\n",
        "                yt = torch.add(yt,er)\n",
        "\n",
        "            # Outliers\n",
        "            if self.outlier_p > 0:\n",
        "                if b_matrix[t] != 0:\n",
        "                    btdt = self.rayleigh_sigma*torch.sqrt(-2*torch.log(torch.rand(self.n,1)))\n",
        "                    yt = torch.add(yt,btdt)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            ########################\n",
        "            ### Squeeze to Array ###\n",
        "            ########################\n",
        "\n",
        "            # Save Current State to Trajectory Array\n",
        "            self.x[:, t] = torch.squeeze(xt)\n",
        "\n",
        "            # Save Current Observation to Trajectory Array\n",
        "            self.y[:, t] = torch.squeeze(yt)\n",
        "\n",
        "            ################################\n",
        "            ### Save Current to Previous ###\n",
        "            ################################\n",
        "            self.x_prev = xt\n",
        "\n",
        "    ######################\n",
        "    ### Generate Batch ###\n",
        "    ######################\n",
        "\n",
        "    def GenerateBatch(self, size, T, randomInit=False, seqInit=False, T_test=0):\n",
        "\n",
        "        # Allocate Empty Array for Input\n",
        "        self.Input = torch.empty(size, self.n, T)\n",
        "\n",
        "        # Allocate Empty Array for Target\n",
        "        self.Target = torch.empty(size, self.m, T)\n",
        "\n",
        "        ### Generate Examples\n",
        "        initConditions = self.m1x_0\n",
        "\n",
        "        for i in range(0, size):\n",
        "            # Generate Sequence\n",
        "\n",
        "            # Randomize initial conditions to get a rich dataset\n",
        "            if(randomInit):\n",
        "                variance = 100\n",
        "                initConditions = torch.rand_like(self.m1x_0) * variance\n",
        "            if(seqInit):\n",
        "                initConditions = self.x_prev\n",
        "                if((i*T % T_test)==0):\n",
        "                    initConditions = torch.zeros_like(self.m1x_0)\n",
        "\n",
        "            self.InitSequence(initConditions, self.m2x_0)\n",
        "            self.GenerateSequence(self.Q, self.R, T)\n",
        "\n",
        "            # Training sequence input\n",
        "            self.Input[i, :, :] = self.y\n",
        "\n",
        "            # Training sequence output\n",
        "            self.Target[i, :, :] = self.x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoWDzmEnyAjy"
      },
      "source": [
        "# KFTest #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "S_9rD_FLvtHY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def KFTest(SysModel, test_input, test_target):\n",
        "    N_T = test_target.size()[0]\n",
        "    print(\"N_T = \", N_T)\n",
        "\n",
        "    # LOSS\n",
        "    loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    # MSE [Linear]\n",
        "    MSE_KF_linear_arr = torch.empty(N_T)\n",
        "    MSE_per_iter = torch.zeros(SysModel.T)\n",
        "    MSE_finalize = torch.zeros(SysModel.T)\n",
        "\n",
        "    start = time.time()\n",
        "    KF = KalmanFilter(SysModel)\n",
        "    KF.InitSequence(SysModel.m1x_0, SysModel.m2x_0)\n",
        "\n",
        "    for j in range(0, N_T):\n",
        "        KF.GenerateSequence(test_input[j, :, :], KF.T_test)\n",
        "        for t in range(SysModel.T):\n",
        "          MSE_per_iter[t] = loss_fn(KF.x[:,t], test_target[j, :, t])\n",
        "        MSE_KF_linear_arr[j] = loss_fn(KF.x, test_target[j, :, :]).item()\n",
        "        #MSE_KF_linear_arr[j] = loss_fn(test_input[j, :, :], test_target[j, :, :]).item()\n",
        "        MSE_finalize += MSE_per_iter\n",
        "    end = time.time()\n",
        "    t = end - start\n",
        "\n",
        "    MSE_KF_linear_avg = torch.mean(MSE_KF_linear_arr)\n",
        "    MSE_KF_dB_avg = 10 * torch.log10(MSE_KF_linear_avg)\n",
        "\n",
        "    # Standard deviation\n",
        "    MSE_KF_dB_std = torch.std(MSE_KF_linear_arr, unbiased=True)\n",
        "    MSE_KF_dB_std = 10 * torch.log10(MSE_KF_dB_std)\n",
        "\n",
        "    MSE_finalize = MSE_finalize / N_T # average\n",
        "    MSE_finalize_dB = 10 * torch.log10(MSE_finalize)\n",
        "\n",
        "    print(\"Kalman Filter - MSE LOSS:\", MSE_KF_dB_avg, \"[dB]\")\n",
        "    print(\"EKF - MSE STD:\", MSE_KF_dB_std, \"[dB]\")\n",
        "    # Print Run Time\n",
        "    print(\"Inference Time:\", t)\n",
        "\n",
        "    return [MSE_finalize_dB, MSE_KF_linear_arr, MSE_KF_linear_avg, MSE_KF_dB_avg]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt__wIJyyFAv"
      },
      "source": [
        "# Extended Sysmodel #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Y9niB6BUD-FK"
      },
      "outputs": [],
      "source": [
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "\n",
        "# from filing_paths import path_model\n",
        "# import sys\n",
        "\n",
        "# sys.path.insert(1, path_model)\n",
        "# from parameters import delta_t, delta_t_gen, variance\n",
        "\n",
        "\n",
        "class SystemModel:\n",
        "\n",
        "    def __init__(self, f, q, h, r, T, T_test, m, n, L, V, V_t, modelname='lor'):\n",
        "\n",
        "        ####################\n",
        "        ### Motion Model ###\n",
        "        ####################\n",
        "        self.L = L\n",
        "        self.V = V\n",
        "        self.V_t = V_t\n",
        "        self.modelname = modelname\n",
        "\n",
        "        self.f = f\n",
        "        self.m = m\n",
        "        print(\"m,n = \",m,n)\n",
        "        self.q = q\n",
        "        self.Q = q * q * torch.eye(self.m)\n",
        "\n",
        "        #########################\n",
        "        ### Observation Model ###\n",
        "        #########################\n",
        "        self.h = h\n",
        "        self.n = n\n",
        "\n",
        "        self.r = r\n",
        "        self.R = r * r * torch.eye(self.n)\n",
        "\n",
        "        # Assign T and T_test\n",
        "        self.T = T\n",
        "        self.T_test = T_test\n",
        "\n",
        "    #####################\n",
        "    ### Init Sequence ###\n",
        "    #####################\n",
        "    def InitSequence(self, m1x_0, m2x_0):\n",
        "\n",
        "        self.m1x_0 = torch.squeeze(m1x_0)\n",
        "        self.m2x_0 = torch.squeeze(m2x_0)\n",
        "\n",
        "    #########################\n",
        "    ### Update Covariance ###\n",
        "    #########################\n",
        "    def UpdateCovariance_Gain(self, q, r):\n",
        "\n",
        "        self.q = q\n",
        "        self.Q = q * q * torch.eye(self.m)\n",
        "\n",
        "        self.r = r\n",
        "        self.R = r * r * torch.eye(self.n)\n",
        "\n",
        "    def UpdateCovariance_Matrix(self, Q, R):\n",
        "\n",
        "        self.Q = Q\n",
        "\n",
        "        self.R = R\n",
        "\n",
        "    #########################\n",
        "    ### Generate Sequence ###\n",
        "    #########################\n",
        "    def GenerateSequence(self, Q_gen, R_gen, T):\n",
        "        # Pre allocate an array for current state\n",
        "        self.x = torch.empty(size=[self.m, T])\n",
        "        # Pre allocate an array for current observation\n",
        "        self.y = torch.empty(size=[self.n, T])\n",
        "        # Set x0 to be x previous\n",
        "        self.x_prev = self.m1x_0\n",
        "\n",
        "        # Generate Sequence Iteratively\n",
        "        for t in range(0, T):\n",
        "            ########################\n",
        "            #### State Evolution ###\n",
        "            ########################\n",
        "            # Process Noise\n",
        "            if self.q == 0:\n",
        "                xt = self.f(self.x_prev)\n",
        "            else:\n",
        "                xt = self.f(self.x_prev)\n",
        "                mean = torch.zeros([self.m])\n",
        "                if self.modelname == \"pendulum\":\n",
        "                    distrib = MultivariateNormal(loc=mean, covariance_matrix=Q_gen)\n",
        "                    eq = distrib.rsample()\n",
        "                else:\n",
        "                    eq = torch.normal(mean, self.q)\n",
        "\n",
        "                # Additive Process Noise\n",
        "                xt = torch.add(xt, eq)\n",
        "\n",
        "            ################\n",
        "            ### Emission ###\n",
        "            ################\n",
        "            yt = self.h(xt)\n",
        "\n",
        "            # Observation Noise\n",
        "            mean = torch.zeros([self.n])\n",
        "            er = torch.normal(mean, self.r)\n",
        "            # er = np.random.multivariate_normal(mean, R_gen, 1)\n",
        "            # er = torch.transpose(torch.tensor(er), 0, 1)\n",
        "\n",
        "            # Additive Observation Noise\n",
        "            yt = torch.add(yt, er)\n",
        "\n",
        "            ########################\n",
        "            ### Squeeze to Array ###\n",
        "            ########################\n",
        "\n",
        "            # Save Current State to Trajectory Array\n",
        "            self.x[:, t] = torch.squeeze(xt)\n",
        "\n",
        "            # Save Current Observation to Trajectory Array\n",
        "            self.y[:, t] = torch.squeeze(yt)\n",
        "\n",
        "            ################################\n",
        "            ### Save Current to Previous ###\n",
        "            ################################\n",
        "            self.x_prev = xt\n",
        "\n",
        "    ######################\n",
        "    ### Generate Batch ###\n",
        "    ######################\n",
        "    def GenerateBatch(self, size, T, randomInit=False):\n",
        "\n",
        "        # Allocate Empty Array for Input\n",
        "        self.Input = torch.empty(size, self.n, T)\n",
        "\n",
        "        # Allocate Empty Array for Target\n",
        "        self.Target = torch.empty(size, self.m, T)\n",
        "\n",
        "        initConditions = self.m1x_0\n",
        "\n",
        "        ### Generate Examples\n",
        "        for i in range(0, size):\n",
        "            # Generate Sequence\n",
        "            # Randomize initial conditions to get a rich dataset\n",
        "            if (randomInit):\n",
        "                initConditions = torch.rand_like(self.m1x_0) * variance\n",
        "            self.InitSequence(initConditions, self.m2x_0)\n",
        "            self.GenerateSequence(self.Q, self.R, T)\n",
        "\n",
        "            # Training sequence input\n",
        "            self.Input[i, :, :] = self.y\n",
        "\n",
        "            # Training sequence output\n",
        "            self.Target[i, :, :] = self.x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-5ZkvqYyJaQ"
      },
      "source": [
        "# GSP-KalmanNet #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mNexVbpAJ0Z5"
      },
      "outputs": [],
      "source": [
        "class GSPKalmanNetNN(torch.nn.Module):\n",
        "\n",
        "    ###################\n",
        "    ### Constructor ###\n",
        "    ###################\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #############\n",
        "    ### Build ###\n",
        "    #############\n",
        "    def Build(self, ssModel):\n",
        "        self.V = ssModel.V\n",
        "        self.V_t = ssModel.V_t\n",
        "        self.InitSystemDynamics(ssModel.F, ssModel.H)\n",
        "        # Number of neurons in the 1st hidden layer\n",
        "        H1_KNet = (ssModel.m + ssModel.n) * 10 * 8     # Here we can reduce the latent space\n",
        "\n",
        "        # Number of neurons in the 2nd hidden layer\n",
        "        H2_KNet = (ssModel.m * ssModel.n) * 1 * (4)      # Here we can reduce the latent space\n",
        "\n",
        "        self.InitKGainNet(H1_KNet, H2_KNet)\n",
        "\n",
        "    ######################################\n",
        "    ### Initialize Kalman Gain Network ###\n",
        "    ######################################\n",
        "    def InitKGainNet(self, H1, H2):\n",
        "        # Input Dimensions\n",
        "        D_in = self.m + self.n  # x(t-1), y(t)\n",
        "\n",
        "        # Output Dimensions\n",
        "        D_out = self.n  # Diagonal Kalman Gain ## GSP update 8.3\n",
        "\n",
        "        ###################\n",
        "        ### Input Layer ###\n",
        "        ###################\n",
        "        # Linear Layer\n",
        "        self.KG_l1 = torch.nn.Linear(D_in, H1, bias=True)\n",
        "\n",
        "        # ReLU (Rectified Linear Unit) Activation Function\n",
        "        self.KG_relu1 = torch.nn.ReLU()\n",
        "\n",
        "        ###########\n",
        "        ### GRU ###\n",
        "        ###########\n",
        "        # Input Dimension\n",
        "        self.input_dim = H1\n",
        "        # Hidden Dimension\n",
        "        self.hidden_dim = (self.m * self.m + self.n * self.n) * 10\n",
        "        # Number of Layers\n",
        "        self.n_layers = 1\n",
        "        # Batch Size\n",
        "        self.batch_size = 1\n",
        "        # Input Sequence Length\n",
        "        self.seq_len_input = 1\n",
        "        # Hidden Sequence Length\n",
        "        self.seq_len_hidden = self.n_layers\n",
        "\n",
        "        # batch_first = False\n",
        "        # dropout = 0.1 ;\n",
        "\n",
        "        # Initialize a Tensor for GRU Input\n",
        "        # self.GRU_in = torch.empty(self.seq_len_input, self.batch_size, self.input_dim)\n",
        "\n",
        "        # Initialize a Tensor for Hidden State\n",
        "        self.hn = torch.randn(self.seq_len_hidden, self.batch_size, self.hidden_dim).to(self.device,non_blocking = True)\n",
        "\n",
        "        # Iniatialize GRU Layer\n",
        "        self.rnn_GRU = nn.GRU(self.input_dim, self.hidden_dim, self.n_layers)\n",
        "\n",
        "        ####################\n",
        "        ### Hidden Layer ###\n",
        "        ####################\n",
        "        self.KG_l2 = torch.nn.Linear(self.hidden_dim, H2, bias=True)\n",
        "\n",
        "        # ReLU (Rectified Linear Unit) Activation Function\n",
        "        self.KG_relu2 = torch.nn.ReLU()\n",
        "\n",
        "        ####################\n",
        "        ### Output Layer ###\n",
        "        ####################\n",
        "        self.KG_l3 = torch.nn.Linear(H2, D_out, bias=True)\n",
        "\n",
        "    ##################################\n",
        "    ### Initialize System Dynamics ###\n",
        "    ##################################\n",
        "    def InitSystemDynamics(self, F, H):\n",
        "        # Set State Evolution Matrix\n",
        "        self.F = self.GFT_matrix(F.to(self.device,non_blocking = True))\n",
        "        self.F_T = torch.transpose(F, 0, 1)\n",
        "        self.m = self.F.size()[0]\n",
        "\n",
        "        # Set Observation Matrix\n",
        "        self.H = self.GFT_matrix(H.to(self.device,non_blocking = True))\n",
        "        self.H_T = torch.transpose(H, 0, 1)\n",
        "        self.n = self.H.size()[0]\n",
        "\n",
        "    ###########################\n",
        "    ### Initialize Sequence ###\n",
        "    ###########################\n",
        "    # def InitSequence(self, M1_0):\n",
        "    #     M1_0 = self.GFT(M1_0)\n",
        "    #     self.m1x_prior = M1_0.to(self.device,non_blocking = True)\n",
        "\n",
        "    #     self.m1x_posterior = M1_0.to(self.device,non_blocking = True)\n",
        "\n",
        "    #     self.state_process_posterior_0 = M1_0.to(self.device,non_blocking = True)\n",
        "\n",
        "    def InitSequence(self, M1_0, T):\n",
        "        M1_0 = self.GFT(M1_0.squeeze())\n",
        "\n",
        "        self.m1x_posterior = torch.squeeze(M1_0).to(dev)\n",
        "        self.m1x_posterior_previous = 0  # for t=0\n",
        "\n",
        "        self.T = T\n",
        "        self.x_out = torch.empty(self.m, T)\n",
        "\n",
        "        self.m1x_prior = M1_0.to(self.device,non_blocking = True)\n",
        "        self.state_process_posterior_0 = torch.squeeze(M1_0).to(dev)\n",
        "        self.m1x_prior_previous = self.m1x_posterior\n",
        "\n",
        "        # KGain saving\n",
        "        self.i = 0\n",
        "        self.KGain_array = self.KG_array = torch.zeros((self.T*10, self.m, self.n)).to(dev)\n",
        "\n",
        "    ######################\n",
        "    ### Compute Priors ###\n",
        "    ######################\n",
        "    def step_prior(self):\n",
        "        # Compute the 1-st moment of x based on model knowledge and without process noise\n",
        "        bmm_mul = torch.bmm(self.F.expand(self.state_process_posterior_0.size()[0], -1, -1).type(torch.DoubleTensor).to(dev), self.state_process_posterior_0.unsqueeze(-1).type(torch.DoubleTensor).to(dev)).squeeze(-1)\n",
        "        self.state_process_prior_0 = bmm_mul\n",
        "\n",
        "        # Compute the 1-st moment of y based on model knowledge and without noise\n",
        "        bmm_mul = torch.bmm(self.H.expand(self.state_process_prior_0.size()[0], -1, -1).type(torch.DoubleTensor).to(dev), self.state_process_prior_0.unsqueeze(-1).type(torch.DoubleTensor).to(dev)).squeeze(-1)\n",
        "        self.obs_process_0 = bmm_mul\n",
        "\n",
        "        # Predict the 1-st moment of x\n",
        "        self.m1x_prev_prior = self.m1x_prior.squeeze()\n",
        "        bmm_mul = torch.bmm(self.F.expand(self.m1x_posterior.size()[0], -1, -1).type(torch.DoubleTensor).to(dev), self.m1x_posterior.unsqueeze(-1).type(torch.DoubleTensor).to(dev)).squeeze(-1)\n",
        "        self.m1x_prior = bmm_mul\n",
        "\n",
        "        # Predict the 1-st moment of y\n",
        "        bmm_mul = torch.bmm(self.H.expand(self.m1x_prior.size()[0], -1, -1).type(torch.DoubleTensor).to(dev), self.m1x_prior.unsqueeze(-1).type(torch.DoubleTensor).to(dev)).squeeze(-1)\n",
        "        self.m1y = bmm_mul\n",
        "\n",
        "\n",
        "    ##############################\n",
        "    ### Kalman Gain Estimation ###\n",
        "    ##############################\n",
        "    def step_KGain_est(self, y):\n",
        "\n",
        "      # Reshape and Normalize the difference in X prior\n",
        "      # Featture 4: x_t|t - x_t|t-1\n",
        "      #dm1x = self.m1x_prior - self.state_process_prior_0\n",
        "      dm1x = self.m1x_posterior - self.m1x_prev_prior\n",
        "      dm1x_reshape = torch.squeeze(dm1x)\n",
        "      dm1x_norm = func.normalize(dm1x_reshape, p=2, dim=0, eps=1e-12, out=None)\n",
        "\n",
        "      # Feature 2: yt - y_t+1|t\n",
        "      dm1y = y.squeeze() - torch.squeeze(self.m1y)\n",
        "      dm1y_norm = func.normalize(dm1y, p=2, dim=0, eps=1e-12, out=None)\n",
        "\n",
        "      # KGain Net Input\n",
        "      KGainNet_in = torch.cat([dm1y_norm, dm1x_norm], dim=1)\n",
        "      # Kalman Gain Network Step\n",
        "      KG = self.KGain_step(KGainNet_in)\n",
        "      # Reshape Kalman Gain to a Matrix\n",
        "      KG = torch.diag_embed(KG)\n",
        "      self.KGain = torch.reshape(KG, (-1,self.m, self.n))\n",
        "\n",
        "    #######################\n",
        "    ### Kalman Net Step ###\n",
        "    #######################\n",
        "    def KNet_step(self, y):\n",
        "        # Compute Priors\n",
        "        self.step_prior()\n",
        "\n",
        "        # Compute Kalman Gain\n",
        "        self.step_KGain_est(y)\n",
        "\n",
        "        # Innovation\n",
        "        y_obs = torch.squeeze(y)\n",
        "        dy = y_obs - self.m1y\n",
        "        # Compute the 1-st posterior moment\n",
        "        # bmm_mul = torch.bmm(self.F.expand(self.state_process_posterior_0.size()[0], -1, -1).type(torch.DoubleTensor).to(dev), self.state_process_posterior_0.unsqueeze(-1).type(torch.DoubleTensor).to(dev)).squeeze(-1)\n",
        "        INOV = torch.matmul(self.KGain.float(), dy.unsqueeze(-1).float()).squeeze()\n",
        "        self.m1x_posterior = self.m1x_prior + INOV\n",
        "\n",
        "        # return\n",
        "        return torch.squeeze(self.m1x_posterior)\n",
        "\n",
        "    ########################\n",
        "    ### Kalman Gain Step ###\n",
        "    ########################\n",
        "    def KGain_step(self, KGainNet_in):\n",
        "\n",
        "        ###################\n",
        "        ### Input Layer ###\n",
        "        ###################\n",
        "        L1_out = self.KG_l1(KGainNet_in.type(torch.FloatTensor).to(dev))\n",
        "        La1_out = self.KG_relu1(L1_out)\n",
        "\n",
        "        ###########\n",
        "        ### GRU ###\n",
        "        ###########\n",
        "        GRU_in = torch.empty(self.seq_len_input, self.batch_size, self.input_dim).to(self.device,non_blocking = True)\n",
        "        GRU_in[0, :, :] = La1_out\n",
        "        GRU_out, self.hn = self.rnn_GRU(GRU_in, self.hn)\n",
        "        # GRU_out_reshape = torch.reshape(GRU_out, (1, self.hidden_dim))\n",
        "\n",
        "        ####################\n",
        "        ### Hidden Layer ###\n",
        "        ####################\n",
        "        L2_out = self.KG_l2(GRU_out)\n",
        "        La2_out = self.KG_relu2(L2_out)\n",
        "\n",
        "        ####################\n",
        "        ### Output Layer ###\n",
        "        ####################\n",
        "        L3_out = self.KG_l3(La2_out)\n",
        "        return L3_out\n",
        "\n",
        "    ###############\n",
        "    ### Forward ###\n",
        "    ###############\n",
        "    def forward(self, yt):\n",
        "        yt = self.GFT(yt.squeeze())\n",
        "        yt = yt.to(self.device,non_blocking = True)\n",
        "        return self.IGFT(self.KNet_step(yt))\n",
        "\n",
        "    #########################\n",
        "    ### Init Hidden State ###\n",
        "    #########################\n",
        "    def init_hidden(self):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, self.batch_size, self.hidden_dim).zero_()\n",
        "        self.hn = hidden.data\n",
        "\n",
        "    def GFT(self, input):\n",
        "        # return torch.matmul(self.V_t, input.to(dev)).type(torch.FloatTensor)\n",
        "        return self.BMM_multipy(self.V_t,input.to(dev)).type(torch.FloatTensor)\n",
        "\n",
        "    def IGFT(self, input):\n",
        "        # return torch.matmul(self.V, input.to(dev)).type(torch.FloatTensor)\n",
        "        return self.BMM_multipy(self.V,input.to(dev)).type(torch.FloatTensor)\n",
        "\n",
        "    def GFT_matrix(self, input):\n",
        "        return torch.matmul(torch.matmul(self.V_t, input.to(dev)), self.V).type(torch.FloatTensor)\n",
        "        # return self.BMM_multipy(self.V_t,input.to(dev)).type(torch.FloatTensor)\n",
        "\n",
        "    def IGFT_matrix(self, input):\n",
        "        return torch.matmul(torch.matmul(self.V, input.to(dev)), self.V).type(torch.FloatTensor)\n",
        "\n",
        "    def BMM_multipy(self,a,b):\n",
        "        try:\n",
        "          return torch.bmm(a.expand(b.size()[0], -1, -1).type(torch.DoubleTensor).to(dev), b.unsqueeze(-1).type(torch.DoubleTensor).to(dev)).squeeze(-1)\n",
        "        except:\n",
        "          print(a.shape,b.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DQDye1EUiyV"
      },
      "source": [
        "# **Ex Kalmannet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lT-WWQpWUjUM"
      },
      "outputs": [],
      "source": [
        "\n",
        "nGRU = 2\n",
        "class ExtendedKalmanNetNN(torch.nn.Module):\n",
        "\n",
        "    ###################\n",
        "    ### Constructor ###\n",
        "    ###################\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    ######################################\n",
        "    ### Initialize Kalman Gain Network ###\n",
        "    ######################################\n",
        "\n",
        "    def Build(self, ssModel, infoString='fullInfo'):\n",
        "\n",
        "        self.InitSystemDynamics(ssModel.f, ssModel.h, ssModel.m, ssModel.n, infoString='fullInfo')\n",
        "        self.InitSequence(ssModel.m1x_0, ssModel.T)\n",
        "\n",
        "        # Number of neurons in the 1st hidden layer\n",
        "        # H1_KNet = (ssModel.m + ssModel.n) * (10) * 8\n",
        "        H1_KNet = (ssModel.m + ssModel.n) * 8\n",
        "        # Number of neurons in the 2nd hidden layer\n",
        "        H2_KNet = (ssModel.m * ssModel.n)  * 1 * 2\n",
        "\n",
        "        self.InitKGainNet(H1_KNet, H2_KNet)\n",
        "\n",
        "    def InitKGainNet(self, H1, H2):\n",
        "        # Input Dimensions (+1 for time input)\n",
        "        D_in = self.m + self.m + self.n  # F1,3,4\n",
        "        # Output Dimensions\n",
        "        D_out = self.m * self.n  # Kalman Gain\n",
        "\n",
        "        ###################\n",
        "        ### Input Layer ###\n",
        "        ###################\n",
        "        # Linear Layer\n",
        "        self.KG_l1 = torch.nn.Linear(D_in, H1, bias=True) #.type(torch.DoubleTensor)\n",
        "        # ReLU (Rectified Linear Unit) Activation Function\n",
        "        self.KG_relu1 = torch.nn.ReLU()\n",
        "        ###########\n",
        "        ### GRU ###\n",
        "        ###########\n",
        "\n",
        "        # Input Dimension\n",
        "        self.input_dim = H1\n",
        "        # Hidden Dimension\n",
        "        self.hidden_dim = ((self.n * self.n) + (self.m * self.m)) * 8\n",
        "        # Number of Layers\n",
        "        self.n_layers = nGRU\n",
        "        # Batch Size\n",
        "        self.batch_size = 1\n",
        "        # Input Sequence Length\n",
        "        self.seq_len_input = 1\n",
        "        # Hidden Sequence Length\n",
        "        self.seq_len_hidden = self.n_layers\n",
        "\n",
        "        # batch_first = False\n",
        "        # dropout = 0.1 ;\n",
        "\n",
        "        # Initialize a Tensor for GRU Input\n",
        "        # self.GRU_in = torch.empty(self.seq_len_input, self.batch_size, self.input_dim)\n",
        "\n",
        "        # Initialize a Tensor for Hidden State\n",
        "        self.hn = torch.randn(self.seq_len_hidden, self.batch_size, self.hidden_dim)\n",
        "\n",
        "        # Iniatialize GRU Layer\n",
        "        self.rnn_GRU = nn.GRU(self.input_dim, self.hidden_dim, self.n_layers, dropout=0)\n",
        "\n",
        "        ####################\n",
        "        ### Hidden Layer ###\n",
        "        ####################\n",
        "        self.KG_l2 = torch.nn.Linear(self.hidden_dim, H2, bias=True)\n",
        "\n",
        "        # ReLU (Rectified Linear Unit) Activation Function\n",
        "        self.KG_relu2 = torch.nn.ReLU()\n",
        "\n",
        "        ####################\n",
        "        ### Output Layer ###\n",
        "        ####################\n",
        "        self.KG_l3 = torch.nn.Linear(H2, D_out, bias=True)\n",
        "    ##################################\n",
        "    ### Initialize System Dynamics ###\n",
        "    ##################################\n",
        "    def InitSystemDynamics(self, f, h, m, n, infoString='fullInfo'):\n",
        "\n",
        "        if (infoString == 'partialInfo'):\n",
        "            self.fString = 'ModInacc'\n",
        "            self.hString = 'ObsInacc'\n",
        "        else:\n",
        "            self.fString = 'ModAcc'\n",
        "            self.hString = 'ObsAcc'\n",
        "\n",
        "        # Set State Evolution Function\n",
        "        self.f = f\n",
        "        self.m = m\n",
        "\n",
        "        # Set Observation Function\n",
        "        self.h = h\n",
        "        self.n = n\n",
        "\n",
        "    ###########################\n",
        "    ### Initialize Sequence ###\n",
        "    ###########################\n",
        "    def InitSequence(self, M1_0, T):\n",
        "\n",
        "        self.m1x_posterior = torch.squeeze(M1_0).to(dev)\n",
        "        self.m1x_posterior_previous = 0  # for t=0\n",
        "\n",
        "        self.T = T\n",
        "        self.x_out = torch.empty(self.m, T)\n",
        "\n",
        "        self.state_process_posterior_0 = torch.squeeze(M1_0).to(dev)\n",
        "        self.m1x_prior_previous = self.m1x_posterior\n",
        "\n",
        "        # KGain saving\n",
        "        self.i = 0\n",
        "        self.KGain_array = self.KG_array = torch.zeros((self.T*1000, self.m, self.n)).to(dev)\n",
        "\n",
        "    ######################\n",
        "    ### Compute Priors ###\n",
        "    ######################\n",
        "    def step_prior(self):\n",
        "        # Predict the 1-st moment of x\n",
        "        self.m1x_prior = torch.squeeze(self.f(self.m1x_posterior))\n",
        "\n",
        "        # Predict the 1-st moment of y\n",
        "        self.m1y = self.h(self.m1x_prior)\n",
        "\n",
        "        # Update Jacobians\n",
        "        # self.JFt = get_Jacobian(self.m1x_posterior, self.fString)\n",
        "        # self.JHt = get_Jacobian(self.m1x_prior, self.hString)\n",
        "\n",
        "        self.state_process_prior_0 = torch.squeeze(self.f(self.state_process_posterior_0))\n",
        "        self.obs_process_0 = torch.unsqueeze(self.h(self.state_process_prior_0),-1)\n",
        "\n",
        "    ##############################\n",
        "    ### Kalman Gain Estimation ###\n",
        "    ##############################\n",
        "    def step_KGain_est(self, y):\n",
        "        # Feature 1: yt - yt-1\n",
        "        try:\n",
        "            my_f1_0 = y - torch.squeeze(self.y_previous.to(dev)).to(dev)\n",
        "        except:\n",
        "            # print(torch.squeeze(self.obs_process_0).shape)\n",
        "            my_f1_0 = y - self.obs_process_0.to(dev).to(dev)  # when t=0\n",
        "        # my_f1_reshape = torch.squeeze(my_f1_0)\n",
        "        y_f1_norm = func.normalize(my_f1_0, p=2, dim=0, eps=1e-12, out=None)\n",
        "\n",
        "        # Feature 2: yt - y_t+1|t\n",
        "        # my_f2_0 = y - torch.squeeze(self.m1y)\n",
        "        # my_f2_reshape = torch.squeeze(my_f2_0)\n",
        "        # y_f2_norm = func.normalize(my_f2_reshape, p=2, dim=0, eps=1e-12, out=None)\n",
        "        # print(\"1\",self.m1x_posterior)\n",
        "        # print(\"2\",self.m1x_posterior_previous)\n",
        "        # print(\"3\",self.m1x_prior_previous)\n",
        "        # Feature 3: x_t|t - x_t-1|t-1\n",
        "        self.m1x_posterior = self.m1x_posterior.reshape(-1,14,1)\n",
        "        # self.m1x_posterior_previous = self.m1x_posterior_previous.reshape(-1,10,1)\n",
        "        m1x_f3_0 = self.m1x_posterior - self.m1x_posterior_previous\n",
        "        m1x_f3_reshape = torch.squeeze(m1x_f3_0)\n",
        "        # m1x_f3_reshape = m1x_f3_0\n",
        "        m1x_f3_norm = func.normalize(m1x_f3_reshape, p=2, dim=0, eps=1e-12, out=None)\n",
        "        m1x_f3_norm = m1x_f3_norm.unsqueeze(-1)\n",
        "        # Reshape and Normalize m1x Posterior\n",
        "        # m1x_post_0 = self.m1x_posterior - self.state_process_posterior_0 # Option 1\n",
        "\n",
        "        # Featture 4: x_t|t - x_t|t-1\n",
        "\n",
        "        m1x_f4_0 = self.m1x_posterior.squeeze() - self.m1x_prior_previous\n",
        "        # m1x_reshape = torch.squeeze(self.m1x_posterior) # Option 3\n",
        "        m1x_f4_reshape = torch.squeeze(m1x_f4_0)\n",
        "        # m1x_f4_reshape = m1x_f4_0\n",
        "        m1x_f4_norm = func.normalize(m1x_f4_reshape, p=2, dim=0, eps=1e-12, out=None)\n",
        "        m1x_f4_norm = m1x_f4_norm.unsqueeze(-1)\n",
        "\n",
        "        # Normalize y\n",
        "        # my_0 = y - torch.squeeze(self.obs_process_0) # Option 1\n",
        "        # my_0 = y - torch.squeeze(self.m1y) # Option 2\n",
        "        # my_0 = y\n",
        "        # y_norm = func.normalize(my_0, p=2, dim=0, eps=1e-12, out=None)\n",
        "        # y_norm = func.normalize(y, p=2, dim=0, eps=1e-12, out=None);\n",
        "\n",
        "        # Input for counting\n",
        "        count_norm = func.normalize(torch.tensor([self.i]).float(), dim=0, eps=1e-12, out=None)\n",
        "        # KGain Net Input\n",
        "        # KGainNet_in = torch.cat([y_f1_norm.to(dev), m1x_f3_norm.to(dev), m1x_f4_norm.to(dev)], dim=0).type(torch.FloatTensor)\n",
        "        KGainNet_in = torch.cat([y_f1_norm.to(dev), m1x_f3_norm.to(dev), m1x_f4_norm.to(dev)], dim=1).type(torch.FloatTensor)\n",
        "\n",
        "        # KGainNet_in = torch.cat([y_f1_norm.to(dev), m1x_f3_norm.to(dev)],dim=0).type(torch.FloatTensor) # m1x_f4_norm.to(dev)], dim=0).type(torch.FloatTensor)\n",
        "        KGainNet_in = KGainNet_in.to(dev)\n",
        "        # Kalman Gain Network Step\n",
        "        KG = self.KGain_step(KGainNet_in.to(dev)).to(dev)\n",
        "        # Reshape Kalman Gain to a Matrix\n",
        "        self.KGain = torch.reshape(KG, (-1, self.m, self.n)).type(torch.FloatTensor).to(dev)\n",
        "\n",
        "    #######################\n",
        "    ### Kalman Net Step ###\n",
        "    #######################\n",
        "    def KNet_step(self, y):\n",
        "        # Compute Priors\n",
        "        self.step_prior()\n",
        "\n",
        "        # Compute Kalman Gain\n",
        "        self.step_KGain_est(y.to(dev))\n",
        "        # Save KGain in array\n",
        "        self.KGain_array[self.i:self.i+self.KGain.shape[0]] = self.KGain.to(dev)\n",
        "        self.i += 1\n",
        "\n",
        "        # Innovation\n",
        "        # y_obs = torch.unsqueeze(y, 1)\n",
        "        dy = torch.squeeze(y).to(dev) - self.m1y.to(dev)\n",
        "\n",
        "        # Compute the 1-st posterior moment\n",
        "        INOV = torch.matmul(self.KGain.float().to(dev), dy.unsqueeze(-1).float())\n",
        "        self.m1x_posterior_previous = self.m1x_posterior.to(dev)\n",
        "        self.m1x_posterior = self.m1x_prior.unsqueeze(-1).to(dev) +INOV.to(dev)\n",
        "\n",
        "        self.state_process_posterior_0 = self.state_process_prior_0.to(dev)\n",
        "        self.m1x_prior_previous = self.m1x_prior.to(dev)\n",
        "        self.y_previous = y.to(dev)\n",
        "\n",
        "        # return\n",
        "        return torch.squeeze(self.m1x_posterior)\n",
        "\n",
        "    ########################\n",
        "    ### Kalman Gain Step ###\n",
        "    ########################\n",
        "    def KGain_step(self, KGainNet_in):\n",
        "\n",
        "        def expand_dim(x):\n",
        "            expanded = torch.empty(self.seq_len_input, self.batch_size, x.shape[-1]).to(self.device)\n",
        "            expanded[0, :, :] = x\n",
        "            return expanded\n",
        "\n",
        "        # obs_diff = expand_dim(obs_diff)\n",
        "        # obs_innov_diff = expand_dim(obs_innov_diff)\n",
        "        # fw_evol_diff = expand_dim(fw_evol_diff)\n",
        "        # fw_update_diff = expand_dim(fw_update_diff)\n",
        "\n",
        "        ###################\n",
        "        ### Input Layer ###\n",
        "        ###################\n",
        "        self.dropout1 = nn.Dropout(p=0.1)\n",
        "        self.dropout2 = nn.Dropout(p=0.1)\n",
        "        # Reshape KGainNet_in to match the input shape expected by KG_l1\n",
        "        KGainNet_in = KGainNet_in.view(KGainNet_in.size(0), -1)\n",
        "        L1_out = self.KG_l1(KGainNet_in)\n",
        "        La1_out = self.KG_relu1(L1_out)\n",
        "        # La1_out = self.dropout1(self.KG_relu1(L1_out))\n",
        "\n",
        "        ###########\n",
        "        ### GRU ###\n",
        "        ###########\n",
        "        GRU_in = torch.zeros(self.seq_len_input, self.batch_size, self.input_dim)\n",
        "        GRU_in[0, :, :] = La1_out.to(dev)\n",
        "        GRU_out, self.hn = self.rnn_GRU(GRU_in.type(torch.FloatTensor).to(dev), self.hn.type(torch.FloatTensor).to(dev))\n",
        "        # GRU_out_reshape = torch.reshape(GRU_out, (1, self.hidden_dim))\n",
        "\n",
        "        ####################\n",
        "        ### Hidden Layer ###\n",
        "        ####################\n",
        "        L2_out = self.KG_l2(GRU_out)\n",
        "        La2_out = self.KG_relu2(L2_out)\n",
        "\n",
        "        ####################\n",
        "        ### Output Layer ###\n",
        "        ####################\n",
        "        L3_out = self.KG_l3(La2_out)\n",
        "        return L3_out\n",
        "\n",
        "    ###############\n",
        "    ### Forward ###\n",
        "    ###############\n",
        "    def forward(self, y):\n",
        "        # yt = torch.squeeze(y)\n",
        "        if True in torch.isnan(y):\n",
        "          print(\" ####################### Obsrevations contains nan!!\")\n",
        "        '''\n",
        "        for t in range(0, self.T):\n",
        "            self.x_out[:, t] = self.KNet_step(y[:, t])\n",
        "        '''\n",
        "        self.x_out = self.KNet_step(y.to(dev))\n",
        "        return self.x_out\n",
        "\n",
        "    #########################\n",
        "    ### Init Hidden State ###\n",
        "    #########################\n",
        "\n",
        "    def init_hidden(self):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, self.batch_size, self.hidden_dim).zero_()\n",
        "        self.hn = hidden.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIAIgmpKJSyy"
      },
      "source": [
        "# ***GSP_EXtended Kalmannet***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wxLi0Fl5JcHj"
      },
      "outputs": [],
      "source": [
        "\"\"\"# **Class: KalmanNet**\"\"\"\n",
        "nGRU = 2\n",
        "class GSPExtendedKalmanNetNN(torch.nn.Module):\n",
        "\n",
        "    ###################\n",
        "    ### Constructor ###\n",
        "    ###################\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    ######################################\n",
        "    ### Initialize Kalman Gain Network ###\n",
        "    ######################################\n",
        "\n",
        "    def Build(self, ssModel, infoString='fullInfo'):\n",
        "        self.V = ssModel.V\n",
        "        self.V_t = ssModel.V_t\n",
        "        self.InitSystemDynamics(ssModel.f, ssModel.h, ssModel.m, ssModel.n, infoString='fullInfo')\n",
        "\n",
        "        # Number of neurons in the 1st hidden layer\n",
        "        H1_KNet = (ssModel.m + ssModel.n) * 8\n",
        "\n",
        "        # Number of neurons in the 2nd hidden layer\n",
        "        H2_KNet = (ssModel.n) * 1 * (4)\n",
        "\n",
        "        self.InitKGainNet(H1_KNet, H2_KNet)\n",
        "\n",
        "    def InitKGainNet(self, H1, H2):\n",
        "        # Input Dimensions (+1 for time input)\n",
        "        D_in = self.m + self.m + self.n  # F1,3,4\n",
        "\n",
        "        # Output Dimensions\n",
        "        D_out = self.n  # Diagonal Kalman Gain\n",
        "\n",
        "        ###################\n",
        "        ### Input Layer ###\n",
        "        ###################\n",
        "\n",
        "        # Linear Layer\n",
        "        self.KG_l1 = torch.nn.Linear(D_in, H1, bias=True).type(torch.FloatTensor)\n",
        "\n",
        "        # ReLU (Rectified Linear Unit) Activation Function\n",
        "        self.KG_relu1 = torch.nn.ReLU()\n",
        "\n",
        "        ###########\n",
        "        ### GRU ###\n",
        "        ###########\n",
        "\n",
        "        # Input Dimension\n",
        "        self.input_dim = H1\n",
        "        # Hidden Dimension\n",
        "        # self.hidden_dim = ((self.n * self.n) + (self.m * self.m)) * 10 * 1\n",
        "        self.hidden_dim = ((self.n + self.m)) * 10\n",
        "        # Number of Layers\n",
        "        self.n_layers = nGRU\n",
        "        # Batch Size\n",
        "        self.batch_size = 1\n",
        "        # Input Sequence Length\n",
        "        self.seq_len_input = 1\n",
        "        # Hidden Sequence Length\n",
        "        self.seq_len_hidden = self.n_layers\n",
        "\n",
        "        # batch_first = False\n",
        "        # dropout = 0.1 ;\n",
        "\n",
        "        # Initialize a Tensor for GRU Input\n",
        "        # self.GRU_in = torch.empty(self.seq_len_input, self.batch_size, self.input_dim)\n",
        "\n",
        "        # Initialize a Tensor for Hidden State\n",
        "        self.hn = torch.randn(self.seq_len_hidden, self.batch_size, self.hidden_dim)\n",
        "\n",
        "        # Iniatialize GRU Layer\n",
        "        self.rnn_GRU = nn.GRU(self.input_dim, self.hidden_dim, self.n_layers)\n",
        "\n",
        "        ####################\n",
        "        ### Hidden Layer ###\n",
        "        ####################\n",
        "\n",
        "        self.KG_l2 = torch.nn.Linear(self.hidden_dim, H2, bias=True)\n",
        "\n",
        "\n",
        "        # ReLU (Rectified Linear Unit) Activation Function\n",
        "        self.KG_relu2 = torch.nn.ReLU()\n",
        "\n",
        "        ####################\n",
        "        ### Output Layer ###\n",
        "        ####################\n",
        "        self.KG_l3 = torch.nn.Linear(H2, D_out, bias=True)\n",
        "\n",
        "    ##################################\n",
        "    ### Initialize System Dynamics ###\n",
        "    ##################################\n",
        "    def InitSystemDynamics(self, f, h, m, n, infoString='fullInfo'):\n",
        "\n",
        "        if (infoString == 'partialInfo'):\n",
        "            self.fString = 'ModInacc'\n",
        "            self.hString = 'ObsInacc'\n",
        "        else:\n",
        "            self.fString = 'ModAcc'\n",
        "            self.hString = 'ObsAcc'\n",
        "\n",
        "        # Set State Evolution Function\n",
        "        self.f = f\n",
        "        self.m = m\n",
        "\n",
        "        # Set Observation Function\n",
        "        self.h = h\n",
        "        self.n = n\n",
        "\n",
        "    ###########################\n",
        "    ### Initialize Sequence ###\n",
        "    ###########################\n",
        "    def InitSequence(self, M1_0, T):\n",
        "        M1_0 = self.GFT(M1_0.type(torch.FloatTensor).to(dev))\n",
        "        self.m1x_posterior = M1_0.unsqueeze(-1)\n",
        "        self.m1x_posterior_previous = 0  # for t=0\n",
        "\n",
        "        self.T = T\n",
        "        self.x_out = torch.zeros(self.m, T)\n",
        "\n",
        "        self.state_process_posterior_0 = torch.squeeze(M1_0)\n",
        "        self.m1x_prior_previous = self.m1x_posterior\n",
        "\n",
        "        # KGain saving\n",
        "        self.i = 0\n",
        "        self.KGain_array = self.KG_array = torch.zeros((self.T*1000, self.m, self.n)).to(dev)\n",
        "\n",
        "    ######################\n",
        "    ### Compute Priors ###\n",
        "    ######################\n",
        "    def step_prior(self):\n",
        "        # Predict the 1-st moment of x\n",
        "        temp = self.f(self.BMM_multipy(self.V.type(torch.DoubleTensor), self.m1x_posterior.type(torch.DoubleTensor))).type(torch.DoubleTensor)\n",
        "        self.m1x_prior = self.BMM_multipy(self.V_t, torch.squeeze(temp).type(torch.FloatTensor).to(dev))\n",
        "\n",
        "        # Predict the 1-st moment of y\n",
        "        temp = torch.squeeze(self.h(self.BMM_multipy(self.V, self.m1x_prior)))\n",
        "        # print(temp.shape)\n",
        "        self.m1y = self.BMM_multipy(self.V_t, temp)\n",
        "\n",
        "        # Update Jacobians\n",
        "        # self.JFt = get_Jacobian(self.m1x_posterior, self.fString)\n",
        "        # self.JHt = get_Jacobian(self.m1x_prior, self.hString)\n",
        "\n",
        "        self.state_process_prior_0 = torch.squeeze(self.f(self.state_process_posterior_0))\n",
        "        self.obs_process_0 = torch.squeeze(self.h(self.state_process_prior_0))\n",
        "\n",
        "    ##############################\n",
        "    ### Kalman Gain Estimation ###\n",
        "    ##############################\n",
        "    def step_KGain_est(self, y):\n",
        "        # Feature 1: yt - yt-1\n",
        "        try:\n",
        "            my_f1_0 = y.to(dev) - torch.squeeze(self.y_previous).to(dev)\n",
        "        except:\n",
        "            my_f1_0 = y.to(dev) - torch.squeeze(self.obs_process_0).to(dev)  # when t=0\n",
        "        # my_f1_reshape = torch.squeeze(my_f1_0)\n",
        "        y_f1_norm = func.normalize(my_f1_0.to(dev), p=2, dim=0, eps=1e-12, out=None).to(dev)\n",
        "\n",
        "        # Feature 2: yt - y_t+1|t\n",
        "        # my_f2_0 = y - torch.squeeze(self.m1y)\n",
        "        # my_f2_reshape = torch.squeeze(my_f2_0)\n",
        "        # y_f2_norm = func.normalize(my_f2_reshape, p=2, dim=0, eps=1e-12, out=None)\n",
        "\n",
        "        # Feature 3: x_t|t - x_t-1|t-1\n",
        "        m1x_f3_0 = self.m1x_posterior.to(dev) - self.m1x_posterior_previous\n",
        "        m1x_f3_reshape = torch.squeeze(m1x_f3_0)\n",
        "        m1x_f3_norm = func.normalize(m1x_f3_reshape, p=2, dim=0, eps=1e-12, out=None)\n",
        "\n",
        "        # Reshape and Normalize m1x Posterior\n",
        "        # m1x_post_0 = self.m1x_posterior - self.state_process_posterior_0 # Option 1\n",
        "\n",
        "        # Featture 4: x_t|t - x_t|t-1\n",
        "        try:\n",
        "          m1x_f4_0 = self.m1x_posterior - self.m1x_prior_previous\n",
        "        except:\n",
        "          m1x_f4_0 = self.m1x_posterior - self.m1x_prior_previous.unsqueeze(-1)\n",
        "        m1x_reshape = torch.squeeze(self.m1x_posterior) # Option 3\n",
        "        m1x_f4_reshape = torch.squeeze(m1x_reshape)\n",
        "        m1x_f4_norm = func.normalize(m1x_f4_reshape, p=2, dim=0, eps=1e-12, out=None)\n",
        "\n",
        "        # Normalize y\n",
        "        # my_0 = y - torch.squeeze(self.obs_process_0) # Option 1\n",
        "        # my_0 = y - torch.squeeze(self.m1y) # Option 2\n",
        "        # my_0 = y\n",
        "        # y_norm = func.normalize(my_0, p=2, dim=0, eps=1e-12, out=None)\n",
        "        # y_norm = func.normalize(y, p=2, dim=0, eps=1e-12, out=None);\n",
        "\n",
        "        # Input for counting\n",
        "        count_norm = func.normalize(torch.tensor([self.i]).float(), dim=0, eps=1e-12, out=None)\n",
        "\n",
        "        # KGain Net Input\n",
        "        KGainNet_in = torch.cat([y_f1_norm.to(dev), m1x_f3_norm.to(dev), m1x_f4_norm.to(dev)], dim=1).type(torch.FloatTensor)\n",
        "\n",
        "        # KGainNet_in = torch.cat([y_f1_norm.to(dev), m1x_f3_norm.to(dev)],dim=0).type(torch.FloatTensor) # m1x_f4_norm.to(dev)], dim=0).type(torch.FloatTensor)\n",
        "        KGainNet_in = KGainNet_in.to(dev)\n",
        "        # Kalman Gain Network Step\n",
        "        KG = self.KGain_step(KGainNet_in.to(dev)).to(dev)\n",
        "        KG = torch.diag_embed(KG)\n",
        "        # Reshape Kalman Gain to a Matrix\n",
        "        self.KGain = torch.reshape(KG, (-1, self.m, self.n)).type(torch.FloatTensor).to(dev)\n",
        "\n",
        "    #######################\n",
        "    ### Kalman Net Step ###\n",
        "    #######################\n",
        "    def KNet_step(self, y):\n",
        "        # Compute Priors\n",
        "        self.step_prior()\n",
        "\n",
        "        # Compute Kalman Gain\n",
        "        self.step_KGain_est(y.to(dev))\n",
        "\n",
        "        # Save KGain in array\n",
        "        self.KGain_array[self.i:self.i+self.KGain.shape[0]] = self.KGain.to(dev)\n",
        "        self.i += 1\n",
        "\n",
        "        # Innovation\n",
        "        # y_obs = torch.unsqueeze(y, 1)\n",
        "        dy = torch.squeeze(y).to(dev) - self.m1y.to(dev)\n",
        "\n",
        "        # Compute the 1-st posterior moment\n",
        "        INOV = torch.matmul(self.KGain.float().to(dev), dy.unsqueeze(-1).float())\n",
        "        self.m1x_posterior_previous = self.m1x_posterior.to(dev)\n",
        "        self.m1x_posterior = self.m1x_prior.unsqueeze(-1).to(dev) +INOV.to(dev)\n",
        "\n",
        "        self.state_process_posterior_0 = self.state_process_prior_0\n",
        "        self.m1x_prior_previous = self.m1x_prior\n",
        "        self.y_previous = y\n",
        "\n",
        "        # return\n",
        "        return torch.squeeze(self.m1x_posterior)\n",
        "\n",
        "    ########################\n",
        "    ### Kalman Gain Step ###\n",
        "    ########################\n",
        "    def KGain_step(self, KGainNet_in):\n",
        "        def expand_dim(x):\n",
        "            expanded = torch.empty(self.seq_len_input, self.batch_size, x.shape[-1]).to(self.device)\n",
        "            expanded[0, :, :] = x\n",
        "            return expanded\n",
        "\n",
        "        ###################\n",
        "        ### Input Layer ###\n",
        "        ###################\n",
        "\n",
        "        KGainNet_in = KGainNet_in.view(KGainNet_in.size(0), -1)\n",
        "        # print(KGainNet_in.shape)\n",
        "        L1_out = self.KG_l1(KGainNet_in)\n",
        "        La1_out = self.KG_relu1(L1_out)\n",
        "\n",
        "        ###########\n",
        "        ### GRU ###\n",
        "        ###########\n",
        "        GRU_in = torch.zeros(self.seq_len_input, self.batch_size, self.input_dim)\n",
        "        GRU_in[0, :, :] = La1_out.to(dev)\n",
        "        GRU_out, self.hn = self.rnn_GRU(GRU_in.type(torch.FloatTensor).to(dev), self.hn.type(torch.FloatTensor).to(dev))\n",
        "        # GRU_out_reshape = torch.reshape(GRU_out, (1, self.hidden_dim))\n",
        "\n",
        "        ####################\n",
        "        ### Hidden Layer ###\n",
        "        ####################\n",
        "        L2_out = self.KG_l2(GRU_out)\n",
        "        La2_out = self.KG_relu2(L2_out)\n",
        "\n",
        "        ####################\n",
        "        ### Output Layer ###\n",
        "        ####################\n",
        "        L3_out = self.KG_l3(La2_out)\n",
        "        return L3_out\n",
        "\n",
        "    ###############\n",
        "    ### Forward ###\n",
        "    ###############\n",
        "    def forward(self, y):\n",
        "        yt = torch.squeeze(y)\n",
        "        yt = self.GFT(yt).to(dev)\n",
        "        # self.x_out = self.KNet_step(yt)\n",
        "        return self.IGFT(self.KNet_step(yt.to(dev).to(dev)))\n",
        "\n",
        "    #########################\n",
        "    ### Init Hidden State ###\n",
        "    #########################\n",
        "    def init_hidden(self):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, self.batch_size, self.hidden_dim).zero_()\n",
        "        self.hn = hidden.data\n",
        "\n",
        "    def GFT(self, input):\n",
        "        return self.BMM_multipy(self.V_t,input.to(dev)).type(torch.FloatTensor)\n",
        "\n",
        "    def IGFT(self, input):\n",
        "        # return torch.matmul(self.V, input.to(dev)).type(torch.FloatTensor)\n",
        "        return self.BMM_multipy(self.V,input.to(dev)).type(torch.FloatTensor)\n",
        "\n",
        "    def GFT_matrix(self, input):\n",
        "        return torch.matmul(torch.matmul(self.V_t, input.to(dev)), self.V).type(torch.FloatTensor)\n",
        "        # return self.BMM_multipy(self.V_t,input.to(dev)).type(torch.FloatTensor)\n",
        "\n",
        "    def IGFT_matrix(self, input):\n",
        "        return torch.matmul(torch.matmul(self.V, input.to(dev)), self.V).type(torch.FloatTensor)\n",
        "\n",
        "    def BMM_multipy(self,a,b):\n",
        "        if len(b.size())==2:\n",
        "          b = b.unsqueeze(-1)\n",
        "        return torch.bmm(a.expand(b.size()[0], -1, -1).type(torch.DoubleTensor).to(dev), b.type(torch.DoubleTensor).to(dev)).squeeze(-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9iDDT0RyOj2"
      },
      "source": [
        "# Linear Sys model #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uXO79Es7IQtf"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "\n",
        "class SystemModel_KF:\n",
        "\n",
        "    def __init__(self, F, q, H, r, T, T_test, m, n, L, V, V_t, outlier_p=0, rayleigh_sigma=10000,):\n",
        "\n",
        "        self.L = L.to(dev)\n",
        "        self.V = V.to(dev)\n",
        "        self.V_t = V_t.to(dev)\n",
        "        self.m = m\n",
        "        self.n = n\n",
        "\n",
        "        self.outlier_p = outlier_p\n",
        "        self.rayleigh_sigma = rayleigh_sigma\n",
        "        ####################\n",
        "        ### Motion Model ###\n",
        "        ####################\n",
        "        self.F = F\n",
        "        self.m = self.F.size()[0]\n",
        "\n",
        "        self.q = q\n",
        "        self.Q = q * q * torch.eye(self.m).to(dev)\n",
        "\n",
        "        #########################\n",
        "        ### Observation Model ###\n",
        "        #########################\n",
        "        self.H = H\n",
        "        self.n = self.H.size()[0]\n",
        "\n",
        "        self.r = r\n",
        "        self.R = r * r * torch.eye(self.n).to(dev)\n",
        "\n",
        "        # Assign T and T_test\n",
        "        self.T = T\n",
        "        self.T_test = T_test\n",
        "\n",
        "    #####################\n",
        "    ### Init Sequence ###\n",
        "    #####################\n",
        "    def InitSequence(self, m1x_0, m2x_0):\n",
        "\n",
        "        self.m1x_0 = m1x_0\n",
        "        self.m2x_0 = m2x_0\n",
        "\n",
        "    #########################\n",
        "    ### Update Covariance ###\n",
        "    #########################\n",
        "    def UpdateCovariance_Gain(self, q, r):\n",
        "\n",
        "        self.q = q\n",
        "        self.Q = q * q * torch.eye(self.m)\n",
        "\n",
        "        self.r = r\n",
        "        self.R = r * r * torch.eye(self.n)\n",
        "\n",
        "    def UpdateCovariance_Matrix(self, Q, R):\n",
        "        self.Q = Q\n",
        "        self.R = R\n",
        "\n",
        "    #########################\n",
        "    ### Generate Sequence ###\n",
        "    #########################\n",
        "    def GenerateSequence(self, Q_gen, R_gen, T):\n",
        "        # Pre allocate an array for current state\n",
        "        self.x = torch.zeros(size=[self.m, T])\n",
        "        # Pre allocate an array for current observation\n",
        "        self.y = torch.zeros(size=[self.n, T])\n",
        "        # Set x0 to be x previous\n",
        "        self.x_prev = self.m1x_0\n",
        "\n",
        "        # Outliers\n",
        "        if self.outlier_p > 0:\n",
        "            b_matrix = torch.bernoulli(self.outlier_p * torch.ones(T))\n",
        "\n",
        "        # Generate Sequence Iteratively\n",
        "        for t in range(0, T):\n",
        "            ########################\n",
        "            #### State Evolution ###\n",
        "            ########################\n",
        "            # Process Noise\n",
        "            if self.q == 0:\n",
        "                xt = self.F.matmul(self.x_prev)\n",
        "            else:\n",
        "                xt = self.F.matmul(self.x_prev)\n",
        "                mean = torch.zeros([self.m])\n",
        "                distrib = MultivariateNormal(loc=mean, covariance_matrix=Q_gen)\n",
        "                eq = distrib.rsample()\n",
        "                # eq = torch.normal(mean, self.q)\n",
        "                eq = torch.reshape(eq[:], [self.m, 1])\n",
        "                # Additive Process Noise\n",
        "                xt = torch.add(xt, eq)\n",
        "\n",
        "            ################\n",
        "            ### Emission ###\n",
        "            ################\n",
        "            # Observation Noise\n",
        "            if self.r == 0:\n",
        "                yt = self.H.matmul(xt)\n",
        "            else:\n",
        "                yt = self.H.matmul(xt)\n",
        "                mean = torch.zeros([self.n])\n",
        "                distrib = MultivariateNormal(loc=mean, covariance_matrix=R_gen)\n",
        "                er = distrib.rsample()\n",
        "                er = torch.reshape(er[:], [self.n, 1])\n",
        "                # mean = torch.zeros([self.n,1])\n",
        "                # er = torch.normal(mean, self.r)\n",
        "\n",
        "                # Additive Observation Noise\n",
        "                yt = torch.add(yt, er)\n",
        "\n",
        "            # Outliers\n",
        "            if self.outlier_p > 0:\n",
        "                if b_matrix[t] != 0:\n",
        "                    btdt = self.rayleigh_sigma * torch.sqrt(-2 * torch.log(torch.rand(self.n, 1)))\n",
        "                    yt = torch.add(yt, btdt)\n",
        "\n",
        "            ########################\n",
        "            ### Squeeze to Array ###\n",
        "            ########################\n",
        "\n",
        "            # Save Current State to Trajectory Array\n",
        "            self.x[:, t] = torch.squeeze(xt)\n",
        "\n",
        "            # Save Current Observation to Trajectory Array\n",
        "            self.y[:, t] = torch.squeeze(yt)\n",
        "\n",
        "            ################################\n",
        "            ### Save Current to Previous ###\n",
        "            ################################\n",
        "            self.x_prev = xt\n",
        "\n",
        "    ######################\n",
        "    ### Generate Batch ###\n",
        "    ######################\n",
        "\n",
        "    def GenerateBatch(self, size, T, randomInit=False, seqInit=False, T_test=0):\n",
        "\n",
        "        # Allocate Empty Array for Input\n",
        "        self.Input = torch.empty(size, self.n, T)\n",
        "\n",
        "        # Allocate Empty Array for Target\n",
        "        self.Target = torch.empty(size, self.m, T)\n",
        "\n",
        "        ### Generate Examples\n",
        "        initConditions = self.m1x_0\n",
        "\n",
        "        for i in range(0, size):\n",
        "            # Generate Sequence\n",
        "\n",
        "            # Randomize initial conditions to get a rich dataset\n",
        "            if (randomInit):\n",
        "                variance = 100\n",
        "                initConditions = torch.rand_like(self.m1x_0) * variance\n",
        "            if (seqInit):\n",
        "                initConditions = self.x_prev\n",
        "                if ((i * T % T_test) == 0):\n",
        "                    initConditions = torch.zeros_like(self.m1x_0)\n",
        "\n",
        "            self.InitSequence(initConditions, self.m2x_0)\n",
        "            self.GenerateSequence(self.Q, self.R, T)\n",
        "\n",
        "            # Training sequence input\n",
        "            self.Input[i, :, :] = self.y\n",
        "\n",
        "            # Training sequence output\n",
        "            self.Target[i, :, :] = self.x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "divDfLacyTs8"
      },
      "source": [
        "# Pipline KF #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "59Lr0FMzEfyn"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from Plot import Plot\n",
        "class Pipeline_KF:\n",
        "\n",
        "    def __init__(self, Time, folderName, modelName):\n",
        "        super().__init__()\n",
        "        self.Time = Time\n",
        "        self.folderName = folderName + '/'\n",
        "        self.modelName = modelName\n",
        "        self.modelFileName = self.folderName + \"model.pth\"  # \"model_\" + self.modelName\n",
        "        self.PipelineName = self.folderName + \"pipeline\"  # _\" + self.modelName\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self, self.PipelineName)\n",
        "\n",
        "    def setssModel(self, ssModel):\n",
        "        self.ssModel = ssModel\n",
        "\n",
        "    def setModel(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def setTrainingParams(self, n_Epochs, n_Batch, learningRate, weightDecay):\n",
        "        self.N_Epochs = n_Epochs  # Number of Training Epochs\n",
        "        self.N_B = n_Batch  # Number of Samples in Batch\n",
        "        self.learningRate = learningRate  # Learning Rate\n",
        "        self.weightDecay = weightDecay  # L2 Weight Regularization - Weight Decay\n",
        "\n",
        "        # MSE LOSS Function\n",
        "        self.loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "        # Use the optim package to define an Optimizer that will update the weights of\n",
        "        # the model for us. Here we will use Adam; the optim package contains many other\n",
        "        # optimization algoriths. The first argument to the Adam constructor tells the\n",
        "        # optimizer which Tensors it should update.\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learningRate, weight_decay=self.weightDecay)\n",
        "\n",
        "    def NNTrain(self, n_Examples, train_input, train_target, n_CV, cv_input, cv_target):\n",
        "\n",
        "        self.N_E = n_Examples\n",
        "        self.N_CV = n_CV\n",
        "\n",
        "        MSE_cv_linear_batch = torch.empty([self.N_CV])\n",
        "        self.MSE_cv_linear_epoch = torch.empty([self.N_Epochs])\n",
        "        self.MSE_cv_dB_epoch = torch.empty([self.N_Epochs])\n",
        "\n",
        "        MSE_train_linear_batch = torch.empty([self.N_B])\n",
        "        self.MSE_train_linear_epoch = torch.empty([self.N_Epochs])\n",
        "        self.MSE_train_dB_epoch = torch.empty([self.N_Epochs])\n",
        "\n",
        "        ##############\n",
        "        ### Epochs ###\n",
        "        ##############\n",
        "\n",
        "        self.MSE_cv_dB_opt = 1000\n",
        "        self.MSE_cv_idx_opt = 0\n",
        "\n",
        "        for ti in range(0, self.N_Epochs):\n",
        "\n",
        "            #################################\n",
        "            ### Validation Sequence Batch ###\n",
        "            #################################\n",
        "\n",
        "            # Cross Validation Mode\n",
        "            self.model.eval()\n",
        "\n",
        "            for j in range(0, self.N_CV):\n",
        "                y_cv = cv_input[j, :, :]\n",
        "                # y_cv = cv_input\n",
        "                self.model.InitSequence(self.ssModel.m1x_0)\n",
        "\n",
        "                x_out_cv = torch.empty(self.ssModel.m, self.ssModel.T)\n",
        "                # x_out_cv = torch.empty(self.ssModel.m, 100)\n",
        "                for t in range(0, self.ssModel.T):\n",
        "                # for t in range(0, 100):\n",
        "                    x_out_cv[:, t] = self.model(y_cv[:, t])\n",
        "\n",
        "                # Compute Training Loss\n",
        "                MSE_cv_linear_batch[j] = self.loss_fn(x_out_cv, cv_target[j, :, :]).item()\n",
        "                # MSE_cv_linear_batch[j] = self.loss_fn(x_out_cv, cv_target).item()\n",
        "\n",
        "            # Average\n",
        "            self.MSE_cv_linear_epoch[ti] = torch.mean(MSE_cv_linear_batch)\n",
        "            self.MSE_cv_dB_epoch[ti] = 10 * torch.log10(self.MSE_cv_linear_epoch[ti])\n",
        "\n",
        "            if (self.MSE_cv_dB_epoch[ti] < self.MSE_cv_dB_opt):\n",
        "                self.MSE_cv_dB_opt = self.MSE_cv_dB_epoch[ti]\n",
        "                self.MSE_cv_idx_opt = ti\n",
        "                torch.save(self.model, self.modelFileName)\n",
        "\n",
        "            ###############################\n",
        "            ### Training Sequence Batch ###\n",
        "            ###############################\n",
        "\n",
        "            # Training Mode\n",
        "            self.model.train()\n",
        "\n",
        "            # Init Hidden State\n",
        "            self.model.init_hidden()\n",
        "\n",
        "            Batch_Optimizing_LOSS_sum = 0\n",
        "\n",
        "            for j in range(0, self.N_B):\n",
        "            # for j in range(0, 1):\n",
        "                n_e = random.randint(0, self.N_E - 1)\n",
        "\n",
        "                y_training = train_input[n_e, :, :]\n",
        "                self.model.InitSequence(self.ssModel.m1x_0)\n",
        "\n",
        "                x_out_training = torch.empty(self.ssModel.m, self.ssModel.T)\n",
        "                # x_out_training = torch.empty(self.ssModel.m, 200)\n",
        "                for t in range(0, self.ssModel.T):\n",
        "                # for t in range(0, 200):\n",
        "                    x_out_training[:, t] = self.model(y_training[:, t])\n",
        "\n",
        "                # Compute Training Loss\n",
        "                # LOSS = self.loss_fn(x_out_training, train_target[n_e, :, :])\n",
        "                LOSS = self.loss_fn(x_out_training, train_target)\n",
        "                MSE_train_linear_batch[j] = LOSS.item()\n",
        "\n",
        "                Batch_Optimizing_LOSS_sum = Batch_Optimizing_LOSS_sum + LOSS\n",
        "\n",
        "            # Average\n",
        "            self.MSE_train_linear_epoch[ti] = torch.mean(MSE_train_linear_batch)\n",
        "            self.MSE_train_dB_epoch[ti] = 10 * torch.log10(self.MSE_train_linear_epoch[ti])\n",
        "\n",
        "            ##################\n",
        "            ### Optimizing ###\n",
        "            ##################\n",
        "\n",
        "            # Before the backward pass, use the optimizer object to zero all of the\n",
        "            # gradients for the variables it will update (which are the learnable\n",
        "            # weights of the model). This is because by default, gradients are\n",
        "            # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "            # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Backward pass: compute gradient of the loss with respect to model\n",
        "            # parameters\n",
        "            Batch_Optimizing_LOSS_mean = Batch_Optimizing_LOSS_sum / self.N_B\n",
        "            Batch_Optimizing_LOSS_mean.backward()\n",
        "\n",
        "            # Calling the step function on an Optimizer makes an update to its\n",
        "            # parameters\n",
        "            self.optimizer.step()\n",
        "\n",
        "            ########################\n",
        "            ### Training Summary ###\n",
        "            ########################\n",
        "            print(ti, \"MSE Training :\", self.MSE_train_dB_epoch[ti], \"[dB]\", \"MSE Validation :\",\n",
        "                  self.MSE_cv_dB_epoch[ti],\n",
        "                  \"[dB]\")\n",
        "\n",
        "            if (ti > 1):\n",
        "                d_train = self.MSE_train_dB_epoch[ti] - self.MSE_train_dB_epoch[ti - 1]\n",
        "                d_cv = self.MSE_cv_dB_epoch[ti] - self.MSE_cv_dB_epoch[ti - 1]\n",
        "                print(\"diff MSE Training :\", d_train, \"[dB]\", \"diff MSE Validation :\", d_cv, \"[dB]\")\n",
        "\n",
        "            print(\"Optimal idx:\", self.MSE_cv_idx_opt, \"Optimal :\", self.MSE_cv_dB_opt, \"[dB]\")\n",
        "        self.plot_learning_curve()\n",
        "\n",
        "\n",
        "    def NNTest(self, n_Test, test_input, test_target):\n",
        "\n",
        "        self.N_T = n_Test\n",
        "\n",
        "        self.MSE_test_linear_arr = torch.zeros([self.N_T])\n",
        "        self.MSE_test_per_iter = torch.zeros(test_target.shape[2])\n",
        "        self.MSE_finalize = torch.zeros(test_target.shape[2])\n",
        "        # MSE LOSS Function\n",
        "        loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "        self.model = torch.load(self.modelFileName)\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        torch.no_grad()\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        for j in range(0, self.N_T):\n",
        "\n",
        "            y_mdl_tst = test_input[j, :, :]\n",
        "            # y_mdl_tst = test_input\n",
        "            self.model.InitSequence(self.ssModel.m1x_0)\n",
        "\n",
        "            x_out_test = torch.empty(self.ssModel.m, self.ssModel.T)\n",
        "            # x_out_test = torch.empty(self.ssModel.m, 100)\n",
        "\n",
        "            for t in range(0, self.ssModel.T):\n",
        "            # for t in range(0, 100):\n",
        "                x_out_test[:, t] = self.model(y_mdl_tst[:, t])\n",
        "                self.MSE_test_per_iter[t] = loss_fn(x_out_test[:,t],test_target[j,:,t])\n",
        "\n",
        "            self.MSE_test_linear_arr[j] = loss_fn(x_out_test, test_target[j, :, :]).item()\n",
        "            self.MSE_finalize += self.MSE_test_per_iter\n",
        "            # self.MSE_test_linear_arr[j] = loss_fn(x_out_test, test_target).item()\n",
        "\n",
        "        end = time.time()\n",
        "        t = end - start\n",
        "\n",
        "        # Average\n",
        "        self.MSE_test_linear_avg = torch.mean(self.MSE_test_linear_arr)\n",
        "        self.MSE_test_dB_avg = 10 * torch.log10(self.MSE_test_linear_avg)\n",
        "        self.MSE_finalize = self.MSE_finalize/self.N_T\n",
        "        self.MSE_finalize_dB = 10 * torch.log10(self.MSE_finalize)\n",
        "\n",
        "        # Standard deviation\n",
        "        self.MSE_test_dB_std = torch.std(self.MSE_test_linear_arr, unbiased=True)\n",
        "        self.MSE_test_dB_std = 10 * torch.log10(self.MSE_test_dB_std)\n",
        "\n",
        "        # Print MSE Cross Validation\n",
        "        str = self.modelName + \"-\" + \"MSE Test:\"\n",
        "        print(str, self.MSE_test_dB_avg, \"[dB]\")\n",
        "        str = self.modelName + \"-\" + \"STD Test:\"\n",
        "        print(str, self.MSE_test_dB_std, \"[dB]\")\n",
        "        # Print Run Time\n",
        "        print(\"Inference Time:\", t)\n",
        "\n",
        "        return [self.MSE_finalize_dB, self.MSE_test_linear_arr, self.MSE_test_linear_avg, self.MSE_test_dB_avg, x_out_test]\n",
        "\n",
        "    def PlotTrain_KF(self, MSE_KF_linear_arr, MSE_KF_dB_avg):\n",
        "\n",
        "        self.Plot = Plot(self.folderName, self.modelName)\n",
        "\n",
        "        self.Plot.NNPlot_epochs(self.N_Epochs, MSE_KF_dB_avg,\n",
        "                                self.MSE_test_dB_avg, self.MSE_cv_dB_epoch, self.MSE_train_dB_epoch)\n",
        "\n",
        "        self.Plot.NNPlot_Hist(MSE_KF_linear_arr, self.MSE_test_linear_arr)\n",
        "\n",
        "    def plot_learning_curve(self):\n",
        "        iters_sub = [i for i in range(self.N_Epochs)]\n",
        "        plt.title(\"Learning Curve: Accuracy per Iteration\")\n",
        "        plt.plot(iters_sub, self.MSE_train_dB_epoch.cpu(), label=\"Train\")\n",
        "        plt.plot(iters_sub, self.MSE_cv_dB_epoch.cpu(), label=\"Validation\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Accuracy - MSE\")\n",
        "        plt.legend(loc='best')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsDbdcyDyXIZ"
      },
      "source": [
        "# KalmanNet nn #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pP2lKUh2EsW6"
      },
      "outputs": [],
      "source": [
        "\n",
        "class KalmanNetNN(torch.nn.Module):\n",
        "\n",
        "    ###################\n",
        "    ### Constructor ###\n",
        "    ###################\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #############\n",
        "    ### Build ###\n",
        "    #############\n",
        "    def Build(self, ssModel):\n",
        "\n",
        "        self.InitSystemDynamics(ssModel.F, ssModel.H)\n",
        "\n",
        "        # Number of neurons in the 1st hidden layer\n",
        "        # H1_KNet = (ssModel.m + ssModel.n) * (10) * 8\n",
        "        H1_KNet = (ssModel.m + ssModel.n) * 8\n",
        "\n",
        "        # Number of neurons in the 2nd hidden layer\n",
        "        H2_KNet = (ssModel.m * ssModel.n) * 1 * (4)\n",
        "\n",
        "        self.InitKGainNet(H1_KNet, H2_KNet)\n",
        "\n",
        "    ######################################\n",
        "    ### Initialize Kalman Gain Network ###\n",
        "    ######################################\n",
        "    def InitKGainNet(self, H1, H2):\n",
        "        # Input Dimensions\n",
        "        D_in = self.m + self.n  # x(t-1), y(t)\n",
        "\n",
        "        # Output Dimensions\n",
        "        D_out = self.m * self.n  # Kalman Gain\n",
        "\n",
        "        ###################\n",
        "        ### Input Layer ###\n",
        "        ###################\n",
        "        # Linear Layer\n",
        "        self.KG_l1 = torch.nn.Linear(D_in, H1, bias=True)\n",
        "\n",
        "        # ReLU (Rectified Linear Unit) Activation Function\n",
        "        self.KG_relu1 = torch.nn.ReLU()\n",
        "\n",
        "        ###########\n",
        "        ### GRU ###\n",
        "        ###########\n",
        "        # Input Dimension\n",
        "        self.input_dim = H1\n",
        "        # Hidden Dimension\n",
        "        self.hidden_dim = (self.m * self.m + self.n * self.n) * 10\n",
        "        # Number of Layers\n",
        "        self.n_layers = 1\n",
        "        # Batch Size\n",
        "        self.batch_size = 1\n",
        "        # Input Sequence Length\n",
        "        self.seq_len_input = 1\n",
        "        # Hidden Sequence Length\n",
        "        self.seq_len_hidden = self.n_layers\n",
        "\n",
        "        # batch_first = False\n",
        "        # dropout = 0.1 ;\n",
        "\n",
        "        # Initialize a Tensor for GRU Input\n",
        "        # self.GRU_in = torch.empty(self.seq_len_input, self.batch_size, self.input_dim)\n",
        "\n",
        "        # Initialize a Tensor for Hidden State\n",
        "        self.hn = torch.randn(self.seq_len_hidden, self.batch_size, self.hidden_dim).to(self.device,non_blocking = True)\n",
        "\n",
        "        # Iniatialize GRU Layer\n",
        "        self.rnn_GRU = nn.GRU(self.input_dim, self.hidden_dim, self.n_layers)\n",
        "\n",
        "        ####################\n",
        "        ### Hidden Layer ###\n",
        "        ####################\n",
        "        self.KG_l2 = torch.nn.Linear(self.hidden_dim, H2, bias=True)\n",
        "\n",
        "        # ReLU (Rectified Linear Unit) Activation Function\n",
        "        self.KG_relu2 = torch.nn.ReLU()\n",
        "\n",
        "        ####################\n",
        "        ### Output Layer ###\n",
        "        ####################\n",
        "        self.KG_l3 = torch.nn.Linear(H2, D_out, bias=True)\n",
        "\n",
        "    ##################################\n",
        "    ### Initialize System Dynamics ###\n",
        "    ##################################\n",
        "    def InitSystemDynamics(self, F, H):\n",
        "        # Set State Evolution Matrix\n",
        "        self.F = F.to(self.device,non_blocking = True)\n",
        "        self.F_T = torch.transpose(F, 0, 1)\n",
        "        self.m = self.F.size()[0]\n",
        "\n",
        "        # Set Observation Matrix\n",
        "        self.H = H.to(self.device,non_blocking = True)\n",
        "        self.H_T = torch.transpose(H, 0, 1)\n",
        "        self.n = self.H.size()[0]\n",
        "\n",
        "    ###########################\n",
        "    ### Initialize Sequence ###\n",
        "    ###########################\n",
        "    # def InitSequence(self, M1_0):\n",
        "    #     self.m1x_prior = M1_0.to(self.device,non_blocking = True)\n",
        "\n",
        "    #     self.m1x_posterior = M1_0.to(self.device,non_blocking = True)\n",
        "\n",
        "    #     self.state_process_posterior_0 = M1_0.to(self.device,non_blocking = True)\n",
        "\n",
        "    def InitSequence(self, M1_0, T):\n",
        "\n",
        "        self.m1x_posterior = torch.squeeze(M1_0).to(dev)\n",
        "        self.m1x_posterior_previous = 0  # for t=0\n",
        "\n",
        "        self.T = T\n",
        "        self.x_out = torch.empty(self.m, T)\n",
        "\n",
        "        self.m1x_prior = M1_0.to(self.device,non_blocking = True)\n",
        "        self.state_process_posterior_0 = torch.squeeze(M1_0).to(dev)\n",
        "        self.m1x_prior_previous = self.m1x_posterior\n",
        "\n",
        "        # KGain saving\n",
        "        self.i = 0\n",
        "        self.KGain_array = self.KG_array = torch.zeros((self.T*10, self.m, self.n)).to(dev)\n",
        "\n",
        "    ######################\n",
        "    ### Compute Priors ###\n",
        "    ######################\n",
        "    def step_prior(self):\n",
        "        # Compute the 1-st moment of x based on model knowledge and without process noise\n",
        "        bmm_mul = torch.bmm(self.F.expand(self.state_process_posterior_0.size()[0], -1, -1).type(torch.DoubleTensor).to(dev), self.state_process_posterior_0.unsqueeze(-1).type(torch.DoubleTensor).to(dev)).squeeze(-1)\n",
        "        self.state_process_prior_0 = bmm_mul\n",
        "\n",
        "        # Compute the 1-st moment of y based on model knowledge and without noise\n",
        "        bmm_mul = torch.bmm(self.H.expand(self.state_process_prior_0.size()[0], -1, -1).type(torch.DoubleTensor).to(dev), self.state_process_prior_0.unsqueeze(-1).type(torch.DoubleTensor).to(dev)).squeeze(-1)\n",
        "        self.obs_process_0 = bmm_mul\n",
        "\n",
        "        # Predict the 1-st moment of x\n",
        "        self.m1x_prev_prior = self.m1x_prior.squeeze()\n",
        "        bmm_mul = torch.bmm(self.F.expand(self.m1x_posterior.size()[0], -1, -1).type(torch.DoubleTensor).to(dev), self.m1x_posterior.unsqueeze(-1).type(torch.DoubleTensor).to(dev)).squeeze(-1)\n",
        "        self.m1x_prior = bmm_mul\n",
        "\n",
        "        # Predict the 1-st moment of y\n",
        "        bmm_mul = torch.bmm(self.H.expand(self.m1x_prior.size()[0], -1, -1).type(torch.DoubleTensor).to(dev), self.m1x_prior.unsqueeze(-1).type(torch.DoubleTensor).to(dev)).squeeze(-1)\n",
        "        self.m1y = bmm_mul\n",
        "\n",
        "\n",
        "    ##############################\n",
        "    ### Kalman Gain Estimation ###\n",
        "    ##############################\n",
        "    def step_KGain_est(self, y):\n",
        "\n",
        "        # Reshape and Normalize the difference in X prior\n",
        "        # Featture 4: x_t|t - x_t|t-1\n",
        "        #dm1x = self.m1x_prior - self.state_process_prior_0\n",
        "        dm1x = self.m1x_posterior - self.m1x_prev_prior\n",
        "        dm1x_reshape = torch.squeeze(dm1x)\n",
        "        dm1x_norm = func.normalize(dm1x_reshape, p=2, dim=0, eps=1e-12, out=None)\n",
        "\n",
        "        # Feature 2: yt - y_t+1|t\n",
        "        dm1y = y.squeeze() - torch.squeeze(self.m1y)\n",
        "        dm1y_norm = func.normalize(dm1y, p=2, dim=0, eps=1e-12, out=None)\n",
        "\n",
        "        # KGain Net Input\n",
        "        KGainNet_in = torch.cat([dm1y_norm, dm1x_norm], dim=1)\n",
        "\n",
        "        # Kalman Gain Network Step\n",
        "        KG = self.KGain_step(KGainNet_in)\n",
        "        # Reshape Kalman Gain to a Matrix\n",
        "        self.KGain = torch.reshape(KG, (-1,self.m, self.n))\n",
        "\n",
        "    #######################\n",
        "    ### Kalman Net Step ###\n",
        "    #######################\n",
        "    def KNet_step(self, y):\n",
        "        # Compute Priors\n",
        "        self.step_prior()\n",
        "\n",
        "        # Compute Kalman Gain\n",
        "        self.step_KGain_est(y)\n",
        "\n",
        "        # Innovation\n",
        "        y_obs = torch.squeeze(y)\n",
        "        dy = y_obs - self.m1y\n",
        "        # Compute the 1-st posterior moment\n",
        "        # bmm_mul = torch.bmm(self.F.expand(self.state_process_posterior_0.size()[0], -1, -1).type(torch.DoubleTensor).to(dev), self.state_process_posterior_0.unsqueeze(-1).type(torch.DoubleTensor).to(dev)).squeeze(-1)\n",
        "        INOV = torch.matmul(self.KGain.float(), dy.unsqueeze(-1).float()).squeeze()\n",
        "        self.m1x_posterior = self.m1x_prior + INOV\n",
        "\n",
        "        # return\n",
        "        return torch.squeeze(self.m1x_posterior)\n",
        "\n",
        "    ########################\n",
        "    ### Kalman Gain Step ###\n",
        "    ########################\n",
        "    def KGain_step(self, KGainNet_in):\n",
        "\n",
        "        ###################\n",
        "        ### Input Layer ###\n",
        "        ###################\n",
        "        L1_out = self.KG_l1(KGainNet_in.type(torch.FloatTensor).to(dev))\n",
        "        La1_out = self.KG_relu1(L1_out)\n",
        "\n",
        "        ###########\n",
        "        ### GRU ###\n",
        "        ###########\n",
        "        GRU_in = torch.empty(self.seq_len_input, self.batch_size, self.input_dim).to(self.device,non_blocking = True)\n",
        "        GRU_in[0, :, :] = La1_out\n",
        "        GRU_out, self.hn = self.rnn_GRU(GRU_in, self.hn)\n",
        "        # GRU_out_reshape = torch.reshape(GRU_out, (1, self.hidden_dim))\n",
        "\n",
        "        ####################\n",
        "        ### Hidden Layer ###\n",
        "        ####################\n",
        "        L2_out = self.KG_l2(GRU_out)\n",
        "        La2_out = self.KG_relu2(L2_out)\n",
        "\n",
        "        ####################\n",
        "        ### Output Layer ###\n",
        "        ####################\n",
        "        L3_out = self.KG_l3(La2_out)\n",
        "        return L3_out\n",
        "\n",
        "    ###############\n",
        "    ### Forward ###\n",
        "    ###############\n",
        "    def forward(self, yt):\n",
        "        yt = yt.to(self.device,non_blocking = True)\n",
        "        return self.KNet_step(yt)\n",
        "\n",
        "    #########################\n",
        "    ### Init Hidden State ###\n",
        "    #########################\n",
        "    def init_hidden(self):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, self.batch_size, self.hidden_dim).zero_()\n",
        "        self.hn = hidden.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcQ0KVQwyb8Z"
      },
      "source": [
        "# parameters #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-OI3YhwUCfss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a53fe6dc-0aef-4d6f-ae6c-a8cc68b88c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on the GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-3487eb0e6385>:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  L = torch.tensor(L).type(torch.FloatTensor).to(dev)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "PFandUKF_test = False\n",
        "if torch.cuda.is_available() and not PFandUKF_test:\n",
        "    dev = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n",
        "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    dev = torch.device(\"cpu\")\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    print(\"Running on the CPU\")\n",
        "\n",
        "r2 = 0.21\n",
        "vdB = -20  # ratio v=q2/r2\n",
        "v = 10 ** (vdB / 10)\n",
        "q2 = torch.mul(v, r2)\n",
        "q = torch.sqrt(q2)\n",
        "\n",
        "T = 200 # <=200\n",
        "T_test = T\n",
        "m = 10\n",
        "n = 10\n",
        "F = torch.eye(10).to(dev)\n",
        "H = torch.diag(torch.tensor([0.1, 0.2, 0.3, 0.1, 0.2, 0.3, 0.1, 0.2, 0.3, 0.4])).to(dev)\n",
        "m1x_0 = torch.ones(m, 1).to(dev)\n",
        "m2x_0 = 0 * 0 * torch.eye(m).to(dev)\n",
        "\n",
        "\n",
        "# G = nx.random_regular_graph(3, 10)                         # the graph of the network\n",
        "# L = nx.laplacian_matrix(G).toarray()\n",
        "# print('L', L)\n",
        "L = torch.Tensor([[3, -1,  0,  0,  0,  0, -1,  0,  0, -1],\n",
        "                  [-1, 2, 0,  0,  0,  0,  0,  0, -1,  0],\n",
        "                  [0, 0,  2, -1,  0,  0,  0, -1,  0,  0],\n",
        "                  [0,  0, -1,  3, -1, -1,  0,  0,  0,  0],\n",
        "                  [0,  0,  0, -1,  3,  0,  0, -1, -1,  0],\n",
        "                  [0,  0,  0, -1,  0,  3,  0,  0, -1, -1],\n",
        "                  [-1,  0,  0,  0,  0,  0,  3, -1,  0, -1],\n",
        "                  [0,  0, -1,  0, -1,  0, -1,  3,  0,  0],\n",
        "                  [0, -1,  0,  0, -1, -1,  0,  0,  3,  0],\n",
        "                  [-1,  0,  0,  0,  0, -1, -1,  0,  0,  3]]).to(dev)\n",
        "\n",
        "# L = torch.Tensor([[6, -1, 0, -1, -1, -1, -1, 0, 0, -1],\n",
        "#                   [-1, 6, 0, -1, 0, -1, -1, -1, 0, -1],\n",
        "#                   [0, 0, 6, -1, -1, -1, 0, -1, -1, -1],\n",
        "#                   [-1, -1, -1, 6, -1, 0, -1, 0, 0, -1],\n",
        "#                   [-1, 0, -1, -1, 6, 0, -1, -1, -1, 0],\n",
        "#                   [-1, -1, -1, 0, 0, 6, 0, -1, -1, -1],\n",
        "#                   [-1, -1, 0, -1, -1, 0, 6, -1, -1, 0],\n",
        "#                   [0, -1, -1, 0, -1, -1, -1, 6, -1, 0],\n",
        "#                   [0, 0, -1, 0, -1, -1, -1, -1, 6, -1],\n",
        "#                   [-1, -1, -1, -1, 0, -1, 0, 0, -1, 6]])\n",
        "\n",
        "\n",
        "# # print(\"L \\n\",L)                                  # the laplacian matrix of the graph\n",
        "# L = torch.eye(10)\n",
        "W, V = np.linalg.eig(L.cpu())\n",
        "V = torch.from_numpy(V).type(torch.FloatTensor).to(dev)\n",
        "V_t = torch.transpose(V, 0, 1).to(dev)\n",
        "L = torch.tensor(L).type(torch.FloatTensor).to(dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkekjBheyf3n"
      },
      "source": [
        "# Model #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6KZMvJfBAcXm"
      },
      "outputs": [],
      "source": [
        "\n",
        "torch.pi = torch.acos(torch.zeros(1)).item() * 2  # which is 3.1415927410125732\n",
        "# from filing_paths import path_model\n",
        "import sys\n",
        "\n",
        "# sys.path.insert(1, path_model)\n",
        "\n",
        "def f(x):\n",
        "    # B = x.size()[0]\n",
        "    # if len(x.shape) != 3:\n",
        "    #   x = x.unsqueeze(-1)\n",
        "    # temp = torch.bmm(F.expand(B, -1, -1).type(torch.DoubleTensor).to(dev), x.type(torch.DoubleTensor).to(dev)).squeeze(-1)\n",
        "    # return temp.reshape(-1,10,1).type(torch.FloatTensor).to(dev)\n",
        "    return torch.matmul(F.type(torch.DoubleTensor), x.type(torch.DoubleTensor))\n",
        "\n",
        "def h(x):\n",
        "    # B = x.size()[0]\n",
        "    # if len(x.shape) != 3:\n",
        "    #   x = x.unsqueeze(-1)\n",
        "    # temp = torch.bmm(H.expand(B, -1, -1).type(torch.DoubleTensor).to(dev), x.type(torch.DoubleTensor).to(dev)).squeeze(-1)\n",
        "    # return temp.reshape(-1,10,1).type(torch.FloatTensor).to(dev)\n",
        "    return torch.matmul(H.type(torch.DoubleTensor), x.type(torch.DoubleTensor))\n",
        "\n",
        "def Naive(x):\n",
        "    H_inv = torch.inverse(H.to(dev))\n",
        "    return torch.matmul(H_inv.type(torch.DoubleTensor), x.type(torch.DoubleTensor)).to(dev)\n",
        "\n",
        "def getJacobian(x, a):\n",
        "    try:\n",
        "        if (x.size()[1] == 1):\n",
        "            y = torch.reshape((x.T), [x.size()[0]])\n",
        "    except:\n",
        "        y = torch.reshape((x.T), [x.size()[0]])\n",
        "\n",
        "    if (a == 'ObsAcc'):\n",
        "        g = h\n",
        "    elif (a == 'ModAcc'):\n",
        "        g = f\n",
        "\n",
        "    Jac = autograd.functional.jacobian(g, y)\n",
        "    Jac = Jac.view(-1, m)\n",
        "\n",
        "    return Jac.to(dev)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hqyhAyByj5k"
      },
      "source": [
        "# EKF #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3std74jPAQmM"
      },
      "outputs": [],
      "source": [
        "# from filing_paths import path_model\n",
        "# import sys\n",
        "#\n",
        "# sys.path.insert(1, path_model)\n",
        "# print(sys.path)\n",
        "class ExtendedKalmanFilter:\n",
        "\n",
        "    def __init__(self, SystemModel, equation=13, model='regular', mode='full'):\n",
        "        self.L = SystemModel.L.to(dev)\n",
        "        self.V = SystemModel.V.to(dev)\n",
        "        self.V_t = SystemModel.V_t.to(dev)\n",
        "        self.f = SystemModel.f\n",
        "        self.m = SystemModel.m\n",
        "        self.r = SystemModel.r\n",
        "\n",
        "        # Has to be transformed because of EKF non-linearity\n",
        "        self.Q = SystemModel.Q\n",
        "        self.Q_gsp = torch.matmul(self.V_t.to(dev), self.Q.to(dev))\n",
        "        self.Q_gsp = torch.matmul(self.Q_gsp.to(dev), self.V.to(dev)).to(dev)\n",
        "\n",
        "        self.h = SystemModel.h\n",
        "        self.n = SystemModel.n\n",
        "\n",
        "        # Has to be transofrmed because of EKF non-linearity\n",
        "        self.R = SystemModel.R\n",
        "        self.R_gsp = torch.matmul(self.V_t.to(dev), self.R.to(dev))  # add\n",
        "        self.R_gsp = torch.matmul(self.R_gsp.to(dev), self.V).to(dev)   # add\n",
        "\n",
        "        self.T = SystemModel.T\n",
        "        self.T_test = SystemModel.T_test\n",
        "\n",
        "        # Pre allocate KG array\n",
        "        self.KG_array = torch.zeros((self.T_test, self.m, self.n)).to(dev)\n",
        "        self.model = model\n",
        "        self.equation = equation\n",
        "        # Full knowledge about the model or partial? (Should be made more elegant)\n",
        "        if (mode == 'full'):\n",
        "            self.fString = 'ModAcc'\n",
        "            self.hString = 'ObsAcc'\n",
        "        elif (mode == 'partial'):\n",
        "            self.fString = 'ModInacc'\n",
        "            self.hString = 'ObsInacc'\n",
        "\n",
        "    # Predict\n",
        "    def Predict(self):\n",
        "        # Predict the 1-st moment of x\n",
        "        self.m1x_prior = torch.matmul(self.V_t.float().to(dev), torch.squeeze(self.f(torch.matmul(self.V.float().to(dev), self.m1x_posterior.float().to(dev))).float()).to(dev))  # X_hat t|t-1 (equation 21)\n",
        "        # Compute the Jacobians\n",
        "        # if self.model == 'pypower':\n",
        "        #     self.UpdateJacobians(pypower_getJacobian(torch.matmul(self.V, self.m1x_posterior), self.fString),\n",
        "        #                          pypower_getJacobian(torch.matmul(self.V, self.m1x_prior), self.hString))\n",
        "        if self.model == 'NonLinear':\n",
        "            self.UpdateJacobians(pypower_getJacobian(torch.matmul(self.V, self.m1x_posterior.float().to(dev)), self.fString),\n",
        "                                 pypower_getJacobian(torch.matmul(self.V, self.m1x_prior.float().to(dev)), self.hString))\n",
        "        else:\n",
        "            self.UpdateJacobians(pypower_getJacobian(torch.matmul(self.V, self.m1x_posterior.float().to(dev)), self.fString).to(dev),\n",
        "                                 pypower_getJacobian(torch.matmul(self.V, self.m1x_prior.float().to(dev)), self.hString).to(dev))\n",
        "\n",
        "\n",
        "        # Predict the 2-nd moment of x\n",
        "        self.m2x_prior = torch.matmul(self.F.to(dev), self.m2x_posterior.to(dev)).to(dev)\n",
        "        self.m2x_prior = torch.matmul(self.m2x_prior.to(dev), self.F_T.to(dev)) + self.Q_gsp   # Pt|t-1 (equation 22)\n",
        "\n",
        "        # Predict the 1-st moment of y\n",
        "        temp = self.h(torch.matmul(self.V, self.m1x_prior.float()).float()).float()\n",
        "        self.m1y = torch.squeeze(torch.matmul(self.V_t, torch.squeeze(temp.to(dev))))  # calc for equation 23\n",
        "        # Predict the 2-nd moment of y\n",
        "        self.m2y = torch.matmul(self.H.to(dev), self.m2x_prior)\n",
        "        self.m2y = torch.matmul(self.m2y, self.H_T.to(dev)) + self.R_gsp                               # S (13 in article) calc for KG equation 20\n",
        "\n",
        "    # Compute the Kalman Gain\n",
        "    def KGain(self):\n",
        "        if self.equation == 13:\n",
        "            self.KG = torch.matmul(self.m2x_prior, self.H_T.to(dev))\n",
        "            self.KG = torch.matmul(self.KG, torch.inverse(self.m2y+epsilon))    # equation 13\n",
        "        else:\n",
        "            self.KG = torch.diag(torch.diagonal(torch.matmul(self.m2x_prior, self.H_T.to(dev))))\n",
        "            self.KG = torch.matmul(self.KG,torch.inverse(epsilon+torch.diag(torch.diagonal(self.m2y))))    # equation 20\n",
        "        # Save KalmanGain\n",
        "        self.KG_array[self.i] = self.KG\n",
        "        self.i += 1\n",
        "\n",
        "    # Innovation\n",
        "    def Innovation(self, y):\n",
        "        self.dy = y - self.m1y\n",
        "\n",
        "    # Compute Posterior\n",
        "    def Correct(self):\n",
        "        # Compute the 1-st posterior moment\n",
        "        self.m1x_posterior = self.m1x_prior + torch.matmul(self.KG.to(dev), self.dy.to(dev)).to(dev)   # X_hat_t|t (equation 23)\n",
        "\n",
        "        # Compute the 2-nd posterior moment\n",
        "        # self.m2x_posterior = torch.matmul(self.m2y, torch.transpose(self.KG, 0, 1))\n",
        "        # self.m2x_posterior = self.m2x_prior - torch.matmul(self.KG, self.m2x_posterior)\n",
        "        I = torch.eye(torch.matmul(self.KG.to(dev), self.H.to(dev)).size()[1]).to(dev) - torch.matmul(self.KG.to(dev), self.H.to(dev)).to(dev)     # I - KG*H\n",
        "        self.m2x_posterior = torch.matmul(I, self.m2x_prior.to(dev)).to(dev)                                      # (I - KG*H)*P_t|t-1\n",
        "        self.m2x_posterior = torch.matmul(self.m2x_posterior.to(dev), torch.transpose(I, 0, 1)).to(dev)            # (I - KG*H)* P_t|t-1 * (I - KG*H)^T  (equation 24)\n",
        "        self.m2x_posterior = self.m2x_posterior.to(dev) + torch.matmul(self.KG.to(dev), torch.matmul(self.R_gsp.to(dev), torch.transpose(self.KG.to(dev), 0, 1).to(dev)).to(dev))\n",
        "\n",
        "    def Update(self, y):\n",
        "        self.Predict()\n",
        "        self.KGain()\n",
        "        self.Innovation(y)\n",
        "        self.Correct()\n",
        "\n",
        "        return self.m1x_posterior, self.m2x_posterior\n",
        "\n",
        "    def InitSequence(self, m1x_0, m2x_0):\n",
        "        self.m1x_0 = torch.matmul(self.V_t.type(torch.DoubleTensor), m1x_0.type(torch.DoubleTensor))\n",
        "        self.m2x_0 = self.GFT(m2x_0)\n",
        "\n",
        "        #########################\n",
        "\n",
        "    def UpdateJacobians(self, F1, H1):\n",
        "        self.F = self.GFT(F1)\n",
        "        self.F_T = torch.transpose(self.F, 0, 1)\n",
        "        # print('H1', H1.shape)\n",
        "        self.H = self.GFT(H1)\n",
        "        self.H_T = torch.transpose(self.H, 0, 1)\n",
        "        if torch.isnan(self.F).any():\n",
        "            print(\"Variable F contains NaN values.\")\n",
        "        if torch.isnan(self.H).any():\n",
        "            print(\"Variable H contains NaN values.\")\n",
        "\n",
        "    ### Generate Sequence ###\n",
        "    #########################\n",
        "    def GenerateSequence(self, y, T):\n",
        "        # Pre allocate an array for predicted state and variance\n",
        "        self.x = torch.zeros(size=[self.m, T])\n",
        "        self.sigma = torch.zeros(size=[self.m, self.m, T])\n",
        "        # Pre allocate KG array\n",
        "        self.KG_array = torch.zeros((T, self.m, self.n))\n",
        "        self.i = 0  # Index for KG_array alocation\n",
        "\n",
        "        self.m1x_posterior = torch.squeeze(self.m1x_0)\n",
        "        self.m2x_posterior = torch.squeeze(self.m2x_0)\n",
        "\n",
        "        for t in range(0, T):\n",
        "            # print(y.shape,self.V_t.shape)\n",
        "            yt = torch.squeeze(torch.matmul(self.V_t, y[:, t].float()).to(dev)) # + (torch.randn(self.m) * self.r).to(dev)\n",
        "            xt, sigmat = self.Update(yt)\n",
        "            self.x[:, t] = torch.squeeze(torch.matmul(self.V, xt))\n",
        "            self.sigma[:, :, t] = torch.squeeze(sigmat)\n",
        "\n",
        "    def GFT(self, x):\n",
        "        return torch.matmul(self.V_t,torch.matmul(x.to(dev), self.V))\n",
        "\n",
        "    def IGFT(self, x):\n",
        "        return torch.matmul(self.V, torch.matmul(x.to(dev), self.V_t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3yKaD0jKCRm"
      },
      "source": [
        "# Pipline **EKF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "l4smmXlZKFPb"
      },
      "outputs": [],
      "source": [
        "class Pipeline_EKF:\n",
        "\n",
        "    def __init__(self, Time, folderName, modelName):\n",
        "        super().__init__()\n",
        "        self.Time = Time\n",
        "        self.folderName = folderName + '/'\n",
        "        self.modelName = modelName\n",
        "        self.modelFileName = self.folderName + \"model_\" + self.modelName\n",
        "        self.PipelineName = self.folderName + \"pipeline_\" + self.modelName\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self, self.PipelineName)\n",
        "\n",
        "    def setssModel(self, ssModel):\n",
        "        self.ssModel = ssModel\n",
        "\n",
        "    def setModel(self, model,checkpoint):\n",
        "        self.model = model.to('cuda:0')\n",
        "        self.checkpoint = checkpoint\n",
        "        try:\n",
        "          self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "          print(\"pre trained 2\")\n",
        "        except:\n",
        "          print(\"not pre trained1\")\n",
        "\n",
        "    def setTrainingParams(self, n_Epochs, n_Batch, learningRate, weightDecay):\n",
        "        self.N_Epochs = n_Epochs  # Number of Training Epochs\n",
        "        self.N_B = n_Batch  # Number of Samples in Batch\n",
        "        self.learningRate = learningRate  # Learning Rate\n",
        "        self.weightDecay = weightDecay  # L2 Weight Regularization - Weight Decay\n",
        "\n",
        "        # MSE LOSS Function\n",
        "        self.loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "        # Use the optim package to define an Optimizer that will update the weights of\n",
        "        # the model for us. Here we will use Adam; the optim package contains many other\n",
        "        # optimization algoriths. The first argument to the Adam constructor tells the\n",
        "        # optimizer which Tensors it should update.\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learningRate, weight_decay=self.weightDecay)\n",
        "\n",
        "    def NNTrain(self, n_Examples, train_input, train_target, n_CV, cv_input, cv_target):\n",
        "        try:\n",
        "          self.optimizer.load_state_dict(self.checkpoint['optimizer_state_dict'])\n",
        "          print(\"pre trained 2\")\n",
        "        except:\n",
        "          print('not loading2')\n",
        "        # before training the model\n",
        "        # try:\n",
        "\n",
        "        # except:\n",
        "        #   print(\"Did not load pre-trained model!\")\n",
        "\n",
        "        self.N_E = n_Examples\n",
        "        self.N_CV = n_CV\n",
        "\n",
        "        MSE_cv_linear_batch = torch.zeros([self.N_CV])\n",
        "        self.MSE_cv_linear_epoch = torch.zeros([self.N_Epochs])\n",
        "        self.MSE_cv_dB_epoch = torch.zeros([self.N_Epochs])\n",
        "\n",
        "        MSE_train_linear_batch = torch.zeros([self.N_B])\n",
        "        self.MSE_train_linear_epoch = torch.zeros([self.N_Epochs])\n",
        "        self.MSE_train_dB_epoch = torch.zeros([self.N_Epochs])\n",
        "\n",
        "        ##############\n",
        "        ### Epochs ###\n",
        "        ##############\n",
        "\n",
        "        self.MSE_cv_dB_opt = 1000\n",
        "        self.MSE_cv_idx_opt = 0\n",
        "\n",
        "        for ti in range(0, self.N_Epochs):\n",
        "\n",
        "            ###############################\n",
        "            ### Training Sequence Batch ###\n",
        "            ###############################\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Training Mode\n",
        "            self.model.train()\n",
        "            self.model.batch_size = self.N_B\n",
        "\n",
        "            # Init Hidden State\n",
        "            self.model.init_hidden()\n",
        "\n",
        "            # Init Training Batch tensors\n",
        "            y_training_batch = torch.zeros([self.N_B, self.ssModel.n, self.ssModel.T]).to(dev)\n",
        "            train_target_batch = torch.zeros([self.N_B, self.ssModel.m, self.ssModel.T]).to(dev)\n",
        "            x_out_training_batch = torch.zeros([self.N_B, self.ssModel.m, self.ssModel.T]).to(dev)\n",
        "            Batch_Optimizing_LOSS_sum = 0\n",
        "            check = True\n",
        "\n",
        "            # Randomly select N_B training sequences\n",
        "            n_e = random.sample(range(self.N_E), k=self.N_B)\n",
        "            ii = 0\n",
        "            for index in n_e:\n",
        "                y_training_batch[ii,:,:] = train_input[index]\n",
        "                train_target_batch[ii,:,:] = train_target[index]\n",
        "                ii += 1\n",
        "\n",
        "            self.model.InitSequence(\\\n",
        "                self.ssModel.m1x_0.reshape(1,self.ssModel.m,1).repeat(self.N_B,1,1), self.ssModel.T)\n",
        "\n",
        "            MSE_trainbatch_linear_LOSS = 0\n",
        "            # Forward Computation\n",
        "            for t in range(0, self.ssModel.T):\n",
        "              x_out_training_batch[:, :, t] = torch.squeeze(self.model(torch.unsqueeze(y_training_batch[:, :, t],2)))\n",
        "            MSE_trainbatch_linear_LOSS = self.loss_fn(x_out_training_batch, train_target_batch)\n",
        "\n",
        "            # dB Loss\n",
        "            self.MSE_train_linear_epoch[ti] = MSE_trainbatch_linear_LOSS.item()\n",
        "            self.MSE_train_dB_epoch[ti] = 10 * torch.log10(self.MSE_train_linear_epoch[ti])\n",
        "\n",
        "            ##################\n",
        "            ### Optimizing ###\n",
        "            ##################\n",
        "\n",
        "            # Before the backward pass, use the optimizer object to zero all of the\n",
        "            # gradients for the variables it will update (which are the learnable\n",
        "            # weights of the model). This is because by default, gradients are\n",
        "            # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "            # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "\n",
        "            # Backward pass: compute gradient of the loss with respect to model\n",
        "            # parameters\n",
        "            MSE_trainbatch_linear_LOSS.backward(retain_graph=True)\n",
        "            # torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1)\n",
        "            # Calling the step function on an Optimizer makes an update to its\n",
        "            # parameters\n",
        "            self.optimizer.step()\n",
        "            # self.scheduler.step(self.MSE_cv_dB_epoch[ti])\n",
        "\n",
        "            #################################\n",
        "            ### Validation Sequence Batch ###\n",
        "            #################################\n",
        "\n",
        "            # Cross Validation Mode\n",
        "            self.model.eval()\n",
        "            self.model.batch_size = self.N_CV\n",
        "            # Init Hidden State\n",
        "            self.model.init_hidden()\n",
        "            with torch.no_grad():\n",
        "\n",
        "                self.ssModel.T_test = cv_input.size()[-1] # T_test is the maximum length of the CV sequences\n",
        "                x_out_cv_batch = torch.empty([self.N_CV, self.ssModel.m, self.ssModel.T_test]).to(dev)\n",
        "\n",
        "                # Init Sequence\n",
        "                self.model.InitSequence(\\\n",
        "                    self.ssModel.m1x_0.reshape(1,self.ssModel.m,1).repeat(self.N_CV,1,1), self.ssModel.T_test)\n",
        "\n",
        "                for t in range(0, self.ssModel.T_test):\n",
        "                    x_out_cv_batch[:, :, t] = torch.squeeze(self.model(torch.unsqueeze(cv_input[:, :, t],2)))\n",
        "\n",
        "                # Compute CV Loss\n",
        "                MSE_cvbatch_linear_LOSS = 0\n",
        "                MSE_cvbatch_linear_LOSS = self.loss_fn(x_out_cv_batch, cv_target)\n",
        "\n",
        "                # dB Loss\n",
        "                self.MSE_cv_linear_epoch[ti] = MSE_cvbatch_linear_LOSS.item()\n",
        "                self.MSE_cv_dB_epoch[ti] = 10 * torch.log10(self.MSE_cv_linear_epoch[ti])\n",
        "\n",
        "                if (self.MSE_cv_dB_epoch[ti] < self.MSE_cv_dB_opt):\n",
        "                    self.MSE_cv_dB_opt = self.MSE_cv_dB_epoch[ti]\n",
        "                    self.MSE_cv_idx_opt = ti\n",
        "\n",
        "                    # torch.save(self.model, path_results + 'best-model.pt')\n",
        "\n",
        "            ########################\n",
        "            ### Training Summary ###\n",
        "            ########################\n",
        "            print(ti, \"MSE Training :\", self.MSE_train_dB_epoch[ti], \"[dB]\", \"MSE Validation :\", self.MSE_cv_dB_epoch[ti],\n",
        "                  \"[dB]\")\n",
        "\n",
        "            if (ti > 1):\n",
        "                d_train = self.MSE_train_dB_epoch[ti] - self.MSE_train_dB_epoch[ti - 1]\n",
        "                d_cv = self.MSE_cv_dB_epoch[ti] - self.MSE_cv_dB_epoch[ti - 1]\n",
        "                print(\"diff MSE Training :\", d_train, \"[dB]\", \"diff MSE Validation :\", d_cv, \"[dB]\")\n",
        "            print(\"Optimal idx:\", self.MSE_cv_idx_opt, \"Optimal :\", self.MSE_cv_dB_opt, \"[dB]\")\n",
        "        self.plot_learning_curve()\n",
        "\n",
        "        #     for j in range(0, self.N_B):\n",
        "        #         # n_e = random.randint(0, self.N_E - 1)\n",
        "        #         random_traj = random.sample(range(self.N_E+1), self.N_B)\n",
        "        #         index_tensor = torch.tensor(random_traj)\n",
        "        #         y_training = train_input[index_tensor, :, :]\n",
        "        #         # self.model.InitSequence(self.ssModel.m1x_0, self.ssModel.T)\n",
        "        #         self.model.InitSequence(\\\n",
        "        #         self.ssModel.m1x_0.reshape(1,self.ssModel.m,1).repeat(self.N_B,1,1), self.ssModel.T)\n",
        "        #         x_out_training = torch.zeros([self.N_B, self.ssModel.m, self.ssModel.T]).to(dev)\n",
        "        #         for t in range(0, self.ssModel.T):\n",
        "        #             # x_out_training[:, t] = self.model(y_training[:, t])\n",
        "        #             x_out_training[:, :, t] = torch.squeeze(self.model(torch.unsqueeze(y_training[:, :, t],2)))\n",
        "        #         # Compute Training Loss\n",
        "        #         LOSS = self.loss_fn(x_out_training.to(dev), train_target[index_tensor, :, :])\n",
        "        #         if torch.isnan(LOSS) and check:\n",
        "        #           print(y_training[:, t])\n",
        "        #           check = False\n",
        "        #         MSE_train_linear_batch[j] = LOSS.item()\n",
        "        #         Batch_Optimizing_LOSS_sum = Batch_Optimizing_LOSS_sum + LOSS\n",
        "\n",
        "        #     # Average\n",
        "        #     self.MSE_train_linear_epoch[ti] = torch.mean(MSE_train_linear_batch)\n",
        "        #     self.MSE_train_dB_epoch[ti] = 10 * torch.log10(self.MSE_train_linear_epoch[ti])\n",
        "\n",
        "        #     ##################\n",
        "        #     ### Optimizing ###\n",
        "        #     ##################\n",
        "\n",
        "        #     # Before the backward pass, use the optimizer object to zero all of the\n",
        "        #     # gradients for the variables it will update (which are the learnable\n",
        "        #     # weights of the model). This is because by default, gradients are\n",
        "        #     # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "        #     # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "        #     # self.optimizer.zero_grad()\n",
        "\n",
        "        #     Batch_Optimizing_LOSS_sum = Batch_Optimizing_LOSS_sum\n",
        "        #     # Backward pass: compute gradient of the loss with respect to model\n",
        "        #     # parameters\n",
        "        #     # Batch_Optimizing_LOSS_mean = (Batch_Optimizing_LOSS_sum.float() / self.N_B)\n",
        "        #     Batch_Optimizing_LOSS_mean = (Batch_Optimizing_LOSS_sum / self.N_B)\n",
        "        #     Batch_Optimizing_LOSS_mean.backward()\n",
        "        #     ### gradient clipping ###\n",
        "        #     # nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1000)\n",
        "        #     #########################\n",
        "        #     # Calling the step function on an Optimizer makes an update to its\n",
        "        #     # parameters\n",
        "        #     self.optimizer.step()\n",
        "\n",
        "        #     ########################\n",
        "        #     ### Training Summary ###\n",
        "        #     ########################\n",
        "        #     print(ti, \"MSE Training :\", self.MSE_train_dB_epoch[ti], \"[dB]\", \"MSE Validation :\",\n",
        "        #           self.MSE_cv_dB_epoch[ti],\n",
        "        #           \"[dB]\")\n",
        "\n",
        "        #     if (ti > 1):\n",
        "        #         d_train = self.MSE_train_dB_epoch[ti] - self.MSE_train_dB_epoch[ti - 1]\n",
        "        #         d_cv = self.MSE_cv_dB_epoch[ti] - self.MSE_cv_dB_epoch[ti - 1]\n",
        "        #         print(\"diff MSE Training :\", d_train, \"[dB]\", \"diff MSE Validation :\", d_cv, \"[dB]\")\n",
        "\n",
        "        #     print(\"Optimal idx:\", self.MSE_cv_idx_opt, \"Optimal :\", self.MSE_cv_dB_opt, \"[dB]\")\n",
        "        # self.plot_learning_curve()\n",
        "\n",
        "    def NNTest(self,n_Test ,test_input, test_target):\n",
        "        # Load model\n",
        "        # if load_model:\n",
        "        #     self.model = torch.load(load_model_path, map_location=self.device)\n",
        "        # else:\n",
        "        #     self.model = torch.load(path_results+'best-model.pt', map_location=self.device)\n",
        "\n",
        "        self.N_T = test_input.shape[0]\n",
        "        # SysModel.T_test = test_input.size()[-1]\n",
        "        self.MSE_test_linear_arr = torch.zeros([self.N_T])\n",
        "        SysModel = self.ssModel\n",
        "        x_out_test = torch.zeros([self.N_T, SysModel.m,SysModel.T_test]).to(dev)\n",
        "\n",
        "        # if MaskOnState:\n",
        "        #     mask = torch.tensor([True,False,False])\n",
        "        #     if SysModel.m == 2:\n",
        "        #         mask = torch.tensor([True,False])\n",
        "\n",
        "        # MSE LOSS Function\n",
        "        loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "        # Test mode\n",
        "        self.model.eval()\n",
        "        self.model.batch_size = self.N_T\n",
        "        # Init Hidden State\n",
        "        self.model.init_hidden()\n",
        "        torch.no_grad()\n",
        "\n",
        "        start = time.time()\n",
        "        self.model.InitSequence(SysModel.m1x_0.reshape(1,SysModel.m,1).repeat(self.N_T,1,1), SysModel.T_test)\n",
        "\n",
        "        for t in range(0, SysModel.T_test):\n",
        "            x_out_test[:,:, t] = torch.squeeze(self.model(torch.unsqueeze(test_input[:,:, t],2)))\n",
        "\n",
        "        end = time.time()\n",
        "        t = end - start\n",
        "\n",
        "        # MSE loss\n",
        "        # print(\"Estimation: \",x_out_test[5,:,SysModel.T_test-1])\n",
        "        # print(\"Target: \",test_target[5,:,SysModel.T_test-1])\n",
        "        for j in range(self.N_T):# cannot use batch due to different length and std computation\n",
        "          self.MSE_test_linear_arr[j] = loss_fn(x_out_test[j,:,:], test_target[j,:,:]).item()\n",
        "\n",
        "        # Average\n",
        "        self.MSE_test_linear_avg = torch.mean(self.MSE_test_linear_arr)\n",
        "        self.MSE_test_dB_avg = 10 * torch.log10(self.MSE_test_linear_avg)\n",
        "\n",
        "        # Standard deviation\n",
        "        self.MSE_test_linear_std = torch.std(self.MSE_test_linear_arr, unbiased=True)\n",
        "\n",
        "        # Confidence interval\n",
        "        self.test_std_dB = 10 * torch.log10(self.MSE_test_linear_std + self.MSE_test_linear_avg) - self.MSE_test_dB_avg\n",
        "\n",
        "        # Print MSE and std\n",
        "        str = self.modelName + \"-\" + \"MSE Test:\"\n",
        "        print(str, self.MSE_test_dB_avg, \"[dB]\")\n",
        "        str = self.modelName + \"-\" + \"STD Test:\"\n",
        "        print(str, self.test_std_dB, \"[dB]\")\n",
        "        # Print Run Time\n",
        "        print(\"Inference Time:\", t)\n",
        "\n",
        "        return [self.MSE_test_linear_arr, self.MSE_test_linear_avg, self.MSE_test_dB_avg, x_out_test, t]\n",
        "\n",
        "    # def NNTest(self, n_Test, test_input, test_target):\n",
        "\n",
        "    #     self.N_T = n_Test\n",
        "\n",
        "    #     self.MSE_test_linear_arr = torch.empty([self.N_T])\n",
        "    #     self.MSE_test_per_iter = torch.zeros(test_target.shape[2])\n",
        "    #     self.MSE_finalize = torch.zeros(test_target.shape[2])\n",
        "    #     # MSE LOSS Function\n",
        "    #     loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    #     self.model = torch.load(self.modelFileName)\n",
        "\n",
        "    #     self.model.eval()\n",
        "\n",
        "    #     torch.no_grad()\n",
        "\n",
        "    #     x_out_array = torch.empty(self.N_T, self.ssModel.m, self.ssModel.T_test)\n",
        "\n",
        "    #     start = time.time()\n",
        "    #     x_out_test = torch.zeros([self.N_T, self.ssModel.m,self.N_T]).to(dev)\n",
        "    #     for j in range(0, self.N_T):\n",
        "\n",
        "    #         y_mdl_tst = test_input[j, :, :]\n",
        "    #         self.model.InitSequence(self.ssModel.m1x_0, self.ssModel.T_test)\n",
        "    #         # x_out_test = torch.empty(self.ssModel.m, self.ssModel.T_test)\n",
        "    #         for t in range(0, self.ssModel.T_test):\n",
        "    #           x_out_test[j,:, t] = torch.squeeze(self.model(torch.unsqueeze(test_input[j,:, t],2)))\n",
        "    #           self.MSE_test_per_iter[t] = loss_fn(x_out_test[j,:,t],test_target[j,:,t])\n",
        "\n",
        "    #         self.MSE_test_linear_arr[j] = loss_fn(x_out_test, test_target[j, :, :]).item()\n",
        "    #         self.MSE_finalize += self.MSE_test_per_iter\n",
        "    #         x_out_array[j, :, :] = x_out_test\n",
        "\n",
        "    #     end = time.time()\n",
        "    #     t = end - start\n",
        "\n",
        "    #     # Average\n",
        "    #     self.MSE_test_linear_avg = torch.mean(self.MSE_test_linear_arr)\n",
        "    #     self.MSE_test_dB_avg = 10 * torch.log10(self.MSE_test_linear_avg)\n",
        "\n",
        "    #     self.MSE_finalize = self.MSE_finalize/self.N_T\n",
        "    #     self.MSE_finalize_dB = 10 * torch.log10(self.MSE_finalize)\n",
        "\n",
        "    #     # Standard deviation\n",
        "    #     self.MSE_test_dB_std = torch.std(self.MSE_test_linear_arr, unbiased=True)\n",
        "    #     self.MSE_test_dB_std = 10 * torch.log10(self.MSE_test_dB_std)\n",
        "\n",
        "    #     # Print MSE Cross Validation\n",
        "    #     str = self.modelName + \"-\" + \"MSE Test:\"\n",
        "    #     print(str, self.MSE_test_dB_avg, \"[dB]\")\n",
        "    #     str = self.modelName + \"-\" + \"STD Test:\"\n",
        "    #     print(str, self.MSE_test_dB_std, \"[dB]\")\n",
        "    #     # Print Run Time\n",
        "    #     print(\"Inference Time:\", t)\n",
        "\n",
        "    #     return [self.MSE_finalize_dB, self.MSE_test_linear_arr, self.MSE_test_linear_avg, self.MSE_test_dB_avg, x_out_array]\n",
        "\n",
        "    def plot_learning_curve(self):\n",
        "        iters_sub = [i for i in range(self.N_Epochs)]\n",
        "        plt.title(\"Learning Curve: Accuracy per Iteration\")\n",
        "        plt.plot(iters_sub, self.MSE_train_dB_epoch.cpu(), label=\"Train\")\n",
        "        plt.plot(iters_sub, self.MSE_cv_dB_epoch.cpu(), label=\"Validation\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Accuracy - MSE\")\n",
        "        plt.legend(loc='best')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JinM6cEa3jW"
      },
      "source": [
        "# BatchedPipeline EKF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "weu6vzrna7yd"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This file contains the class Pipeline_EKF,\n",
        "which is used to train and test KalmanNet.\n",
        "\"\"\"\n",
        "class Batched_Pipeline_EKF:\n",
        "\n",
        "    def __init__(self, Time, folderName, modelName):\n",
        "        super().__init__()\n",
        "        self.Time = Time\n",
        "        self.folderName = folderName + '/'\n",
        "        self.modelName = modelName\n",
        "        self.modelFileName = self.folderName + \"model_\" + self.modelName + \".pt\"\n",
        "        self.PipelineName = self.folderName + \"pipeline_\" + self.modelName + \".pt\"\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self, self.PipelineName)\n",
        "\n",
        "    def setssModel(self, ssModel):\n",
        "        self.ssModel = ssModel\n",
        "\n",
        "    def setModel(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def setTrainingParams(self, n_Epochs, n_Batch, learningRate, weightDecay, alpha):\n",
        "        self.N_steps = n_Epochs  # Number of Training Steps\n",
        "        self.N_B = n_Batch # Number of Samples in Batch\n",
        "        self.learningRate = learningRate # Learning Rate\n",
        "        self.weightDecay = weightDecay # L2 Weight Regularization - Weight Decay\n",
        "        self.alpha = alpha # Composition loss factor\n",
        "\n",
        "        # MSE LOSS Function\n",
        "        self.loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "        # Use the optim package to define an Optimizer that will update the weights of\n",
        "        # the model for us. Here we will use Adam; the optim package contains many other\n",
        "        # optimization algoriths. The first argument to the Adam constructor tells the\n",
        "        # optimizer which Tensors it should update.\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learningRate, weight_decay=self.weightDecay)\n",
        "\n",
        "    def NNTrain(self, SysModel, cv_input, cv_target, train_input, train_target, path_results, \\\n",
        "        MaskOnState=False, randomInit=False,cv_init=None,train_init=None,\\\n",
        "        train_lengthMask=None,cv_lengthMask=None):\n",
        "\n",
        "        self.N_E = len(train_input)\n",
        "        self.N_CV = len(cv_input)\n",
        "\n",
        "        self.MSE_cv_linear_epoch = torch.zeros([self.N_steps])\n",
        "        self.MSE_cv_dB_epoch = torch.zeros([self.N_steps])\n",
        "\n",
        "        self.MSE_train_linear_epoch = torch.zeros([self.N_steps])\n",
        "        self.MSE_train_dB_epoch = torch.zeros([self.N_steps])\n",
        "\n",
        "        ##############\n",
        "        ### Epochs ###\n",
        "        ##############\n",
        "\n",
        "        self.MSE_cv_dB_opt = 1000\n",
        "        self.MSE_cv_idx_opt = 0\n",
        "\n",
        "        for ti in range(0, self.N_steps):\n",
        "\n",
        "            ###############################\n",
        "            ### Training Sequence Batch ###\n",
        "            ###############################\n",
        "            self.optimizer.zero_grad()\n",
        "            # Training Mode\n",
        "            self.model.train()\n",
        "            self.model.batch_size = self.N_B\n",
        "            # Init Hidden State\n",
        "            self.model.init_hidden_KNet()\n",
        "\n",
        "            # Init Training Batch tensors\n",
        "            y_training_batch = torch.zeros([self.N_B, SysModel.n, SysModel.T]).to(self.device)\n",
        "            train_target_batch = torch.zeros([self.N_B, SysModel.m, SysModel.T]).to(self.device)\n",
        "            x_out_training_batch = torch.zeros([self.N_B, SysModel.m, SysModel.T]).to(self.device)\n",
        "            if self.args.randomLength:\n",
        "                MSE_train_linear_LOSS = torch.zeros([self.N_B])\n",
        "                MSE_cv_linear_LOSS = torch.zeros([self.N_CV])\n",
        "\n",
        "            # Randomly select N_B training sequences\n",
        "            assert self.N_B <= self.N_E # N_B must be smaller than N_E\n",
        "            n_e = random.sample(range(self.N_E), k=self.N_B)\n",
        "            ii = 0\n",
        "            for index in n_e:\n",
        "                if self.args.randomLength:\n",
        "                    y_training_batch[ii,:,train_lengthMask[index,:]] = train_input[index,:,train_lengthMask[index,:]]\n",
        "                    train_target_batch[ii,:,train_lengthMask[index,:]] = train_target[index,:,train_lengthMask[index,:]]\n",
        "                else:\n",
        "                    y_training_batch[ii,:,:] = train_input[index]\n",
        "                    train_target_batch[ii,:,:] = train_target[index]\n",
        "                ii += 1\n",
        "\n",
        "            # Init Sequence\n",
        "            if(randomInit):\n",
        "                train_init_batch = torch.empty([self.N_B, SysModel.m,1]).to(self.device)\n",
        "                ii = 0\n",
        "                for index in n_e:\n",
        "                    train_init_batch[ii,:,0] = torch.squeeze(train_init[index])\n",
        "                    ii += 1\n",
        "                self.model.InitSequence(train_init_batch, SysModel.T)\n",
        "            else:\n",
        "                self.model.InitSequence(\\\n",
        "                SysModel.m1x_0.reshape(1,SysModel.m,1).repeat(self.N_B,1,1), SysModel.T)\n",
        "\n",
        "            # Forward Computation\n",
        "            for t in range(0, SysModel.T):\n",
        "                x_out_training_batch[:, :, t] = torch.squeeze(self.model(torch.unsqueeze(y_training_batch[:, :, t],2)))\n",
        "\n",
        "            # Compute Training Loss\n",
        "            MSE_trainbatch_linear_LOSS = 0\n",
        "            if (self.args.CompositionLoss):\n",
        "                y_hat = torch.zeros([self.N_B, SysModel.n, SysModel.T])\n",
        "                for t in range(SysModel.T):\n",
        "                    y_hat[:,:,t] = torch.squeeze(SysModel.h(torch.unsqueeze(x_out_training_batch[:,:,t])))\n",
        "\n",
        "                if(MaskOnState):### FIXME: composition loss, y_hat may have different mask with x\n",
        "                    if self.args.randomLength:\n",
        "                        jj = 0\n",
        "                        for index in n_e:# mask out the padded part when computing loss\n",
        "                            MSE_train_linear_LOSS[jj] = self.alpha * self.loss_fn(x_out_training_batch[jj,mask,train_lengthMask[index]], train_target_batch[jj,mask,train_lengthMask[index]])+(1-self.alpha)*self.loss_fn(y_hat[jj,mask,train_lengthMask[index]], y_training_batch[jj,mask,train_lengthMask[index]])\n",
        "                            jj += 1\n",
        "                        MSE_trainbatch_linear_LOSS = torch.mean(MSE_train_linear_LOSS)\n",
        "                    else:\n",
        "                        MSE_trainbatch_linear_LOSS = self.alpha * self.loss_fn(x_out_training_batch[:,mask,:], train_target_batch[:,mask,:])+(1-self.alpha)*self.loss_fn(y_hat[:,mask,:], y_training_batch[:,mask,:])\n",
        "                else:# no mask on state\n",
        "                    if self.args.randomLength:\n",
        "                        jj = 0\n",
        "                        for index in n_e:# mask out the padded part when computing loss\n",
        "                            MSE_train_linear_LOSS[jj] = self.alpha * self.loss_fn(x_out_training_batch[jj,:,train_lengthMask[index]], train_target_batch[jj,:,train_lengthMask[index]])+(1-self.alpha)*self.loss_fn(y_hat[jj,:,train_lengthMask[index]], y_training_batch[jj,:,train_lengthMask[index]])\n",
        "                            jj += 1\n",
        "                        MSE_trainbatch_linear_LOSS = torch.mean(MSE_train_linear_LOSS)\n",
        "                    else:\n",
        "                        MSE_trainbatch_linear_LOSS = self.alpha * self.loss_fn(x_out_training_batch, train_target_batch)+(1-self.alpha)*self.loss_fn(y_hat, y_training_batch)\n",
        "\n",
        "            else:# no composition loss\n",
        "                if(MaskOnState):\n",
        "                    if self.args.randomLength:\n",
        "                        jj = 0\n",
        "                        for index in n_e:# mask out the padded part when computing loss\n",
        "                            MSE_train_linear_LOSS[jj] = self.loss_fn(x_out_training_batch[jj,mask,train_lengthMask[index]], train_target_batch[jj,mask,train_lengthMask[index]])\n",
        "                            jj += 1\n",
        "                        MSE_trainbatch_linear_LOSS = torch.mean(MSE_train_linear_LOSS)\n",
        "                    else:\n",
        "                        MSE_trainbatch_linear_LOSS = self.loss_fn(x_out_training_batch[:,mask,:], train_target_batch[:,mask,:])\n",
        "                else: # no mask on state\n",
        "                    if self.args.randomLength:\n",
        "                        jj = 0\n",
        "                        for index in n_e:# mask out the padded part when computing loss\n",
        "                            MSE_train_linear_LOSS[jj] = self.loss_fn(x_out_training_batch[jj,:,train_lengthMask[index]], train_target_batch[jj,:,train_lengthMask[index]])\n",
        "                            jj += 1\n",
        "                        MSE_trainbatch_linear_LOSS = torch.mean(MSE_train_linear_LOSS)\n",
        "                    else:\n",
        "                        MSE_trainbatch_linear_LOSS = self.loss_fn(x_out_training_batch, train_target_batch)\n",
        "\n",
        "            # dB Loss\n",
        "            self.MSE_train_linear_epoch[ti] = MSE_trainbatch_linear_LOSS.item()\n",
        "            self.MSE_train_dB_epoch[ti] = 10 * torch.log10(self.MSE_train_linear_epoch[ti])\n",
        "\n",
        "            ##################\n",
        "            ### Optimizing ###\n",
        "            ##################\n",
        "\n",
        "            # Before the backward pass, use the optimizer object to zero all of the\n",
        "            # gradients for the variables it will update (which are the learnable\n",
        "            # weights of the model). This is because by default, gradients are\n",
        "            # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "            # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "\n",
        "            # Backward pass: compute gradient of the loss with respect to model\n",
        "            # parameters\n",
        "            MSE_trainbatch_linear_LOSS.backward(retain_graph=True)\n",
        "\n",
        "            # Calling the step function on an Optimizer makes an update to its\n",
        "            # parameters\n",
        "            self.optimizer.step()\n",
        "            # self.scheduler.step(self.MSE_cv_dB_epoch[ti])\n",
        "\n",
        "            #################################\n",
        "            ### Validation Sequence Batch ###\n",
        "            #################################\n",
        "\n",
        "            # Cross Validation Mode\n",
        "            self.model.eval()\n",
        "            self.model.batch_size = self.N_CV\n",
        "            # Init Hidden State\n",
        "            self.model.init_hidden_KNet()\n",
        "            with torch.no_grad():\n",
        "\n",
        "                SysModel.T_test = cv_input.size()[-1] # T_test is the maximum length of the CV sequences\n",
        "\n",
        "                x_out_cv_batch = torch.empty([self.N_CV, SysModel.m, SysModel.T_test]).to(self.device)\n",
        "\n",
        "                # Init Sequence\n",
        "                if(randomInit):\n",
        "                    if(cv_init==None):\n",
        "                        self.model.InitSequence(\\\n",
        "                        SysModel.m1x_0.reshape(1,SysModel.m,1).repeat(self.N_CV,1,1), SysModel.T_test)\n",
        "                    else:\n",
        "                        self.model.InitSequence(cv_init, SysModel.T_test)\n",
        "                else:\n",
        "                    self.model.InitSequence(\\\n",
        "                        SysModel.m1x_0.reshape(1,SysModel.m,1).repeat(self.N_CV,1,1), SysModel.T_test)\n",
        "\n",
        "                for t in range(0, SysModel.T_test):\n",
        "                    x_out_cv_batch[:, :, t] = torch.squeeze(self.model(torch.unsqueeze(cv_input[:, :, t],2)))\n",
        "\n",
        "                # Compute CV Loss\n",
        "                MSE_cvbatch_linear_LOSS = 0\n",
        "                if(MaskOnState):\n",
        "                    if self.args.randomLength:\n",
        "                        for index in range(self.N_CV):\n",
        "                            MSE_cv_linear_LOSS[index] = self.loss_fn(x_out_cv_batch[index,mask,cv_lengthMask[index]], cv_target[index,mask,cv_lengthMask[index]])\n",
        "                        MSE_cvbatch_linear_LOSS = torch.mean(MSE_cv_linear_LOSS)\n",
        "                    else:\n",
        "                        MSE_cvbatch_linear_LOSS = self.loss_fn(x_out_cv_batch[:,mask,:], cv_target[:,mask,:])\n",
        "                else:\n",
        "                    if self.args.randomLength:\n",
        "                        for index in range(self.N_CV):\n",
        "                            MSE_cv_linear_LOSS[index] = self.loss_fn(x_out_cv_batch[index,:,cv_lengthMask[index]], cv_target[index,:,cv_lengthMask[index]])\n",
        "                        MSE_cvbatch_linear_LOSS = torch.mean(MSE_cv_linear_LOSS)\n",
        "                    else:\n",
        "                        MSE_cvbatch_linear_LOSS = self.loss_fn(x_out_cv_batch, cv_target)\n",
        "\n",
        "                # dB Loss\n",
        "                self.MSE_cv_linear_epoch[ti] = MSE_cvbatch_linear_LOSS.item()\n",
        "                self.MSE_cv_dB_epoch[ti] = 10 * torch.log10(self.MSE_cv_linear_epoch[ti])\n",
        "\n",
        "                if (self.MSE_cv_dB_epoch[ti] < self.MSE_cv_dB_opt):\n",
        "                    self.MSE_cv_dB_opt = self.MSE_cv_dB_epoch[ti]\n",
        "                    self.MSE_cv_idx_opt = ti\n",
        "\n",
        "                    torch.save(self.model, path_results + 'best-model.pt')\n",
        "\n",
        "            ########################\n",
        "            ### Training Summary ###\n",
        "            ########################\n",
        "            print(ti, \"MSE Training :\", self.MSE_train_dB_epoch[ti], \"[dB]\", \"MSE Validation :\", self.MSE_cv_dB_epoch[ti],\n",
        "                  \"[dB]\")\n",
        "\n",
        "            if (ti > 1):\n",
        "                d_train = self.MSE_train_dB_epoch[ti] - self.MSE_train_dB_epoch[ti - 1]\n",
        "                d_cv = self.MSE_cv_dB_epoch[ti] - self.MSE_cv_dB_epoch[ti - 1]\n",
        "                print(\"diff MSE Training :\", d_train, \"[dB]\", \"diff MSE Validation :\", d_cv, \"[dB]\")\n",
        "\n",
        "            print(\"Optimal idx:\", self.MSE_cv_idx_opt, \"Optimal :\", self.MSE_cv_dB_opt, \"[dB]\")\n",
        "\n",
        "        return [self.MSE_cv_linear_epoch, self.MSE_cv_dB_epoch, self.MSE_train_linear_epoch, self.MSE_train_dB_epoch]\n",
        "\n",
        "    def NNTest(self, SysModel, test_input, test_target, path_results, MaskOnState=False,\\\n",
        "     randomInit=False,test_init=None,load_model=False,load_model_path=None,\\\n",
        "        test_lengthMask=None):\n",
        "        # Load model\n",
        "        if load_model:\n",
        "            self.model = torch.load(load_model_path, map_location=self.device)\n",
        "        else:\n",
        "            self.model = torch.load(path_results+'best-model.pt', map_location=self.device)\n",
        "\n",
        "        self.N_T = test_input.shape[0]\n",
        "        SysModel.T_test = test_input.size()[-1]\n",
        "        self.MSE_test_linear_arr = torch.zeros([self.N_T])\n",
        "        x_out_test = torch.zeros([self.N_T, SysModel.m,SysModel.T_test]).to(self.device)\n",
        "\n",
        "        if MaskOnState:\n",
        "            mask = torch.tensor([True,False,False])\n",
        "            if SysModel.m == 2:\n",
        "                mask = torch.tensor([True,False])\n",
        "\n",
        "        # MSE LOSS Function\n",
        "        loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "        # Test mode\n",
        "        self.model.eval()\n",
        "        self.model.batch_size = self.N_T\n",
        "        # Init Hidden State\n",
        "        self.model.init_hidden_KNet()\n",
        "        torch.no_grad()\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        if (randomInit):\n",
        "            self.model.InitSequence(test_init, SysModel.T_test)\n",
        "        else:\n",
        "            self.model.InitSequence(SysModel.m1x_0.reshape(1,SysModel.m,1).repeat(self.N_T,1,1), SysModel.T_test)\n",
        "\n",
        "        for t in range(0, SysModel.T_test):\n",
        "            x_out_test[:,:, t] = torch.squeeze(self.model(torch.unsqueeze(test_input[:,:, t],2)))\n",
        "\n",
        "        end = time.time()\n",
        "        t = end - start\n",
        "\n",
        "        # MSE loss\n",
        "        for j in range(self.N_T):# cannot use batch due to different length and std computation\n",
        "            if(MaskOnState):\n",
        "                if self.args.randomLength:\n",
        "                    self.MSE_test_linear_arr[j] = loss_fn(x_out_test[j,mask,test_lengthMask[j]], test_target[j,mask,test_lengthMask[j]]).item()\n",
        "                else:\n",
        "                    self.MSE_test_linear_arr[j] = loss_fn(x_out_test[j,mask,:], test_target[j,mask,:]).item()\n",
        "            else:\n",
        "                if self.args.randomLength:\n",
        "                    self.MSE_test_linear_arr[j] = loss_fn(x_out_test[j,:,test_lengthMask[j]], test_target[j,:,test_lengthMask[j]]).item()\n",
        "                else:\n",
        "                    self.MSE_test_linear_arr[j] = loss_fn(x_out_test[j,:,:], test_target[j,:,:]).item()\n",
        "\n",
        "        # Average\n",
        "        self.MSE_test_linear_avg = torch.mean(self.MSE_test_linear_arr)\n",
        "        self.MSE_test_dB_avg = 10 * torch.log10(self.MSE_test_linear_avg)\n",
        "\n",
        "        # Standard deviation\n",
        "        self.MSE_test_linear_std = torch.std(self.MSE_test_linear_arr, unbiased=True)\n",
        "\n",
        "        # Confidence interval\n",
        "        self.test_std_dB = 10 * torch.log10(self.MSE_test_linear_std + self.MSE_test_linear_avg) - self.MSE_test_dB_avg\n",
        "\n",
        "        # Print MSE and std\n",
        "        str = self.modelName + \"-\" + \"MSE Test:\"\n",
        "        print(str, self.MSE_test_dB_avg, \"[dB]\")\n",
        "        str = self.modelName + \"-\" + \"STD Test:\"\n",
        "        print(str, self.test_std_dB, \"[dB]\")\n",
        "        # Print Run Time\n",
        "        print(\"Inference Time:\", t)\n",
        "\n",
        "        return [self.MSE_test_linear_arr, self.MSE_test_linear_avg, self.MSE_test_dB_avg, x_out_test, t]\n",
        "\n",
        "    def PlotTrain_KF(self, MSE_KF_linear_arr, MSE_KF_dB_avg):\n",
        "\n",
        "        self.Plot = Plot_KF(self.folderName, self.modelName)\n",
        "\n",
        "        self.Plot.NNPlot_epochs(self.N_steps, MSE_KF_dB_avg,\n",
        "                                self.MSE_test_dB_avg, self.MSE_cv_dB_epoch, self.MSE_train_dB_epoch)\n",
        "\n",
        "        self.Plot.NNPlot_Hist(MSE_KF_linear_arr, self.MSE_test_linear_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNTovp1gynBo"
      },
      "source": [
        "# EKFTest #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bz4PJvaz_6oM"
      },
      "outputs": [],
      "source": [
        "def EKFTest(SysModel, test_input, test_target, equation=13, model='regular',  modelKnowledge='full', allStates=True):\n",
        "    N_T = test_target.size()[0]\n",
        "\n",
        "    # LOSS\n",
        "    loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "    # MSE [Linear]\n",
        "    MSE_EKF_linear_arr = torch.zeros(N_T)\n",
        "    SNR = torch.zeros(N_T)\n",
        "    EKF = ExtendedKalmanFilter(SysModel, equation, model, modelKnowledge)\n",
        "    EKF.InitSequence(SysModel.m1x_0, SysModel.m2x_0)\n",
        "    MSE_finalize = torch.zeros(test_target.shape[2])\n",
        "    KG_array = torch.zeros_like(EKF.KG_array)\n",
        "    EKF_out = torch.zeros([N_T, SysModel.m, SysModel.T_test])\n",
        "    start = time.time()\n",
        "    MSE_test_per_iter = torch.zeros(test_target.shape[2])\n",
        "    stoper = True\n",
        "    # print(\"Target: \",test_target[4, :, 0:4])\n",
        "    for j in range(0, N_T):\n",
        "        # print(test_input.shape)\n",
        "        EKF.GenerateSequence(test_input[j, :, :], EKF.T_test)\n",
        "        for t in range(0, SysModel.T):\n",
        "            MSE_test_per_iter[t] = loss_fn(EKF.x[:, t], test_target[j, :, t])\n",
        "        # if j%25 == 0:\n",
        "          # print(\"Estimation: \",EKF.x[:, 0:4])\n",
        "        SNR[j] = 1  # torch.sqrt(sum(abs(test_target[:, j+1] - EKF.x[:, j])))\n",
        "        MSE_EKF_linear_arr[j] = loss_fn(EKF.x, test_target[j, :, :]).item()\n",
        "        KG_array = torch.add(EKF.KG_array, KG_array)\n",
        "        EKF_out[j, :, :] = EKF.x\n",
        "        MSE_finalize += MSE_test_per_iter\n",
        "\n",
        "    end = time.time()\n",
        "    t = end-start\n",
        "\n",
        "    # Average KG_array over Test Examples\n",
        "    KG_array /= N_T\n",
        "\n",
        "    # print('SNR', SNR)\n",
        "    SNR_avg = torch.mean(SNR)/(SysModel.r**2)\n",
        "    SNR_dB_avg = 10 * torch.log10(SNR_avg)\n",
        "\n",
        "    # print('MSE', MSE_EKF_linear_arr)\n",
        "    MSE_EKF_linear_avg = torch.mean(MSE_EKF_linear_arr)\n",
        "    MSE_EKF_dB_avg = 10 * torch.log10(MSE_EKF_linear_avg)\n",
        "\n",
        "    # Standard deviation\n",
        "    MSE_EKF_dB_std = torch.std(MSE_EKF_linear_arr, unbiased=True)\n",
        "    MSE_EKF_dB_std = 10 * torch.log10(MSE_EKF_dB_std)\n",
        "    print(model)\n",
        "    print(\"EKF - SNR:\", SNR_dB_avg, \"[dB]\")\n",
        "    print(\"EKF - MSE LOSS:\", MSE_EKF_dB_avg, \"[dB]\")\n",
        "    print(\"EKF - MSE STD:\", MSE_EKF_dB_std, \"[dB]\")\n",
        "\n",
        "    # Print Run Time\n",
        "    print(\"Inference Time:\", t)\n",
        "    print(\"MSE fin not dB:\")\n",
        "\n",
        "    # print(MSE_finalize)\n",
        "    MSE_finalize = MSE_finalize / N_T # average\n",
        "    MSE_finalize_dB = 10 * torch.log10(MSE_finalize)\n",
        "    if torch.isnan(MSE_finalize).any() == True:\n",
        "      print(\"the nan is in the finalize dB!!\")\n",
        "\n",
        "    return [MSE_finalize_dB, MSE_EKF_linear_arr, MSE_EKF_linear_avg, MSE_EKF_dB_avg, KG_array, EKF_out, SNR_dB_avg]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_Q_uL2_yqLd"
      },
      "source": [
        "# Main Random Walk #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qkXaBqx3_RKS"
      },
      "outputs": [],
      "source": [
        "PFandUKF_test = False\n",
        "if torch.cuda.is_available() and not PFandUKF_test:\n",
        "    dev = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n",
        "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    dev = torch.device(\"cpu\")\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    print(\"Running on the CPU\")\n",
        "\n",
        "print(\"Pipeline Start\")\n",
        "\n",
        "################\n",
        "### Get Time ###\n",
        "################\n",
        "\n",
        "today = datetime.today()\n",
        "now = datetime.now()\n",
        "strToday = today.strftime(\"%m.%d.%y\")\n",
        "strNow = now.strftime(\"%H:%M:%S\")\n",
        "strTime = strToday + \"_\" + strNow\n",
        "print(\"Current Time =\", strTime)\n",
        "\n",
        "N_E = 800  # train\n",
        "N_CV = 100 # validation\n",
        "N_T = 100  # test\n",
        "\n",
        "# All together\n",
        "r2 = torch.tensor([0.001,0.01,0.1,1,10])\n",
        "r = torch.sqrt(r2)\n",
        "vdB = -20  # ratio v=q2/r2\n",
        "v = 10 ** (vdB / 10)\n",
        "q2 = torch.mul(v, r2)\n",
        "q = torch.sqrt(q2)\n",
        "EKF_result = torch.empty([len(r2), m, T_test])\n",
        "epsilon = torch.eye(10)*(1E-9)\n",
        "\n",
        "KF_MSE = []\n",
        "Random_walk_GSP_KNet_MSElist = []\n",
        "Random_walk_KNet_MSElist = []\n",
        "Random_walk_MSElist = []\n",
        "Random_walk_MSElist_diag = []\n",
        "SNR_RW_13 = []\n",
        "SNR_RW_20_diag = []\n",
        "Naive_MSE = []\n",
        "\n",
        "for index in range(len(r)):\n",
        "    print(\"1/r2 [dB]: \", 10 * torch.log10(1 / r[index] ** 2), r[index])\n",
        "    print(\"1/q2 [dB]: \", 10 * torch.log10(1 / q[index] ** 2))\n",
        "\n",
        "    target = torch.load('./drive/MyDrive/Project/data/Random Walk/n=10/Test_target_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "    observation = torch.load('./drive/MyDrive/Project/data/Random Walk/n=10/Test_observation_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "    target = target[:,:,0:T]\n",
        "    observation = observation[:,:,0:T]\n",
        "\n",
        "    RW_train_input = torch.load('./drive/MyDrive/Project/data/Random Walk/n=10/Train_observation_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "    RW_cv_input = torch.load('./drive/MyDrive/Project/data/Random Walk/n=10/Validation_observation_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "    RW_test_input = observation\n",
        "    RW_train_target = torch.load('./drive/MyDrive/Project/data/Random Walk/n=10/Train_target_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "    RW_cv_target = torch.load('./drive/MyDrive/Project/data/Random Walk/n=10/Validation_target_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "    RW_test_target = target\n",
        "\n",
        "    RW_train_input = RW_train_input[:,:,0:T]\n",
        "    RW_cv_input = RW_cv_input[:,:,0:T]\n",
        "    RW_test_input = RW_test_input[:,:,0:T]\n",
        "    RW_train_target = RW_train_target[:,:,0:T]\n",
        "    RW_cv_target = RW_cv_target[:,:,0:T]\n",
        "    RW_test_target = RW_test_target[:,:,0:T]\n",
        "    # print(RW_cv_target.shape,RW_cv_input.shape)\n",
        "\n",
        "    # Random Walk Knet\n",
        "    print(\"random walk KNet\")\n",
        "    sys_model = SystemModel_KF(F.to(dev), q[index].to(dev), H.to(dev), r[index].to(dev), T, T_test, m, n, L.to(dev), V.to(dev), V_t.to(dev))\n",
        "    sys_model.InitSequence(m1x_0.to(dev), m2x_0.to(dev))\n",
        "    modelFolder = 'KNet' + '/'\n",
        "    KNet_Pipeline = Batch_Pipeline_EKF(strTime, \"KNet\", \"Random_walk\")\n",
        "    KNet_Pipeline.setssModel(sys_model)\n",
        "    KNet_model = KalmanNetNN()\n",
        "    KNet_model.Build(sys_model)\n",
        "    KNet_Pipeline.setModel(KNet_model)\n",
        "    KNet_Pipeline.setTrainingParams(n_Epochs=70+index*3*index, n_Batch=20, learningRate=1E-3, weightDecay=1E-5)\n",
        "    KNet_Pipeline.NNTrain(N_E, RW_train_input, RW_train_target, N_CV, RW_cv_input, RW_cv_target)\n",
        "    [KNet_MSE_test_linear_arr, KNet_MSE_test_linear_avg, KNet_MSE_test_dB_avg, KNet_test,t] = KNet_Pipeline.NNTest(N_T,\n",
        "                                                                                                                 RW_test_input,\n",
        "                                                                                                                 RW_test_target)\n",
        "    # # Random Walk GSP-Knet\n",
        "    print(\"random walk GSP-KNet\")\n",
        "    sys_model = SystemModel_KF(F.to(dev), q[index].to(dev), H.to(dev), r[index].to(dev), T, T_test, m, n, L.to(dev), V.to(dev), V_t.to(dev))\n",
        "    sys_model.InitSequence(m1x_0.to(dev), m2x_0.to(dev))\n",
        "    modelFolder = 'KNet' + '/'\n",
        "    KNet_Pipeline = Batch_Pipeline_EKF(strTime, \"GSP_KNet\", \"Random_walk\")\n",
        "    KNet_Pipeline.setssModel(sys_model)\n",
        "    KNet_model = GSPKalmanNetNN()\n",
        "    KNet_model.Build(sys_model)\n",
        "    KNet_Pipeline.setModel(KNet_model)\n",
        "    KNet_Pipeline.setTrainingParams(n_Epochs=30+index*3*index, n_Batch=20, learningRate=1E-3, weightDecay=1E-5)\n",
        "\n",
        "    KNet_Pipeline.NNTrain(N_E, RW_train_input, RW_train_target, N_CV, RW_cv_input, RW_cv_target)\n",
        "    [GSP_KNet_MSE_test_linear_arr, GSP_KNet_MSE_test_linear_avg, GSP_KNet_MSE_test_dB_avg, GSP_KNet_test,t] = KNet_Pipeline.NNTest(N_T, RW_test_input, RW_test_target)\n",
        "\n",
        "    # # original KF\n",
        "    rkf = torch.sqrt(r2[index])\n",
        "    qkf = torch.sqrt(q2[index])\n",
        "    sys_model = SystemModel_lin(F, qkf, H, rkf, T, T_test)\n",
        "    sys_model.InitSequence(m1x_0.to(dev), m2x_0.to(dev))\n",
        "    print(\"Evaluate Kalman Filter True\")\n",
        "    print(m1x_0.shape,m2x_0.shape,\"shapes\")\n",
        "    [KF_MSE_finalize_dB, MSE_KF_linear_arr, MSE_KF_linear_avg, MSE_KF_dB_avg] = KFTest(sys_model, RW_test_input, RW_test_target)\n",
        "    KF_MSE.append(MSE_KF_dB_avg)\n",
        "\n",
        "    # # Random walk - GSP full matrix\n",
        "    print(\"GSP-EKF full matrix\")\n",
        "    equation = 13\n",
        "    sys_model = SystemModel(f, q[index].to(dev), h, r[index].to(dev), T, T_test, m, n, L.to(dev), V.to(dev), V_t.to(dev))\n",
        "    sys_model.InitSequence(m1x_0.to(dev), m2x_0.to(dev))\n",
        "    [MSE_finalize_iters13, Rand13_MSE_EKF_linear_arr, Rand13_MSE_EKF_linear_avg, Rand13_MSE_EKF_dB_avg, Rand13_EKF_KG_array,\n",
        "     Rand13_EKF_out, Rand13_SNR] = EKFTest(sys_model, RW_test_input, RW_test_target, equation, 'regular')\n",
        "\n",
        "    # # GSP - diagonal matrix\n",
        "    # equation = 20\n",
        "    # print(\"GSP-EKF diagonal matrix\")\n",
        "    # [MSE_finalize_iters20, Rand20_MSE_EKF_linear_arr, Rand20_MSE_EKF_linear_avg, Rand20_MSE_EKF_dB_avg, Rand20_EKF_KG_array,\n",
        "    #  Rand20_EKF_out, Rand20_SNR] = EKFTest(sys_model, RW_test_input, RW_test_target, equation, 'regular')\n",
        "\n",
        "    ##### Naive approach - inverse(H)*observation\n",
        "    Naive_estimation = Naive(RW_test_input)\n",
        "    # print(\"Naive shape\",RW_test_target[1,:,(0,100,199)], Naive_estimation[1,:,(0,100,199)],RW_test_input[1,:,(0,100,199)])\n",
        "    # Calculate MSE\n",
        "    # print(\"Naieve MSE \", mse)\n",
        "    loss_fn = nn.MSELoss(reduction='mean')\n",
        "    MSE_test_linear_arr = torch.zeros([RW_test_target.shape[0]])\n",
        "    # Test mode\n",
        "    for j in range(RW_test_target.shape[0]):\n",
        "      MSE_test_linear_arr[j] = loss_fn(RW_test_target[j,:,:], Naive_estimation[j,:,:]).item()\n",
        "    ### Average\n",
        "    print(MSE_test_linear_arr)\n",
        "    MSE_test_linear_avg = torch.mean(MSE_test_linear_arr)\n",
        "    MSE_test_dB_avg = 10 * torch.log10(MSE_test_linear_avg)\n",
        "    print(\"Naive MSE \", MSE_test_dB_avg)\n",
        "\n",
        "    Naive_MSE.append(MSE_test_dB_avg)\n",
        "    Random_walk_MSElist.append(Rand13_MSE_EKF_dB_avg)\n",
        "    SNR_RW_13.append(Rand13_SNR)\n",
        "    # Random_walk_MSElist_diag.append(Rand20_MSE_EKF_dB_avg)\n",
        "    # SNR_RW_20_diag.append(Rand20_SNR)\n",
        "\n",
        "    Random_walk_KNet_MSElist.append(KNet_MSE_test_dB_avg)\n",
        "    Random_walk_GSP_KNet_MSElist.append(GSP_KNet_MSE_test_dB_avg)\n",
        "\n",
        "    # plt.figure()\n",
        "    # print(\"GSP-EKF MSE per iteration for full KG matrix\")\n",
        "    # print(MSE_finalize_iters13)\n",
        "    # print(\"GSP-EKF MSE per iteration for diagonal KG matrix\")\n",
        "    # print(MSE_finalize_iters20)\n",
        "    # print(\"KNet MSE per iteration for full KG matrix\")\n",
        "    # print(Knet_MSE_fin)\n",
        "    # print(\"GSP-KNet MSE per iteration for diagonal KG matrix\")\n",
        "    # print(gspKnet_MSE_fin)\n",
        "\n",
        "    # plt.plot(range(2, T+1), Knet_MSE_fin[1:].cpu().detach().numpy(), label='Knet')\n",
        "    # plt.plot(range(2, T+1), KF_MSE_finalize_dB[1:].cpu().detach().numpy(), label='KF')\n",
        "    # plt.plot(range(2, T+1), gspKnet_MSE_fin[1:].cpu().detach().numpy(), label='Knet GSP')\n",
        "    # plt.plot(range(2, T+1), MSE_finalize_iters13[1:].cpu().detach().numpy(), label='GSP-EKF-full')\n",
        "    # plt.plot(range(2, T+1), MSE_finalize_iters20[1:].cpu().detach().numpy(), label='GSP-EKF-diag')\n",
        "    # plt.xlabel('Iteration')\n",
        "    # plt.ylabel('MSE dB')\n",
        "    # plt.title(' MSE vs Iteration')\n",
        "    # plt.legend()\n",
        "    # plt.grid()\n",
        "    # plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFP55lhxj-DP"
      },
      "source": [
        "Plot final graph\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LwIE1_BtN7Ka"
      },
      "outputs": [],
      "source": [
        "# print(\"Saving results ----->\")\n",
        "# torch.save(Random_walk_KNet_MSElist, '/content/drive/MyDrive/Project/results/RW n=10/partially_L_KNet.pth')\n",
        "# torch.save(Random_walk_GSP_KNet_MSElist, '/content/drive/MyDrive/Project/results/RW n=10/partially_L_gspKNet.pth')\n",
        "# torch.save(Random_walk_MSElist, '/content/drive/MyDrive/Project/results/RW n=10/partially_L_gspKF.pth')\n",
        "# torch.save(Naive_MSE, '/content/drive/MyDrive/Project/results/RW n=10/partially_L_Naive.pth')\n",
        "# torch.save(KF_MSE, '/content/drive/MyDrive/Project/results/RW n=10/partially_L_KF.pth')\n",
        "\n",
        "# Random_walk_KNet_MSElist = torch.load('/content/drive/MyDrive/Project/results/RW n=10/KNet.pth')\n",
        "# Random_walk_GSP_KNet_MSElist = torch.load('/content/drive/MyDrive/Project/results/RW n=10/gspKNet.pth')\n",
        "# Random_walk_MSElist = torch.load('/content/drive/MyDrive/Project/results/RW n=10/gspKF.pth')\n",
        "# Naive_MSE = torch.load('/content/drive/MyDrive/Project/results/RW n=10/Naive.pth')\n",
        "# KF_MSE = torch.load('/content/drive/MyDrive/Project/results/RW n=10/KF.pth')\n",
        "\n",
        "# Random_walk_KNet_MSElist = [tensor.cpu() for tensor in Random_walk_KNet_MSElist]\n",
        "# Random_walk_GSP_KNet_MSElist = [tensor.cpu() for tensor in Random_walk_GSP_KNet_MSElist]\n",
        "# Random_walk_MSElist = [tensor.cpu() for tensor in Random_walk_MSElist]\n",
        "# Naive_MSE = [tensor.cpu() for tensor in Naive_MSE]\n",
        "# KF_list_MSE_db = [tensor.cpu() for tensor in KF_MSE]\n",
        "\n",
        "# plt.figure()\n",
        "# line1, = plt.plot(10*torch.log10(torch.tensor(1/r2).cpu()), Random_walk_KNet_MSElist, color='red', linestyle='dashed', linewidth=1,\n",
        "#                   marker='o', markerfacecolor='red', markersize=3, label=\"KNet\")\n",
        "# line2, = plt.plot(10*torch.log10(torch.tensor(1/r2).cpu()), Random_walk_GSP_KNet_MSElist, color='blue', linestyle='dashed', linewidth=1,\n",
        "#                   marker='o', markerfacecolor='blue', markersize=3, label=\"GSP_KNet\")\n",
        "# line3, = plt.plot(10*torch.log10(torch.tensor(1/r2).cpu()), Random_walk_MSElist, color='green', linestyle='dashed', linewidth=1,\n",
        "#              marker='*', markerfacecolor='green', markersize=3, label=\"GSP-KF\")\n",
        "# # line4, = plt.plot(10*torch.log10(torch.tensor(1/r2).cpu()), Naive_MSE, color='yellow', linestyle='dashed', linewidth=1,\n",
        "# #              marker='*', markerfacecolor='yellow', markersize=3, label=\"Naive approach\")\n",
        "# line5, = plt.plot(10*torch.log10(torch.tensor(1/r2).cpu()), KF_list_MSE_db, color='orange', linestyle='dashed', linewidth=1,\n",
        "#                   marker='*', markerfacecolor='blue', markersize=3, label=\"KF\")\n",
        "\n",
        "# leg = plt.legend(loc='upper left')\n",
        "# plt.xlabel('1/r^2[dB]')\n",
        "# plt.ylabel('MSE[dB]')\n",
        "# plt.grid()\n",
        "# plt.title('MSE vs SNR - Random Walk - Partially known L')\n",
        "# plt.savefig('/content/drive/MyDrive/Project/results/RW n=10/partially_known_L_no_Naive.png')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0AQgogJoS5au"
      },
      "outputs": [],
      "source": [
        "# PFandUKF_test = False\n",
        "# if torch.cuda.is_available() and not PFandUKF_test:\n",
        "#     dev = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n",
        "#     torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "#     print(\"Running on the GPU\")\n",
        "# else:\n",
        "\n",
        "#     dev = torch.device(\"cpu\")\n",
        "#     torch.set_default_tensor_type('torch.FloatTensor')\n",
        "#     print(\"Running on the CPU\")\n",
        "\n",
        "# print(\"Pipeline Start\")\n",
        "\n",
        "# ################\n",
        "# ### Get Time ###\n",
        "# ################\n",
        "\n",
        "# today = datetime.today()\n",
        "# now = datetime.now()\n",
        "# strToday = today.strftime(\"%m.%d.%y\")\n",
        "# strNow = now.strftime(\"%H:%M:%S\")\n",
        "# strTime = strToday + \"_\" + strNow\n",
        "# print(\"Current Time =\", strTime)\n",
        "\n",
        "# N_E = 800  # train\n",
        "# N_CV = 100 # validation\n",
        "# N_T = 100  # test\n",
        "\n",
        "# # All together\n",
        "# r2 = torch.tensor([0.001,0.01,0.1,1,10])\n",
        "# r = torch.sqrt(r2)\n",
        "# vdB = -20  # ratio v=q2/r2\n",
        "# v = 10 ** (vdB / 10)\n",
        "# q2 = torch.mul(v, r2)\n",
        "# q = torch.sqrt(q2)\n",
        "# EKF_result = torch.empty([len(r2), m, T_test])\n",
        "# epsilon = torch.eye(10)*(1E-4)\n",
        "\n",
        "# # Random_walk_GSP_KNet_MSElist = []\n",
        "# # Random_walk_KNet_MSElist = []\n",
        "# # Random_walk_MSElist = []\n",
        "# # Random_walk_MSElist_diag = []\n",
        "# # SNR_RW_13 = []\n",
        "# # SNR_RW_20_diag = []\n",
        "# KF_list_MSE_db =[]\n",
        "\n",
        "# for index in range(2, len(r)):\n",
        "#     print(\"1/r2 [dB]: \", 10 * torch.log10(1 / r[index] ** 2), r[index])\n",
        "#     print(\"1/q2 [dB]: \", 10 * torch.log10(1 / q[index] ** 2))\n",
        "\n",
        "#     target = torch.load('/content/Test_target_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     observation = torch.load('/content/Test_observation_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     target = target[:,:,0:T]\n",
        "#     observation = observation[:,:,0:T]\n",
        "\n",
        "#     RW_train_input = torch.load('/content/Train_observation_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     RW_cv_input = torch.load('/content/Validation_observation_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     RW_test_input = observation\n",
        "#     RW_train_target = torch.load('/content/Train_target_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     RW_cv_target = torch.load('/content/Validation_target_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     RW_test_target = target\n",
        "\n",
        "#     RW_train_input = RW_train_input[:,:,0:T]\n",
        "#     RW_cv_input = RW_cv_input[:,:,0:T]\n",
        "#     RW_test_input = RW_test_input[:,:,0:T]\n",
        "#     RW_train_target = RW_train_target[:,:,0:T]\n",
        "#     RW_cv_target = RW_cv_target[:,:,0:T]\n",
        "#     RW_test_target = RW_test_target[:,:,0:T]\n",
        "\n",
        "#     # original KF\n",
        "#     rkf = torch.sqrt(r2[index])\n",
        "#     qkf = torch.sqrt(q2[index])\n",
        "#     sys_model = SystemModel_lin(F, qkf, H, rkf, T, T_test)\n",
        "#     sys_model.InitSequence(m1x_0.to(dev), m2x_0.to(dev))\n",
        "#     print(\"Evaluate Kalman Filter True\")\n",
        "#     [KF_MSE_finalize_dB, MSE_KF_linear_arr, MSE_KF_linear_avg, MSE_KF_dB_avg] = KFTest(sys_model, RW_test_input, RW_test_target)\n",
        "#     KF_list_MSE_db.append(MSE_KF_dB_avg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uH7SktO416s8"
      },
      "outputs": [],
      "source": [
        "# Random_walk_KNet_MSElist = [tensor.cpu() for tensor in Random_walk_KNet_MSElist]\n",
        "# Random_walk_GSP_KNet_MSElist = [tensor.cpu() for tensor in Random_walk_GSP_KNet_MSElist]\n",
        "# Random_walk_MSElist = [tensor.cpu() for tensor in Random_walk_MSElist]\n",
        "# Random_walk_MSElist_diag = [tensor.cpu() for tensor in Random_walk_MSElist_diag]\n",
        "# KF_list_MSE_db = [tensor.cpu() for tensor in KF_MSE]\n",
        "\n",
        "\n",
        "# r2 = torch.tensor([0.01,0.1,1,10])\n",
        "# plt.figure()\n",
        "# line1, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), Random_walk_KNet_MSElist, color='red', linestyle='dashed', linewidth=1,\n",
        "#                   marker='*', markerfacecolor='red', markersize=3, label=\"Random_walk_KNet\")\n",
        "# line2, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), Random_walk_GSP_KNet_MSElist, color='blue', linestyle='dashed', linewidth=1,\n",
        "#                   marker='o', markerfacecolor='blue', markersize=3, label=\"Random_walk_GSP_KNet\")\n",
        "# line3, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), Random_walk_MSElist, color='green', linestyle='dashed', linewidth=1,\n",
        "#              marker='*', markerfacecolor='green', markersize=3, label=\"Rand walk - full inverse\")\n",
        "# line4, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), Random_walk_MSElist_diag, color='yellow', linestyle='dashed', linewidth=1,\n",
        "#              marker='o', markerfacecolor='yellow', markersize=3, label=\"Rand walk - diag inverse\")\n",
        "# line5, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), KF_list_MSE_db, color='orange', linestyle='dashed', linewidth=1,\n",
        "#              marker='o', markerfacecolor='orange', markersize=3, label=\"KF\")\n",
        "\n",
        "# leg = plt.legend(loc='upper left')\n",
        "# plt.xlabel('1/r^2[dB]')\n",
        "# plt.ylabel('MSE[dB]')\n",
        "# plt.grid()\n",
        "# plt.title('MSE vs SNR -RW')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdDIOjJ0J7vR"
      },
      "source": [
        "# Main Non linear with linear parameters #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zIYaevA6J_Rs"
      },
      "outputs": [],
      "source": [
        "def batch_f(x):\n",
        "    B = x.size()[0]\n",
        "    if len(x.shape) != 3:\n",
        "      x = x.unsqueeze(-1)\n",
        "    temp = torch.bmm(F.expand(B, -1, -1).type(torch.DoubleTensor).to(dev), x.type(torch.DoubleTensor).to(dev)).squeeze(-1)\n",
        "    return temp.reshape(-1,10,1).type(torch.FloatTensor).to(dev)\n",
        "\n",
        "def batch_h(x):\n",
        "    B = x.size()[0]\n",
        "    if len(x.shape) != 3:\n",
        "      x = x.unsqueeze(-1)\n",
        "    temp = torch.bmm(H.expand(B, -1, -1).type(torch.DoubleTensor).to(dev), x.type(torch.DoubleTensor).to(dev)).squeeze(-1)\n",
        "    return temp.reshape(-1,10,1).type(torch.FloatTensor).to(dev)\n",
        "\n",
        "\n",
        "T = 200\n",
        "T_test = T\n",
        "nl_T = T\n",
        "nl_T_test = nl_T\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "PFandUKF_test = False\n",
        "if torch.cuda.is_available() and not PFandUKF_test:\n",
        "    dev = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n",
        "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    dev = torch.device(\"cpu\")\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    print(\"Running on the CPU\")\n",
        "\n",
        "print(\"Pipeline Start\")\n",
        "\n",
        "################\n",
        "### Get Time ###\n",
        "################\n",
        "\n",
        "today = datetime.today()\n",
        "now = datetime.now()\n",
        "strToday = today.strftime(\"%m.%d.%y\")\n",
        "strNow = now.strftime(\"%H:%M:%S\")\n",
        "strTime = strToday + \"_\" + strNow\n",
        "print(\"Current Time =\", strTime)\n",
        "\n",
        "N_E = 800  # train\n",
        "N_CV = 100 # validation\n",
        "N_T = 100  # test\n",
        "\n",
        "# All together\n",
        "# r2 = torch.tensor([0.001,0.01,0.1,1,10])\n",
        "r2 = torch.tensor([0.1,1,10])\n",
        "r = torch.sqrt(r2)\n",
        "vdB = -20  # ratio v=q2/r2\n",
        "v = 10 ** (vdB / 10)\n",
        "q2 = torch.mul(v, r2)\n",
        "q = torch.sqrt(q2)\n",
        "EKF_result = torch.empty([len(r2), m, T_test])\n",
        "epsilon = torch.eye(5)*(1E-9)\n",
        "\n",
        "# nl_GSP_KNet_MSElist = []\n",
        "# nl_KNet_MSElist = []\n",
        "# nl_MSElist = []\n",
        "# nl_MSElist_diag = []\n",
        "# SNR_RW_13 = []\n",
        "# SNR_RW_20_diag = []\n",
        "\n",
        "# for index in range(0, len(r)-1):\n",
        "#     print(\"1/r2 [dB]: \", 10 * torch.log10(1 / r[index] ** 2), r[index])\n",
        "#     print(\"1/q2 [dB]: \", 10 * torch.log10(1 / q[index] ** 2))\n",
        "#     target = torch.load('./drive/MyDrive/Project/data/Random Walk/n=10/Test_target_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     # target = torch.load('./gdrive/My Drive/Project/data/Non Linear/test_target_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     observation = torch.load('./drive/MyDrive/Project/data/Random Walk/n=10/Test_observation_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     target = target[:,:,0:T]\n",
        "#     observation = observation[:,:,0:T]\n",
        "\n",
        "#     nl_train_input = torch.load('./drive/MyDrive/Project/data/Random Walk/n=10/Train_observation_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     nl_cv_input = torch.load('./drive/MyDrive/Project/data/Random Walk/n=10/Validation_observation_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     nl_test_input = observation\n",
        "#     nl_train_target = torch.load('./drive/MyDrive/Project/data/Random Walk/n=10/Train_target_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     nl_cv_target = torch.load('./drive/MyDrive/Project/data/Random Walk/n=10/Validation_target_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     nl_test_target = target\n",
        "#     print(nl_train_input.shape)\n",
        "#     nl_train_input = nl_train_input[:,:,0:T].float()\n",
        "#     nl_cv_input = nl_cv_input[:,:,0:T].float()\n",
        "#     nl_test_input = nl_test_input[:,:,0:T].float()\n",
        "#     nl_train_target = nl_train_target[:,:,0:T].float()\n",
        "#     nl_cv_target = nl_cv_target[:,:,0:T].float()\n",
        "#     nl_test_target = nl_test_target[:,:,0:T].float()\n",
        "\n",
        "#     # Non Linear\n",
        "#     nl_m1x_0 = target[0, :, 0].to(dev)\n",
        "#     nl_m2x_0 = 0 * 0 * torch.zeros(m, m).to(dev)\n",
        "#     m1x_0 = nl_m1x_0\n",
        "#     m2x_0 = nl_m2x_0\n",
        "#     n_Batch=7\n",
        "\n",
        "#     # ### KalmanNet\n",
        "#     # sys_model = SystemModel(nl_f, q[index], nl_h, r[index], nl_T, nl_T_test, nl_m, nl_n, nl_L, nl_V, nl_V_t)\n",
        "#     # sys_model.InitSequence(m1x_0, m2x_0)\n",
        "#     # print(\"KNet EKF with full model info\")\n",
        "#     # modelFolder = 'KNet' + '/'\n",
        "#     # KNet_Pipeline = Pipeline_EKF(strTime, \"KNet\", \"KalmanNet\")\n",
        "#     # KNet_Pipeline.setssModel(sys_model)\n",
        "#     # KNet_model = ExtendedKalmanNetNN()\n",
        "#     # KNet_model.apply(init_weights)\n",
        "#     # KNet_model.Build(sys_model)\n",
        "#     # KNet_Pipeline.setModel(KNet_model)\n",
        "#     # # KNet_Pipeline.model = torch.load(modelFolder+\"RW_EKF_KNet.pt\")\n",
        "#     # KNet_Pipeline.setTrainingParams(n_Epochs=300, n_Batch=140 , learningRate=1e-5, weightDecay=1E-6)\n",
        "#     # KNet_Pipeline.NNTrain(N_E, nl_train_input, nl_train_target, N_CV, nl_cv_input, nl_cv_target)\n",
        "#     # [KNet_MSE_test_linear_arr, KNet_MSE_test_linear_avg, KNet_MSE_test_dB_avg, KNet_test, timming] = KNet_Pipeline.NNTest(N_T,\n",
        "#     #                                                                                                             nl_test_input,\n",
        "#     #                                                                                                              nl_test_target)\n",
        "\n",
        "#     # torch.save(KNet_model.state_dict(), 'KNet_weights_'+str(r2[index])+'.pth')\n",
        "\n",
        "#     ## GSP-KalmanNet\n",
        "#     sys_model = SystemModel(batch_f, q[index], batch_h, r[index], T, T_test, m, n, L, V, V_t)\n",
        "#     sys_model.InitSequence(m1x_0.to(dev), m2x_0.to(dev))\n",
        "#     print(\"GSP - KNet EKF with full model info\")\n",
        "#     modelFolder = 'KNet' + '/'\n",
        "#     gspKNet_Pipeline = Pipeline_EKF(strTime, \"KNet\", \"KalmanNet\")\n",
        "#     gspKNet_Pipeline.setssModel(sys_model)\n",
        "#     gspKNet_model = GSPExtendedKalmanNetNN()\n",
        "#     # gspKNet_model.apply(init_weights)\n",
        "#     gspKNet_model.Build(sys_model)\n",
        "#     gspKNet_Pipeline.setModel(gspKNet_model)\n",
        "#     gspKNet_Pipeline.setTrainingParams(n_Epochs=100, n_Batch=50, learningRate=1e-4, weightDecay=1e-6)\n",
        "#     # gspKNet_Pipeline.gspKNet_model = torch.load('gspKNet_weights_'+str(r2[index+1])+'.pth')\n",
        "#     gspKNet_Pipeline.NNTrain(N_E, nl_train_input.float(), nl_train_target.float(), N_CV, nl_cv_input.float(), nl_cv_target.float())\n",
        "#     [KNet_MSE_test_linear_arr, KNet_MSE_test_linear_avg, gspKNet_MSE_test_dB_avg, KNet_test, timming] = gspKNet_Pipeline.NNTest(N_T,\n",
        "#                                                                                                                  nl_test_input,\n",
        "#                                                                                                                  nl_test_target)\n",
        "#     torch.save(gspKNet_model.state_dict(), 'gspKNet_weights2_'+str(r2[index])+'.pth')\n",
        "\n",
        "#     ## EKF\n",
        "#     # print(target[3,:,0])\n",
        "#     # print(nl_m2x_0)\n",
        "#     print(\"EKF calculation\")\n",
        "#     epsilon = torch.eye(10)*(1E-11)\n",
        "#     equation = 13\n",
        "#     sys_model = SystemModel(f, q[index].to(dev), h, r[index].to(dev), T, T_test, m, n, L.to(dev), V.to(dev), V_t.to(dev))\n",
        "#     sys_model.InitSequence(m1x_0, m2x_0)\n",
        "#     [MSE_finalize13, nl13_MSE_EKF_linear_arr, nl13_MSE_EKF_linear_avg, nl13_MSE_EKF_dB_avg, nl13_EKF_KG_array, nl13_EKF_out,\n",
        "#      nl13_SNR] = EKFTest(sys_model, nl_test_input, nl_test_target, equation, '')\n",
        "\n",
        "#     # Diagonal KG EKF\n",
        "#     # equation = 20\n",
        "#     # [MSE_finalize20, nl20_MSE_EKF_linear_arr, nl20_MSE_EKF_linear_avg, nl20_MSE_EKF_dB_avg, nl20_EKF_KG_array, nl20_EKF_out,\n",
        "#     #  nl20_SNR] = EKFTest(sys_model, nl_test_input, nl_test_target, equation, 'NonLinear')\n",
        "\n",
        "#     # print(\"GSP-EKF MSE per iteration for full KG matrix\")\n",
        "#     # print(MSE_finalize_iters13)\n",
        "#     # print(\"GSP-EKF MSE per iteration for diagonal KG matrix\")\n",
        "#     # print(MSE_finalize_iters20)\n",
        "#     # print(\"KNet MSE per iteration for full KG matrix\")\n",
        "#     # print(Knet_MSE_fin)\n",
        "#     # print(\"GSP-KNet MSE per iteration for diagonal KG matrix\")\n",
        "#     # print(gspKnet_MSE_fin)\n",
        "\n",
        "#     # nl_GSP_KNet_MSElist.append(gspKNet_MSE_test_dB_avg)\n",
        "#     # nl_KNet_MSElist.append(KNet_MSE_test_dB_avg)\n",
        "#     nl_MSElist.append(nl13_MSE_EKF_dB_avg)\n",
        "#     # nl_MSElist_diag.append(nl20_MSE_EKF_dB_avg)\n",
        "#     plt.figure()\n",
        "\n",
        "#     # plt.plot(range(2, T+1), MSE_finalize_Knet[1:].cpu().detach().numpy(), label='Knet')\n",
        "#     # plt.plot(range(2, T+1), KF_MSE_finalize_dB[1:].cpu().detach().numpy(), label='KF')\n",
        "#     # plt.plot(range(2, T+1), MSE_finalize_gspKnet[1:].cpu().detach().numpy(), label='Knet GSP')\n",
        "#     plt.plot(range(2, T+1), MSE_finalize13[1:].cpu().detach().numpy(), label='GSP-EKF-full')\n",
        "#     plt.plot(range(1, len(KNet_MSE_test_linear_arr)), KNet_MSE_test_linear_arr[1:].cpu().detach().numpy(), label='GSPKNet-full')\n",
        "#     # plt.plot(range(2, T+1), MSE_finalize20[1:].cpu().detach().numpy(), label='GSP-EKF-diag')\n",
        "#     plt.xlabel('Iteration')\n",
        "#     plt.ylabel('MSE')\n",
        "#     plt.title(' MSE vs Iteration')\n",
        "#     plt.legend()\n",
        "#     plt.grid()\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUdgQSE4K0ka"
      },
      "source": [
        "# Non Linear Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gafQUP5BKxub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "588424cf-17ac-4c6b-8ca9-49a3bdc5d2dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on the GPU\n",
            "torch.Size([10, 10])\n"
          ]
        }
      ],
      "source": [
        "PFandUKF_test = False\n",
        "\n",
        "if torch.cuda.is_available() and not PFandUKF_test:\n",
        "    dev = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc.\n",
        "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    dev = torch.device(\"cpu\")\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    print(\"Running on the CPU\")\n",
        "\n",
        "# nl_L = torch.load('/content/drive/MyDrive/Project/data/Non Linear/n=30/L_30.pt')\n",
        "\n",
        "nl_L = torch.Tensor([[6, -1, 0, -1, -1, -1, -1, 0, 0, -1],\n",
        "                         [-1, 6, 0, -1, 0, -1, -1, -1, 0, -1],\n",
        "                         [0, 0, 6, -1, -1, -1, 0, -1, -1, -1],\n",
        "                         [-1, -1, -1, 6, -1, 0, -1, 0, 0, -1],\n",
        "                         [-1, 0, -1, -1, 6, 0, -1, -1, -1, 0],\n",
        "                         [-1, -1, -1, 0, 0, 6, 0, -1, -1, -1],\n",
        "                         [-1, -1, 0, -1, -1, 0, 6, -1, -1, 0],\n",
        "                         [0, -1, -1, 0, -1, -1, -1, 6, -1, 0],\n",
        "                         [0, 0, -1, 0, -1, -1, -1, -1, 6, -1],\n",
        "                         [-1, -1, -1, -1, 0, -1, 0, 0, -1, 6]])\n",
        "\n",
        "epsilon = 0\n",
        "nl_L = nl_L.type(torch.DoubleTensor)\n",
        "W, V = np.linalg.eig(nl_L)\n",
        "nl_V = torch.from_numpy(V).type(torch.DoubleTensor).to(dev)+epsilon\n",
        "nl_V_t = torch.transpose(nl_V, 0, 1).type(torch.DoubleTensor).to(dev).float()\n",
        "nl_V = nl_V.float()\n",
        "nl_T = 200\n",
        "T = nl_T\n",
        "nl_T_test = nl_T\n",
        "nl_n = 10\n",
        "nl_m = 10\n",
        "\n",
        "def nl_f(x):\n",
        "    # L = nl_L\n",
        "    # # Get adjacency matrix A from Laplacian matrix L\n",
        "    # A1 = torch.diag_embed(torch.diag(L)) - L\n",
        "    # A = torch.diag(L)\n",
        "    # A = A - L\n",
        "    # # print(A.shape)\n",
        "    # # Compute f(x) = sin(x)+cos(A*x)\n",
        "    # a = torch.sin(x.type(torch.DoubleTensor))\n",
        "    # B = x.size()[0]\n",
        "    # if len(x.shape) != 3:\n",
        "    #   x = x.unsqueeze(-1)\n",
        "    # temp = torch.bmm(A1.expand(B, -1, -1).type(torch.DoubleTensor).to(dev), x.type(torch.DoubleTensor).to(dev)).squeeze(-1)\n",
        "    # # temp = torch.matmul(A1.type(torch.DoubleTensor), x.type(torch.DoubleTensor))\n",
        "    # b = torch.cos(temp.type(torch.DoubleTensor))\n",
        "    # X = a.squeeze() + b\n",
        "    # # X = 10 * b\n",
        "    # X = X.reshape(-1,nl_n,1).type(torch.FloatTensor).to(dev)\n",
        "    return 10*(x/10+torch.sin(x/9+3))\n",
        "\n",
        "def nl_h(x):\n",
        "    # return 3 * x\n",
        "    B = x.size()[0]\n",
        "    a = 0.5\n",
        "    if len(x.shape) != 3:\n",
        "      x = x.unsqueeze(-1)\n",
        "    # temp = torch.matmul(nl_V, x.type(torch.DoubleTensor))\n",
        "    x_freq = torch.bmm(nl_V_t.expand(B, -1, -1).type(torch.DoubleTensor).to(dev), x.type(torch.DoubleTensor).to(dev))\n",
        "    # print(temp.shape)\n",
        "    x_freq3 = torch.pow(x_freq,2)\n",
        "    y = torch.bmm(nl_V.expand(B, -1, -1).type(torch.DoubleTensor).to(dev),a*x_freq+(1-a)*x_freq3).squeeze(-1)\n",
        "    # print(y.shape)\n",
        "    return y\n",
        "\n",
        "def nl_f_EKF(x):\n",
        "    # L = nl_L\n",
        "    # # Get adjacency matrix A from Laplacian matrix L\n",
        "    # A1 = torch.diag_embed(torch.diag(L)) - L\n",
        "    # A = torch.diag(L)\n",
        "    # A = A - L\n",
        "    # # print(A.shape)\n",
        "    # # Compute f(x) = sin(x)+cos(A*x)\n",
        "    # a = torch.sin(x.type(torch.DoubleTensor)).to(dev)\n",
        "    # b = torch.cos(torch.matmul(A1.type(torch.DoubleTensor), x.type(torch.DoubleTensor)).type(torch.DoubleTensor)).to(dev)\n",
        "    # x = a + b\n",
        "    return 10*(x/10+torch.sin(x/9+3)).to(dev)\n",
        "\n",
        "def nl_h_EKF(x):\n",
        "    # return 3 * x\n",
        "    a = 0.5\n",
        "    x_freq = torch.matmul(nl_V_t.type(torch.DoubleTensor).to(dev),x.type(torch.DoubleTensor).to(dev)).to(dev)\n",
        "    x_freq3 = torch.pow(x_freq,2)\n",
        "    y = torch.matmul(nl_V.type(torch.DoubleTensor).to(dev),(a*x_freq+(1-a)*x_freq3))\n",
        "    return y\n",
        "\n",
        "def nl_getJacobian(x, a):\n",
        "    try:\n",
        "        if (x.size()[1] == 1):\n",
        "            y = torch.reshape((x.T), [x.size()[0]])\n",
        "    except:\n",
        "        y = torch.reshape((x.T), [x.size()[0]])\n",
        "\n",
        "    if (a == 'ObsAcc'):\n",
        "        g = nl_h_EKF\n",
        "    elif (a == 'ModAcc'):\n",
        "        g = nl_f_EKF\n",
        "\n",
        "    Jac = autograd.functional.jacobian(g, y)\n",
        "    Jac = Jac.view(-1, nl_m)\n",
        "    return Jac.to(dev)\n",
        "\n",
        "print(torch.matmul(nl_V_t,nl_V).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "uyuJ7h2Vh5VT"
      },
      "outputs": [],
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "def init_weights(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        init.xavier_uniform_(module.weight)\n",
        "        module.bias.data.fill_(0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psgmLNinrkhl"
      },
      "source": [
        "# Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "9E4-wMEUri43"
      },
      "outputs": [],
      "source": [
        "# # seed = 0\n",
        "# # torch.manual_seed(seed)\n",
        "# if nl_n == 5:\n",
        "#     x0 = torch.tensor([0.8, 0.3, 0.25, 0.6, 0.1]) * math.pi\n",
        "# elif nl_n == 10:  # nl_n = 10\n",
        "#     x0 = torch.tensor([0.8, 0.3, 0.45, 0.25, 0.6, 0.1, 0.75, 0.22, 0.12, 0.95]) * math.pi\n",
        "# else:  # other nl_n\n",
        "#     x0 = ml_x0 * math.pi\n",
        "\n",
        "# r2 = torch.tensor([0.001, 0.01, 0.1, 1, 10])\n",
        "# vdB = -20  # ratio v=q2/r2\n",
        "# v = 10 ** (vdB / 10)\n",
        "# q2 = torch.mul(v, r2)\n",
        "\n",
        "# N_train = 4000\n",
        "# N_valid = 200\n",
        "# N_test = 200\n",
        "\n",
        "# for index in range(len(r2)):\n",
        "#     print('r2 =', r2[index])\n",
        "#     # Validation data create\n",
        "#     valid_X = torch.zeros(N_valid, nl_n, nl_T)  # Tensor of m-by-T to contain the states thru time.\n",
        "#     valid_Y = torch.zeros(N_valid, nl_m, nl_T)  # Tensor of n-by-T to contain the observations thru time.\n",
        "#     for n in range(N_valid):  # Train\n",
        "#         valid_X[n, :, 0] = x0\n",
        "#         valid_X = valid_X.type(torch.DoubleTensor)\n",
        "#         valid_Y[n, :, 0] = nl_h_EKF(x0)\n",
        "#         w1 = torch.randn(nl_n)\n",
        "#         w1 = w1 * q2[index] ** 0.5\n",
        "#         for i in range(1, nl_T):  # create another T states in time\n",
        "#             valid_X[n, :, i] = nl_f_EKF(valid_X[n, :, i - 1]) + w1\n",
        "#             valid_Y[n, :, i] = nl_h_EKF(valid_X[n, :, i])\n",
        "#             w1 = torch.randn(nl_n) * q2[index] ** 0.5\n",
        "#     valid_Y = valid_Y + torch.randn(valid_Y.shape) * r2[index] ** 0.5\n",
        "#     torch.save(valid_X.type(torch.DoubleTensor), '/content/drive/MyDrive/Project/data/Non Linear H x3/n=10/valid_target_r2_' + str(r2[index]) + '.pt')\n",
        "#     torch.save(valid_Y.type(torch.DoubleTensor), '/content/drive/MyDrive/Project/data/Non Linear H x3/n=10/valid_observation_r2_' + str(r2[index]) + '.pt')\n",
        "\n",
        "#     # Train data create\n",
        "#     train_X = torch.zeros(N_train, nl_n, nl_T)  # Tensor of m-by-T to contain the states thru time.\n",
        "#     train_Y = torch.zeros(N_train, nl_m, nl_T)  # Tensor of n-by-T to contain the observations thru time.\n",
        "#     for n in range(N_train):  # Train\n",
        "#         # torch.manual_seed(seed)\n",
        "#         # torch.cuda.manual_seed(seed)\n",
        "#         train_X[n, :, 0] = x0\n",
        "#         train_X = train_X.type(torch.DoubleTensor)\n",
        "#         train_Y[n, :, 0] = nl_h_EKF(x0)\n",
        "#         for i in range(1, nl_T):  # create another T states in time\n",
        "#             train_X[n, :, i] = nl_f_EKF(train_X[n, :, i - 1]) + torch.randn(nl_n) * q2[index] ** 0.5\n",
        "#             train_Y[n, :, i] = nl_h_EKF(train_X[n, :, i])\n",
        "#             # torch.manual_seed(seed * i)  # Insert any integer\n",
        "#             # torch.cuda.manual_seed(seed * i)  # Insert any integer\n",
        "#     train_Y = train_Y + torch.randn(train_Y.shape)*r2[index]**0.5\n",
        "#     torch.save(train_X.type(torch.DoubleTensor), '/content/drive/MyDrive/Project/data/Non Linear H x3/n=10/train_target_r2_' + str(r2[index]) + '.pt')\n",
        "#     torch.save(train_Y.type(torch.DoubleTensor), '/content/drive/MyDrive/Project/data/Non Linear H x3/n=10/train_observation_r2_' + str(r2[index]) + '.pt')\n",
        "#       # nl_target = X.type(torch.DoubleTensor)\n",
        "#       # nl_observation = Y.type(torch.DoubleTensor)\n",
        "\n",
        "#     # Test data create\n",
        "#     # test_X = torch.zeros(N_test, nl_n, nl_T)  # Tensor of m-by-T to contain the states thru time.\n",
        "#     # test_Y = torch.zeros(N_test, nl_m, nl_T)  # Tensor of n-by-T to contain the observations thru time.\n",
        "#     # for n in range(N_test):  # Train\n",
        "#     #     test_X[n, :, 0] = x0\n",
        "#     #     # test_X = valid_X.type(torch.DoubleTensor)\n",
        "#     #     test_Y[n, :, 0] = nl_h_EKF(x0)\n",
        "#     #     w1 = torch.randn(nl_n)\n",
        "#     #     w1 = w1 * q2[index] ** 0.5\n",
        "#     #     for i in range(1, nl_T):  # create another T states in time\n",
        "#     #         test_X[n, :, i] = nl_f_EKF(test_X[n, :, i - 1]) + w1\n",
        "#     #         test_Y[n, :, i] = nl_h_EKF(test_X[n, :, i])\n",
        "#     #         w1 = torch.randn(nl_n) * q2[index] ** 0.5\n",
        "#     # test_Y[:,:,:] = test_Y[:,:,:] + torch.randn(test_Y[:,:,:].shape) * r2[index] ** 0.5\n",
        "#     # print(test_X[1,1,:])\n",
        "#     # torch.save(test_X.type(torch.DoubleTensor), '/content/drive/MyDrive/Project/data/Non Linear H x3/n=10/test_target_r2_' + str(r2[index]) + '.pt')\n",
        "#     # torch.save(test_Y.type(torch.DoubleTensor), '/content/drive/MyDrive/Project/data/Non Linear H x3/n=10/test_observation_r2_' + str(r2[index]) + '.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2hQOE5haUrV"
      },
      "source": [
        "# Main Non Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "76f11aN1Hjfd"
      },
      "outputs": [],
      "source": [
        "# T = 200\n",
        "# T_valid = T\n",
        "# T_test = T\n",
        "# nl_T = T\n",
        "# nl_T_test = nl_T\n",
        "# # torch.autograd.set_detect_anomaly(False)\n",
        "# PFandUKF_test = False\n",
        "# if torch.cuda.is_available() and not PFandUKF_test:\n",
        "#     dev = torch.device(\"cuda:0\")\n",
        "#     torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "#     print(\"Running on the GPU\")\n",
        "# else:\n",
        "#     dev = torch.device(\"cpu\")\n",
        "#     torch.set_default_tensor_type('torch.FloatTensor')\n",
        "#     print(\"Running on the CPU\")\n",
        "\n",
        "# print(\"Pipeline Start\")\n",
        "\n",
        "# ################\n",
        "# ### Get Time ###\n",
        "# ################\n",
        "\n",
        "# today = datetime.today()\n",
        "# now = datetime.now()\n",
        "# strToday = today.strftime(\"%m.%d.%y\")\n",
        "# strNow = now.strftime(\"%H:%M:%S\")\n",
        "# strTime = strToday + \"_\" + strNow\n",
        "# print(\"Current Time =\", strTime)\n",
        "\n",
        "# N_E = 3000  # train\n",
        "# N_CV = 100 # validation\n",
        "# N_T = 100  # test\n",
        "\n",
        "# # All together\n",
        "# r2 = torch.tensor([0.0010,0.0100,0.1000,1.0000,10.])\n",
        "# r = torch.sqrt(r2)\n",
        "# vdB = -20  # ratio v=q2/r2\n",
        "# v = 10 ** (vdB / 10)\n",
        "# q2 = torch.mul(v, r2)\n",
        "# # q2 = tensor([1.0000e-05, 1.0000e-04, 1.0000e-03, 1.0000e-02, 1.0000e-01])\n",
        "# # q2 = torch.tensor([1000000*1.0000e-05, 10000*1.0000e-04, 120*1.0000e-03, 1.0000e-02, 1.0000e-01])\n",
        "# # q2 = torch.tensor([0.8*1.0000e-02, 2*1.0000e-01, 0.7, 1, 5])\n",
        "# # q2 = torch.tensor([0.0007, 0.0030, 5*1.0000e-02, 1, 15]) # best for non linear n=10\n",
        "# q = torch.sqrt(q2)\n",
        "# EKF_result = torch.empty([len(r2), m, T_test])\n",
        "# epsilon = torch.eye(10)*(1E-13)\n",
        "\n",
        "# nl_GSP_KNet_MSElist = []\n",
        "# nl_KNet_MSElist = []\n",
        "# nl_MSElist = []\n",
        "# nl_MSElist_diag = []\n",
        "# SNR_RW_13 = []\n",
        "# SNR_RW_20_diag = []\n",
        "# Naive_MSE = []\n",
        "\n",
        "# for index in range(len(r2)):\n",
        "\n",
        "#     print(\"1/r2 [dB]: \", 10 * torch.log10(1 / r[index] ** 2), r2[index])\n",
        "#     print(\"1/q2 [dB]: \", 10 * torch.log10(1 / q[index] ** 2))\n",
        "\n",
        "#     target = torch.load('/content/drive/MyDrive/Project/data/Non Linear H x3/n=10/test_target_r2_'+ str(r2[index]) + '.pt').to(dev)\n",
        "#     observation = torch.load('/content/drive/MyDrive/Project/data/Non Linear H x3/n=10/test_observation_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     # target = test_X[index,:,:,:]\n",
        "#     # observation = test_Y[index,:,:,:]\n",
        "#     target = target[:N_T,:,:T_test].float()\n",
        "#     observation = observation[:N_T,:,:T_test].float()\n",
        "#     # print(target.shape)\n",
        "#     nl_train_input = torch.load('/content/drive/MyDrive/Project/data/Non Linear H x3/n=10/train_observation_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     nl_cv_input = torch.load('/content/drive/MyDrive/Project/data/Non Linear H x3/n=10/valid_observation_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     nl_test_input = observation\n",
        "#     nl_train_target = torch.load('/content/drive/MyDrive/Project/data/Non Linear H x3/n=10/train_target_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     nl_cv_target = torch.load('/content/drive/MyDrive/Project/data/Non Linear H x3/n=10/valid_target_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "#     nl_test_target = target\n",
        "#     # print(nl_train_input.shape)\n",
        "#     nl_train_input = nl_train_input[:,:,:T].float()\n",
        "#     nl_cv_input = nl_cv_input[:N_CV,:,:T_valid].float()\n",
        "#     nl_test_input = nl_test_input[:,:,:T].float()\n",
        "#     nl_train_target = nl_train_target[:,:,:T].float()\n",
        "#     nl_cv_target = nl_cv_target[:N_CV,:,:T_valid].float()\n",
        "#     nl_test_target = nl_test_target[:,:,:T].float()\n",
        "#     # print(nl_train_target.shape)\n",
        "\n",
        "#     # Non Linear\n",
        "#     nl_m1x_0 = target[0, :, 0].to(dev)\n",
        "#     nl_m2x_0 = 0 * 0 * torch.zeros(nl_m, nl_m).to(dev)\n",
        "#     m1x_0 = nl_m1x_0\n",
        "#     m2x_0 = nl_m2x_0\n",
        "#     # print(nl_train_input[1,:,(0,1,2)])\n",
        "#     # print(nl_train_target[1,:,(0,1,2,50,90)])\n",
        "\n",
        "#     # # ### KalmanNet\n",
        "#     # sys_model = SystemModel(nl_f, q[index], nl_h, r[index], nl_T, nl_T_test, nl_m, nl_n, nl_L, nl_V, nl_V_t)\n",
        "#     # sys_model.InitSequence(m1x_0, m2x_0)\n",
        "#     # print(\"KNet EKF with full model info\")\n",
        "#     # modelFolder = 'KNet' + '/'\n",
        "#     # KNet_Pipeline = Pipeline_EKF(strTime, \"KNet\", \"KalmanNet\")\n",
        "#     # KNet_Pipeline.setssModel(sys_model)\n",
        "#     # KNet_model = ExtendedKalmanNetNN()\n",
        "#     # try:\n",
        "#     #   checkpoint = torch.load('/content/drive/MyDrive/Project/models/Non Linear H x3 n=10/KNet_weights_n10f'+str(r2[index])+'.pth')\n",
        "#     #   epochs = 200\n",
        "#     # except:\n",
        "#     #   checkpoint = 0\n",
        "#     #   epochs = 45\n",
        "#     #   print(\"Training from zero\")\n",
        "#     #   # KNet_model.apply(init_weights)\n",
        "#     # KNet_model.Build(sys_model)\n",
        "#     # KNet_model.apply(init_weights)\n",
        "#     # KNet_Pipeline.setModel(KNet_model,checkpoint)\n",
        "#     # KNet_Pipeline.setTrainingParams(n_Epochs=epochs, n_Batch=100 , learningRate=1e-4, weightDecay=0)\n",
        "#     # if epochs != 2:\n",
        "#     #   KNet_Pipeline.NNTrain(N_E, nl_train_input, nl_train_target, N_CV, nl_cv_input, nl_cv_target)\n",
        "#     #   print(\"Saving model ----->\")\n",
        "#     #   torch.save({\n",
        "#     #           'model_state_dict': KNet_model.state_dict(),\n",
        "#     #           'optimizer_state_dict': KNet_Pipeline.optimizer.state_dict()\n",
        "#     #           }, '/content/drive/MyDrive/Project/models/Non Linear H x3 n=10/KNet_weights_n10f'+str(r2[index])+'.pth')\n",
        "#     # [KNet_MSE_test_linear_arr, KNet_MSE_test_linear_avg, KNet_MSE_test_dB_avg, KNet_test, timming] = KNet_Pipeline.NNTest(N_T,\n",
        "#     #                                                                                                         nl_test_input,\n",
        "#     #                                                                                                           nl_test_target)\n",
        "\n",
        "#     #### GSP-KalmanNet\n",
        "#     sys_model = SystemModel(nl_f, q[index], nl_h, r[index], nl_T, nl_T_test, nl_m, nl_n, nl_L, nl_V, nl_V_t)\n",
        "#     sys_model.InitSequence(m1x_0.to(dev), m2x_0.to(dev))\n",
        "#     print(\"GSP - KNet EKF with full model info\")\n",
        "#     modelFolder = 'KNet' + '/'\n",
        "#     gspKNet_Pipeline = Pipeline_EKF(strTime, \"KNet\", \"KalmanNet\")\n",
        "#     gspKNet_Pipeline.setssModel(sys_model)\n",
        "#     gspKNet_model = GSPExtendedKalmanNetNN()\n",
        "#     try:\n",
        "#       checkpoint = torch.load('/content/drive/MyDrive/Project/models/Non Linear H x3 n=10/gspKNet_weights_n10p'+str(r2[index])+'.pth')\n",
        "#       epochs = 2\n",
        "#       print(\"From checkpoint\")\n",
        "#     except:\n",
        "#       checkpoint = 0\n",
        "#       epochs = 40\n",
        "#       gspKNet_model.apply(init_weights)\n",
        "#       print(\"Training from zero\")\n",
        "#     gspKNet_model.Build(sys_model)\n",
        "#     gspKNet_Pipeline.setModel(gspKNet_model,checkpoint)\n",
        "#     gspKNet_Pipeline.setTrainingParams(n_Epochs=epochs, n_Batch=100, learningRate=1e-3, weightDecay=0)\n",
        "#     if epochs != 2:\n",
        "#       gspKNet_Pipeline.NNTrain(N_E, nl_train_input.float(), nl_train_target.float(), N_CV, nl_cv_input.float(), nl_cv_target.float())\n",
        "#       print(\"Saving model ----->\")\n",
        "#       torch.save({\n",
        "#               'model_state_dict': gspKNet_model.state_dict(),\n",
        "#               'optimizer_state_dict': gspKNet_Pipeline.optimizer.state_dict()\n",
        "#               }, '/content/drive/MyDrive/Project/models/Non Linear H x3 n=10/gspKNet_weights_n10p'+str(r2[index])+'.pth')\n",
        "\n",
        "#     [KNet_MSE_test_linear_arr, KNet_MSE_test_linear_avg, gspKNet_MSE_test_dB_avg, KNet_test, timming] = gspKNet_Pipeline.NNTest(N_T,\n",
        "#                                                                                                                  nl_test_input,\n",
        "#                                                                                                                  nl_test_target)\n",
        "\n",
        "#     # ####### gsp-EKF ###########\n",
        "#     # mean = torch.mean(nl_test_target, dim=2)\n",
        "#     # # subtract the mean from the data\n",
        "#     # centered_data = nl_test_target - mean.unsqueeze(2)\n",
        "#     # # compute the covariance matrix of the data\n",
        "#     # covariance_matrix = torch.matmul(centered_data, centered_data.transpose(1, 2)) / (centered_data.size(2) - 1)\n",
        "#     # avg_covariance_matrix = torch.mean(covariance_matrix, dim=0)\n",
        "#     # # print(covariance_matrix)\n",
        "#     # # m2x_0 = cov_matrix+covariance_matrix\n",
        "#     # nl_m2x_0 = covariance_matrix[0,:,:]\n",
        "#     # print(\"EKF Calculation - full:\")\n",
        "#     # epsilon = torch.eye(nl_m)*(1E-11)\n",
        "#     # equation = 13\n",
        "#     # # Print(nl_test_input.shape)\n",
        "#     # sys_model = SystemModel(nl_f_EKF, q[index].to(dev), nl_h_EKF, r[index].to(dev), nl_T, nl_T_test, nl_m, nl_n, nl_L.to(dev), nl_V.to(dev), nl_V_t.to(dev))\n",
        "#     # sys_model.InitSequence(nl_m1x_0, nl_m2x_0)\n",
        "\n",
        "#     # [MSE_finalize13, nl13_MSE_EKF_linear_arr, nl13_MSE_EKF_linear_avg, nl13_MSE_EKF_dB_avg, nl13_EKF_KG_array, nl13_EKF_out,\n",
        "#     #  nl13_SNR] = EKFTest(sys_model, nl_test_input, nl_test_target, equation, 'NonLinear')\n",
        "#     # # print(nl13_EKF_KG_array)\n",
        "#     # print(\"EKF calculation - Diagonal:\")\n",
        "#     # ### Diagonal-KG gsp-EKF\n",
        "#     # equation = 20\n",
        "#     # [MSE_finalize20, nl20_MSE_EKF_linear_arr, nl20_MSE_EKF_linear_avg, nl20_MSE_EKF_dB_avg, nl20_EKF_KG_array, nl20_EKF_out,\n",
        "#     #  nl20_SNR] = EKFTest(sys_model, nl_test_input, nl_test_target, equation, 'NonLinear')\n",
        "\n",
        "#     nl_GSP_KNet_MSElist.append(gspKNet_MSE_test_dB_avg)\n",
        "#     # # nl_KNet_MSElist.append(KNet_MSE_test_dB_avg)\n",
        "#     # nl_MSElist.append(nl13_MSE_EKF_dB_avg)\n",
        "#     # nl_MSElist_diag.append(nl20_MSE_EKF_dB_avg)\n",
        "\n",
        "#     # ########## Naive ##########\n",
        "#     # target = nl_test_target  # Shape: (batch_size, height, width)\n",
        "#     # estimation = nl_test_input/3  # Shape: (batch_size, height, width)\n",
        "#     # # print(target.shape)\n",
        "#     # # Calculate MSE\n",
        "#     # # print(\"Naieve MSE \", mse)\n",
        "#     # loss_fn = nn.MSELoss(reduction='mean')\n",
        "#     # MSE_test_linear_arr = torch.zeros([target.shape[0]])\n",
        "#     # # Test mode\n",
        "#     # for j in range(target.shape[0]):# cannot use batch due to different length and std computation\n",
        "#     #   MSE_test_linear_arr[j] = loss_fn(target[j,:,:], estimation[j,:,:]).item()\n",
        "\n",
        "#     # # Average\n",
        "#     # MSE_test_linear_avg = torch.mean(MSE_test_linear_arr)\n",
        "#     # MSE_test_dB_avg = 10 * torch.log10(MSE_test_linear_avg)\n",
        "#     # mse = torch.mean((target - estimation)**2)\n",
        "#     # print(\"Naive MSE \", MSE_test_dB_avg, \" [dB]\")\n",
        "#     # Naive_MSE.append(MSE_test_dB_avg)\n",
        "#     # # plt.figure()\n",
        "\n",
        "#     # plt.plot(range(1,T), MSE_test_linear_arr[1:].cpu().detach().numpy(), label='Knet')\n",
        "#     # # plt.plot(range(2, T+1), KF_MSE_finalize_dB[1:].cpu().detach().numpy(), label='KF')\n",
        "#     # # plt.plot(range(2, T+1), MSE_finalize_gspKnet[1:].cpu().detach().numpy(), label='Knet GSP')\n",
        "#     # # plt.plot(range(2, T+1), MSE_finalize13[1:].cpu().detach().numpy(), label='GSP-EKF-full')\n",
        "#     # # plt.plot(range(2, T+1), MSE_finalize20[1:].cpu().detach().numpy(), label='GSP-EKF-diag')\n",
        "#     # plt.xlabel('Iteration')\n",
        "#     # plt.ylabel('MSE dB')\n",
        "#     # plt.title(' MSE vs Iteration')\n",
        "#     # plt.legend()\n",
        "#     # plt.grid()\n",
        "#     # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WO0m4w5IwM53"
      },
      "outputs": [],
      "source": [
        "# print(observation.shape)\n",
        "# print(observation[1,1:5,(0,1,14,15,65,66,67,68,69)])\n",
        "# print(target[1,1:5,(0,1,14,15,65,66,67,68,69,98,99)])\n",
        "# # print(nl_cv_input[1,:,:])\n",
        "# # print(nl_cv_target[1,:,:])\n",
        "# print(nl13_EKF_KG_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "PuhRpF0gnRSE"
      },
      "outputs": [],
      "source": [
        "# print(\"results for partially known h = x3 with a = 0.5 and f(x)=x+10sin(x/9+3)\")\n",
        "# print(\"gspKNet \",nl_GSP_KNet_MSElist)\n",
        "# print(\"gspEKF full \",nl_MSElist)\n",
        "# print(\"gspEKF diag \",nl_MSElist_diag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "AYswM8-dQBwz"
      },
      "outputs": [],
      "source": [
        "# print(\"results for fully known h = x3 with a = 0.5 and f(x)=x+10sin(x/10+3)\")\n",
        "# print(\"gspKNet \",nl_GSP_KNet_MSElist)\n",
        "# print(\"gspEKF full \",nl_MSElist)\n",
        "# print(\"gspEKF diag \",nl_MSElist_diag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "IjqHM44p6SyI"
      },
      "outputs": [],
      "source": [
        "# Random_walk_KNet_MSElist = torch.load('/content/drive/MyDrive/Project/results/RW n=10/KNet.pth')\n",
        "# Random_walk_GSP_KNet_MSElist = torch.load('/content/drive/MyDrive/Project/results/RW n=10/gspKNet.pth')\n",
        "# nl_MSElist = torch.load('/content/drive/MyDrive/Project/results/NL n=10/gspEKF_full.pth')\n",
        "# Naive_MSE = torch.load('/content/drive/MyDrive/Project/results/RW n=10/Naive.pth')\n",
        "# KF_MSE = torch.load('/content/drive/MyDrive/Project/results/RW n=10/KF.pth')\n",
        "# print(\"results for q2 = \",q2)\n",
        "# print(nl_MSElist)\n",
        "# print(nl_MSElist_diag)\n",
        "# print(Naive_MSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "j9J3drFH9fNb"
      },
      "outputs": [],
      "source": [
        "# torch.save(nl_MSElist, '/content/drive/MyDrive/Project/results/NL n=10/gspEKF_partialL.pth')\n",
        "# torch.save(nl_MSElist_diag, '/content/drive/MyDrive/Project/results/NL n=10/gspEKF_diag_partialL.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "N0sIaG-SrNVj"
      },
      "outputs": [],
      "source": [
        "# print(nl_test_input[1,:,(1,50,60,80,100,150,190)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "kyGcGKIO_89N"
      },
      "outputs": [],
      "source": [
        "# KNet_MSE = [10.6498,12.8069,11.667,11.7245]\n",
        "# GSP_KNet_MSE = [9.664,9.6756,10.1198,10.9619]\n",
        "# EKF_full = [-39.8957,-30.788,-20.2903,-10.1124,-0.0107]\n",
        "# gspEKF_full = [-42.8759,-32.3004,-20.4124,-10.1224,-0.0107]\n",
        "# gspEKF_daig_full =[-40.5532,-30.5267,-20.5252,-10.5463,-1.0915]\n",
        "# Naive = [-39.5502,-29.5339,-19.5439,-9.5569,0.4507]\n",
        "# gspKnet_partial = [-40.006,-30.1806,-20.3006,-10.5164,-1.572]\n",
        "# gspEKF_partial = [-32.5025,-25.6184,-19.9859,-10.114,-0.112]\n",
        "# gspKnet_full = [-39.3152,-30.225,-20.3546,-10.529,-1.9169]\n",
        "# EKF_partial = [-36.1425,-25.558,-19.9688,-10.1145,-0.0113]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "l8GZ4sjsPOzQ"
      },
      "outputs": [],
      "source": [
        "# KNet_MSE = [10.6498,12.8069,11.667,11.7245]\n",
        "gspKnet = [-35.8912,-34.0972,-27.9755,-19.4206,-9.9104]\n",
        "# EKF_full = [-39.8957,-30.788,-20.2903,-10.1124,-0.0107]\n",
        "gspEKF = [-23.9347,-23.8326,-22.9922,-18.6092,-9.9194]\n",
        "gspEKF_diag =[-23.9273,-23.8318,-22.9922,-18.6092,-9.9194]\n",
        "# Naive = [-39.5502,-29.5339,-19.5439,-9.5569,0.4507]\n",
        "gspKnet_partial = [-33.8479, -32.6360, -27.6377, -19.2326,-9.8185]\n",
        "gspEKF_partial = [-17.4519,-17.2144,-16.9914,-15.4116,-9.3228]\n",
        "gspEKF__diag_partial = [-17.2878,-17.1963,-16.9898,-15.4115,-9.3228]\n",
        "# EKF_partial = [-36.1425,-25.558,-19.9688,-10.1145,-0.0113]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "CGmXqIM-OGC1"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# nl_MSElist_diag = [tensor.cpu() for tensor in nl_MSElist_diag]\n",
        "# # Assuming you have a list of MSE values called 'mse_list'\n",
        "# mse_array = np.array(nl_MSElist_diag)\n",
        "\n",
        "# # Calculate dB values\n",
        "# db_array = 10 * np.log10(mse_array)\n",
        "\n",
        "# # Convert the numpy array back to a list (if needed)\n",
        "# db_list = db_array.tolist()\n",
        "\n",
        "# # Print the list of MSE values in dB\n",
        "# print(db_list)\n",
        "# print(nl_MSElist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0Q9Wgu5UIgF8"
      },
      "outputs": [],
      "source": [
        "# gspEKF = [-38.4439,-28.6645,-18.3233]\n",
        "# gspEKF_diag = [-38.4258,-28.6094,-18.2964]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "2hKTCH4imnxc"
      },
      "outputs": [],
      "source": [
        "# # nl_KNet_MSElist = [tensor.cpu().detach() for tensor in nl_KNet_MSElist]\n",
        "# # nl_GSP_KNet_MSElist = [tensor.cpu().detach() for tensor in nl_GSP_KNet_MSElist]\n",
        "# # nl_MSElist = [tensor.cpu().detach() for tensor in nl_MSElist]\n",
        "# # nl_MSElist_diag = [tensor.cpu() for tensor in nl_MSElist_diag]\n",
        "# # print(nl_KNet_MSElist)\n",
        "# r2 = torch.tensor([0.001,0.01,0.1,1,10])\n",
        "# # print(len(nl_KNet_MSElist))\n",
        "# plt.figure()\n",
        "# line1, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gspEKF, color='red',  linewidth=1.6,\n",
        "#                   marker='*', markerfacecolor='red', markersize=3, label=\"GSP-EKF\")\n",
        "# line3, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gspEKF_diag, color='green',  linewidth=1,\n",
        "#              marker='*', markerfacecolor='green', markersize=3, label=\"GSP-EKF diag\")\n",
        "# line2, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gspKnet, color='blue', linewidth=1,\n",
        "#                   marker='o', markerfacecolor='blue', markersize=3, label=\"GSP-KNet\")\n",
        "# line4, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gspEKF_partial, color='red',  linestyle='dashed',linewidth=1.6,\n",
        "#                   marker='*', markerfacecolor='red', markersize=3, label=\"partial GSP-EKF\")\n",
        "# line6, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gspEKF__diag_partial, color='green',   linestyle='dashed',linewidth=1,\n",
        "#              marker='*', markerfacecolor='green', markersize=3, label=\"partial GSP-EKF diag\")\n",
        "# line5, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gspKnet_partial, color='blue',  linestyle='dashed',linewidth=1,\n",
        "#                   marker='o', markerfacecolor='blue', markersize=3, label=\"partial GSP-KNet\")\n",
        "\n",
        "# plt.legend(loc='upper right')\n",
        "# plt.xlabel('1/r^2[dB]')\n",
        "# plt.ylabel('MSE[dB]')\n",
        "# plt.grid()\n",
        "# plt.title(\"f(x)=x+10sin(x/10+3)    h~(x)=0.5~x+0.5~x^3\")\n",
        "# plt.show()\n",
        "\n",
        "# # print(nl_GSP_KNet_MSElist.detach())\n",
        "# # print(nl_KNet_MSElist.detach())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "yhJPefhzaIAR"
      },
      "outputs": [],
      "source": [
        "# for index in range(len(r)):\n",
        "#   # try:\n",
        "#   tens = torch.load('/content/gspKNet_weights2_' + str(r2[index]) + '.pth')\n",
        "#   torch.save(tens,'./drive/MyDrive/Project/data')\n",
        "#   # except:\n",
        "#     # print(\"no such num\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "r5MomZ7razKf"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "\n",
        "# # Generate sample 3D tensors\n",
        "# target = nl_test_target  # Shape: (batch_size, height, width)\n",
        "# estimation = nl_test_input/3  # Shape: (batch_size, height, width)\n",
        "# # Calculate MSE\n",
        "# mse = torch.mean((target - estimation)**2)\n",
        "\n",
        "# print(mse.item())  # Print MSE value as a scalar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "k4Z_25at6uWz"
      },
      "outputs": [],
      "source": [
        "# EKF_full = [-39.8957,-30.788,-20.2903,-10.1124,-0.0107]\n",
        "# gspEKF_full = [-42.8759,-32.3004,-20.4124,-10.1224,-0.0107]\n",
        "# gspEKF_daig_full =[-40.5532,-30.5267,-20.5252,-10.5463,-1.0915]\n",
        "# Naive = [-39.5502,-29.5339,-19.5439,-9.5569,0.4507]\n",
        "# gspKnet_partial = [-40.006,-30.1806,-20.3006,-10.5164,-1.572]\n",
        "# gspEKF_partial = [-32.5025,-25.6184,-19.9859,-10.114,-0.112]\n",
        "# gspKnet_full = [-39.3152,-30.225,-20.3546,-10.529,-1.9169]\n",
        "# EKF_partial = [-36.1425,-25.558,-19.9688,-10.1145,-0.0113]\n",
        "\n",
        "# r2 = torch.tensor([0.001,0.01,0.1,1.0,10.0])\n",
        "# print(len(nl_KNet_MSElist))\n",
        "# plt.figure()\n",
        "# line1, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gspEKF_daig_full, color='red', linewidth=1,\n",
        "#                   marker='*', markerfacecolor='red', markersize=3, label=\"GSP-EKF\")\n",
        "# line2, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gspKnet_full, color='blue', linewidth=1,\n",
        "#                   marker='o', markerfacecolor='blue', markersize=3, label=\"GSP-KNet\")\n",
        "# line3, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), EKF_full, color='green', linewidth=1,\n",
        "#              marker='*', markerfacecolor='green', markersize=3, label=\"EKF\")\n",
        "# line4, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), Naive, color='yellow', linewidth=1,\n",
        "#              marker='o', markerfacecolor='yellow', markersize=3, label=\"Naive approach\")\n",
        "# line5, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gspEKF_partial, color='red', linestyle='dashed', linewidth=1,\n",
        "#                   marker='*', markerfacecolor='red', markersize=3, label=\"GSP-EKF partial\")\n",
        "# line6, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gspKnet_partial, color='blue', linestyle='dashed', linewidth=1,\n",
        "#                   marker='o', markerfacecolor='blue', markersize=3, label=\"GSP-KNet partial\")\n",
        "# line7, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), EKF_partial, color='green', linestyle='dashed', linewidth=1,\n",
        "#              marker='*', markerfacecolor='green', markersize=3, label=\"EKF partial\")\n",
        "\n",
        "# leg = plt.legend(loc='upper right')\n",
        "# plt.xlabel('1/r^2[dB]')\n",
        "# plt.ylabel('MSE[dB]')\n",
        "# plt.grid()\n",
        "# plt.title('MSE vs SNR - Non Linear n=30')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "mYfF2fuxxgGt"
      },
      "outputs": [],
      "source": [
        "# # for n=30\n",
        "# EKF_time = 577\n",
        "# GSP_EKF_time = 353\n",
        "# GSP_KalmanNet = 3.9\n",
        "\n",
        "# # for n=10\n",
        "# EKF_time = 577\n",
        "# GSP_EKF_time = 353\n",
        "# GSP_KalmanNet = 3.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "HROoXohE0biZ"
      },
      "outputs": [],
      "source": [
        "# print(\"For n=30:\")\n",
        "# print(\"EKF: \",EKF_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWRUZUeAQIIg"
      },
      "source": [
        "# Power Grid Simulation - Data and Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "nadf4dpaQPtY"
      },
      "outputs": [],
      "source": [
        "L14 = [\n",
        "    [19.4980702055144, -15.2630865231796, 0, 0, -4.23498368233483, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [-15.2630865231796, 30.3547153987791, -4.78186315175772, -5.11583832587208, -5.19392739796971, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, -4.78186315175772, 9.85068012935164, -5.06881697759392, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, -5.11583832587208, -5.06881697759392, 38.3431317384716, -21.5785539816916, 0, -4.78194338179036, 0, -1.79797907152361, 0, 0, 0, 0, 0],\n",
        "    [-4.23498368233483, -5.19392739796971, 0, -21.5785539816916, 34.9754041144523, -3.96793905245615, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, -3.96793905245615, 17.3407328099191, 0, 0, 0, 0, -4.09407434424044, -3.17596396502940, -6.10275544819312, 0],\n",
        "    [0, 0, 0, -4.78194338179036, 0, 0, 19.5490059482647, -5.67697984672154, -9.09008271975275, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, -5.67697984672154, 5.67697984672154, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, -1.79797907152361, 0, 0, -9.09008271975275, 0, 24.2825063752679, -10.3653941270609, 0, 0, 0, -3.02905045693060],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, -10.3653941270609, 14.7683378765214, -4.40294374946052, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, -4.09407434424044, 0, 0, 0, -4.40294374946052, 8.49701809370096, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, -3.17596396502940, 0, 0, 0, 0, 0, 5.42793859120161, -2.25197462617221, 0],\n",
        "    [0, 0, 0, 0, 0, -6.10275544819312, 0, 0, 0, 0, 0, -2.25197462617221, 10.6696935494707, -2.31496347510535],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, -3.02905045693060, 0, 0, 0, -2.31496347510535, 5.34401393203596]\n",
        "]\n",
        "\n",
        "PG_L14 = torch.tensor(L14)\n",
        "\n",
        "const = 15\n",
        "\n",
        "patrial_L14 = [\n",
        "    [19.4980702055144 -const, -15.2630865231796 +const, 0, 0, -4.23498368233483, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [-15.2630865231796 +const, 30.3547153987791 -const, -4.78186315175772, -5.11583832587208, -5.19392739796971, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, -4.78186315175772, 9.85068012935164-5, -5.06881697759392+5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, -5.11583832587208, -5.06881697759392+5, 38.3431317384716-5, -21.5785539816916, 0, -4.78194338179036, 0, -1.79797907152361, 0, 0, 0, 0, 0],\n",
        "    [-4.23498368233483, -5.19392739796971, 0, -21.5785539816916, 34.9754041144523, -3.96793905245615, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, -3.96793905245615, 17.3407328099191, 0, 0, 0, 0, -4.09407434424044, -3.17596396502940, -6.10275544819312, 0],\n",
        "    [0, 0, 0, -4.78194338179036, 0, 0, 19.5490059482647, -5.67697984672154, -9.09008271975275, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, -5.67697984672154, 5.67697984672154, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, -1.79797907152361, 0, 0, -9.09008271975275, 0, 24.2825063752679, -10.3653941270609, 0, 0, 0, -3.02905045693060],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, -10.3653941270609, 14.7683378765214, -4.40294374946052, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, -4.09407434424044, 0, 0, 0, -4.40294374946052, 8.49701809370096, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, -3.17596396502940, 0, 0, 0, 0, 0, 5.42793859120161, -2.25197462617221, 0],\n",
        "    [0, 0, 0, 0, 0, -6.10275544819312, 0, 0, 0, 0, 0, -2.25197462617221, 10.6696935494707-2, -2.31496347510535+2],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, -3.02905045693060, 0, 0, 0, -2.31496347510535+2, 5.34401393203596-2]\n",
        "]\n",
        "\n",
        "partial_PG_L14 = torch.tensor(patrial_L14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "lomLs2JkSkRw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b59d5c8-2ca9-4c7d-e266-08c319b08500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "buses must appear in order by bus number\n"
          ]
        }
      ],
      "source": [
        "# vdB = -20\n",
        "# v = 10 ** (vdB / 10)\n",
        "# r2 = 0.001 #torch.tensor([0.001])\n",
        "# q2 = r2*v # torch.mul(v, r2)\n",
        "# Data generation\n",
        "# x=Hy+w, w∼N(μ,σ2I)\n",
        "# y - target signal\n",
        "# x - observed measurements\n",
        "\n",
        "def get_nl_model():\n",
        "  ppc = case14()\n",
        "  branch = ppc[\"branch\"]\n",
        "  nl = ppc[\"branch\"].shape[0]\n",
        "  f = branch[:, F_BUS] - np.ones(nl)\n",
        "  t = branch[:, T_BUS] - np.ones(nl)\n",
        "  branch[:, F_BUS] = f\n",
        "  branch[:, T_BUS] = t\n",
        "  ppc[\"branch\"] = branch\n",
        "  Ybus, Yf, Yt = makeYbus(ppc[\"baseMVA\"], ppc[\"bus\"], ppc[\"branch\"])\n",
        "  return Ybus\n",
        "\n",
        "# simulation setup parameters\n",
        "Ybus = get_nl_model()\n",
        "Ybus = Ybus.todense()\n",
        "Ybus = torch.tensor(Ybus)\n",
        "# print(Ybus)\n",
        "\n",
        "###### Partial: ########\n",
        "Ybus_part = Ybus\n",
        "mis_Ybus = 1.8+1.8j\n",
        "Ybus_part[0,0] -= mis_Ybus\n",
        "Ybus_part[1,0] += mis_Ybus\n",
        "Ybus_part[0,1] += mis_Ybus\n",
        "Ybus_part[1,1] -= mis_Ybus\n",
        "########################\n",
        "\n",
        "# print(Ybus_part)\n",
        "# print(L)\n",
        "# print(L.shape)\n",
        "nl_n_x = np.shape(Ybus)[0]\n",
        "nl_n_y = np.shape(Ybus)[0]\n",
        "n_x = nl_n_x\n",
        "n_y = nl_n_y\n",
        "P = 100\n",
        "\n",
        "pypower_T = P\n",
        "pypower_T_test = pypower_T\n",
        "\n",
        "# nl_sigma_w = 0.05\n",
        "# nl_num_iterations = 400\n",
        "# rezulotion_snr = 10\n",
        "# rezulotion_P = 10\n",
        "# nl_min_power_sigma = -2\n",
        "# nl_max_power_sigma = -1\n",
        "# lower_set_size = 2.1\n",
        "# higher_set_size = 4\n",
        "# H = -Ybus.imag[:, 1:]\n",
        "# prior_mu_y = np.zeros(nl_n_y).reshape(nl_n_y, 1)\n",
        "\n",
        "L = data = [[19.4980702055144, -15.2630865231796, 0, 0, -4.23498368233483, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [-15.2630865231796, 30.3547153987791, -4.78186315175772, -5.11583832587208, -5.19392739796971, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, -4.78186315175772, 9.85068012935164, -5.06881697759392, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, -5.11583832587208, -5.06881697759392, 38.3431317384716, -21.5785539816916, 0, -4.78194338179036, 0, -1.79797907152361, 0, 0, 0, 0, 0],\n",
        "    [-4.23498368233483, -5.19392739796971, 0, -21.5785539816916, 34.9754041144523, -3.96793905245615, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, -3.96793905245615, 17.3407328099191, 0, 0, 0, 0, -4.09407434424044, -3.17596396502940, -6.10275544819312, 0],\n",
        "    [0, 0, 0, -4.78194338179036, 0, 0, 19.5490059482647, -5.67697984672154, -9.09008271975275, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, 0, -5.67697984672154, 5.67697984672154, 0, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 0, -1.79797907152361, 0, 0, -9.09008271975275, 0, 24.2825063752679, -10.3653941270609, 0, 0, 0, -3.02905045693060],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, -10.3653941270609, 14.7683378765214, -4.40294374946052, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, -4.09407434424044, 0, 0, 0, -4.40294374946052, 8.49701809370096, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 0, -3.17596396502940, 0, 0, 0, 0, 0, 5.42793859120161, -2.25197462617221, 0],\n",
        "    [0, 0, 0, 0, 0, -6.10275544819312, 0, 0, 0, 0, 0, -2.25197462617221, 10.6696935494707, -2.31496347510535],\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, -3.02905045693060, 0, 0, 0, -2.31496347510535, 5.34401393203596]]\n",
        "\n",
        "# sigma_w = 0.0003\n",
        "# num_iterations = 3\n",
        "\n",
        "################### Full L #############################\n",
        "# pypower_L = torch.tensor(L)\n",
        "# L = torch.tensor(L)\n",
        "# prior_C_yy = -np.linalg.inv(Ybus.imag[1:, 1:].cpu())\n",
        "################### Partial L ##########################\n",
        "Ybus = Ybus_part\n",
        "pypower_L = partial_PG_L14\n",
        "L = partial_PG_L14\n",
        "prior_C_yy = -np.linalg.inv(Ybus_part.imag[1:, 1:].cpu())\n",
        "\n",
        "U, S, Vh = np.linalg.svd(prior_C_yy, full_matrices=True)\n",
        "S = np.diag(S)\n",
        "sqrt_R_y = np.matmul(np.matmul(U, np.sqrt(S)), Vh)\n",
        "\n",
        "pypower_m = 14\n",
        "pypower_n = 14\n",
        "\n",
        "W, V = np.linalg.eig(pypower_L.cpu())\n",
        "pypower_V = torch.from_numpy(V).type(torch.FloatTensor)\n",
        "pypower_V_t = torch.transpose(pypower_V, 0, 1)\n",
        "\n",
        "# Get adjacency matrix A from Laplacian matrix L\n",
        "A1 = (torch.diag_embed(torch.diag(L)) - L).cuda().clone().detach().requires_grad_(True)\n",
        "A = torch.diag(L)\n",
        "A = (A - L).cuda().clone().detach().requires_grad_(True)\n",
        "\n",
        "def pypower_f_EKF(x):\n",
        "    x = x.cuda().clone().detach().requires_grad_(True)\n",
        "    temp = torch.matmul(A1, x)/30\n",
        "    return 1 - torch.mul(temp, temp)/2\n",
        "    # return x\n",
        "\n",
        "def pypower_h_EKF(y):\n",
        "    v = torch.exp(1j * y)\n",
        "    # Ybus_conj_v = torch.matmul(torch.conj(torch.transpose(Ybus_part, 0, 1)).to(torch.complex64), torch.conj(v).to(torch.complex64))\n",
        "    Ybus_conj_v = torch.matmul(torch.conj(torch.transpose(Ybus, 0, 1)).to(torch.complex64), torch.conj(v).to(torch.complex64))\n",
        "    s = torch.mul(v, Ybus_conj_v)\n",
        "    p = torch.real(s)\n",
        "    return p\n",
        "\n",
        "def pypower_f(x):\n",
        "    B = x.size()[0]\n",
        "    if len(x.shape) != 3:\n",
        "      x = x.unsqueeze(-1)\n",
        "    temp = (torch.bmm(A1.expand(B, -1, -1).type(torch.DoubleTensor).to(dev), x.type(torch.DoubleTensor).to(dev)).squeeze(-1))/30\n",
        "    return 1 - torch.mul(temp, temp)/2\n",
        "\n",
        "def pypower_h(y):\n",
        "    B = y.size()[0]\n",
        "    theta = y\n",
        "    v = torch.exp(1j * torch.tensor(theta, dtype=torch.float)).cuda().clone().detach().requires_grad_(True)\n",
        "    trans = torch.transpose(Ybus, 1, 0)\n",
        "    # trans = torch.transpose(Ybus_part, 1, 0)\n",
        "    Ybus_conj_v = torch.bmm(torch.conj(trans).expand(B,-1,-1).float(), torch.conj(v).unsqueeze(-1).float()).clone().detach().requires_grad_(True)\n",
        "    s = torch.mul(v.squeeze(), Ybus_conj_v.squeeze())\n",
        "    p = torch.real(s).clone().detach()\n",
        "    return p\n",
        "\n",
        "def pypower_getJacobian(x, a):\n",
        "    # if(x.size()[1] == 1):\n",
        "    #     y = torch.reshape((x.T),[x.size()[0]])\n",
        "    try:\n",
        "        if (x.size()[1] == 1):\n",
        "            y = torch.reshape((x.T), [x.size()[0]])\n",
        "    except:\n",
        "        y = torch.reshape((x.T), [x.size()[0]])\n",
        "    if (a == 'ObsAcc'):\n",
        "        g = pypower_h_EKF\n",
        "    elif (a == 'ModAcc'):\n",
        "        g = pypower_f_EKF\n",
        "    if torch.isnan(x).any():\n",
        "      print(\"Variable F contains NaN values. \", g)\n",
        "    Jac = autograd.functional.jacobian(g, y)\n",
        "    if torch.isnan(Jac).any():\n",
        "      print(x)\n",
        "      print(\"Jac. \", g)\n",
        "    Jac = Jac.view(-1, pypower_m)\n",
        "    return Jac"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgguNTR1nLz3"
      },
      "source": [
        "# Main Pypower"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIPaUhpsnPij"
      },
      "outputs": [],
      "source": [
        "T = 200\n",
        "T_valid = T\n",
        "T_test = T\n",
        "nl_T = T\n",
        "nl_T_test = nl_T\n",
        "nl_m = pypower_m\n",
        "nl_n = pypower_n\n",
        "# torch.autograd.set_detect_anomaly(False)\n",
        "PFandUKF_test = False\n",
        "if torch.cuda.is_available() and not PFandUKF_test:\n",
        "    dev = torch.device(\"cuda:0\")\n",
        "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    dev = torch.device(\"cpu\")\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    print(\"Running on the CPU\")\n",
        "print(\"Pipeline Start\")\n",
        "\n",
        "################\n",
        "### Get Time ###\n",
        "################\n",
        "\n",
        "today = datetime.today()\n",
        "now = datetime.now()\n",
        "strToday = today.strftime(\"%m.%d.%y\")\n",
        "strNow = now.strftime(\"%H:%M:%S\")\n",
        "strTime = strToday + \"_\" + strNow\n",
        "print(\"Current Time =\", strTime)\n",
        "\n",
        "N_E = 1800 # train\n",
        "N_CV = 100 # validation\n",
        "N_T = 100  # test\n",
        "\n",
        "# All together\n",
        "r2_list = [0.001, 0.01, 0.1, 1, 10]\n",
        "r2 = torch.tensor([0.0010,0.0100,0.1000,1.0000,10.])\n",
        "r = torch.sqrt(r2)\n",
        "vdB = -20\n",
        "# ratio v=q2/r2\n",
        "v = 10 ** (vdB / 10)\n",
        "q2 = torch.mul(v, r2)\n",
        "q = torch.sqrt(q2)\n",
        "EKF_result = torch.empty([len(r2), nl_m, T_test])\n",
        "epsilon = torch.eye(nl_n)*(1E-16)\n",
        "\n",
        "nl_GSP_KNet_MSElist = []\n",
        "nl_KNet_MSElist = []\n",
        "nl_MSElist = []\n",
        "nl_MSElist_diag = []\n",
        "SNR_RW_13 = []\n",
        "SNR_RW_20_diag = []\n",
        "Naive_MSE = []\n",
        "\n",
        "kalmanNet = True\n",
        "GSP_KalmanNet = False\n",
        "GSP_EKF = False\n",
        "EKF = False\n",
        "\n",
        "for index in range(3,len(r2)-1):\n",
        "\n",
        "    print(\"1/r2 [dB]: \", 10 * torch.log10(1 / r[index] ** 2), r2[index])\n",
        "    print(\"1/q2 [dB]: \", 10 * torch.log10(1 / q[index] ** 2))\n",
        "\n",
        "    target = torch.load('/content/drive/MyDrive/Project/data/pypower14f/test_target_r2_'+ str(r2[index]) + '.pt').to(dev)\n",
        "    print(target.shape)\n",
        "    observation = torch.load('/content/drive/MyDrive/Project/data/pypower14f/test_observation_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "    target = target[:N_T,:,:T_test].float()\n",
        "    observation = observation[:N_T,:,:T_test].float()\n",
        "\n",
        "    nl_train_input = torch.load('/content/drive/MyDrive/Project/data/pypower14f/train_observation_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "    nl_cv_input = torch.load('/content/drive/MyDrive/Project/data/pypower14f/valid_observation_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "    nl_test_input = observation\n",
        "    nl_train_target = torch.load('/content/drive/MyDrive/Project/data/pypower14f/train_target_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "    nl_cv_target = torch.load('/content/drive/MyDrive/Project/data/pypower14f/valid_target_r2_' + str(r2[index]) + '.pt').to(dev)\n",
        "    nl_test_target = target\n",
        "    print(nl_train_target.shape)\n",
        "    nl_train_input = nl_train_input[:,:,:T].float()\n",
        "    nl_cv_input = nl_cv_input[:N_CV,:,:T_valid].float()\n",
        "    nl_test_input = nl_test_input[:,:,:T].float()\n",
        "    nl_train_target = nl_train_target[:,:,:T].float()\n",
        "    nl_cv_target = nl_cv_target[:N_CV,:,:T_valid].float()\n",
        "    nl_test_target = nl_test_target[:,:,:T].float()\n",
        "\n",
        "    ### Pypower\n",
        "    m1x_0 = target[0, :, 0].to(dev)\n",
        "    m2x_0 = 0 * 0 * torch.zeros(nl_m, nl_m).to(dev)\n",
        "\n",
        "    ### KalmanNet\n",
        "    if kalmanNet:\n",
        "        sys_model = SystemModel(pypower_f, q[index], pypower_h, r[index], nl_T, nl_T_test, nl_m, nl_n, pypower_L, pypower_V, pypower_V_t)\n",
        "        sys_model.InitSequence(m1x_0, m2x_0)\n",
        "        print(\"KNet EKF with full model info\")\n",
        "        modelFolder = 'KNet' + '/'\n",
        "        KNet_Pipeline = Pipeline_EKF(strTime, \"KNet\", \"KalmanNet\")\n",
        "        KNet_Pipeline.setssModel(sys_model)\n",
        "        KNet_model = ExtendedKalmanNetNN()\n",
        "        try:\n",
        "          checkpoint = torch.load('/content/drive/MyDrive/Project/models/pypower14/KNet_weights_partial'+str(r2[index])+'.pth')\n",
        "          epochs = 30\n",
        "        except:\n",
        "          checkpoint = 0\n",
        "          epochs = 100\n",
        "          print(\"Training from zero\")\n",
        "          # KNet_model.apply(init_weights)\n",
        "        KNet_model.Build(sys_model)\n",
        "        KNet_model.apply(init_weights)\n",
        "        KNet_Pipeline.setModel(KNet_model,checkpoint)\n",
        "        KNet_Pipeline.setTrainingParams(n_Epochs=epochs, n_Batch=35 , learningRate=1e-5, weightDecay=0)\n",
        "        if epochs != 2:\n",
        "          KNet_Pipeline.NNTrain(N_E, nl_train_input, nl_train_target, N_CV, nl_cv_input, nl_cv_target)\n",
        "          print(\"Saving model ----->\")\n",
        "          torch.save({\n",
        "                  'model_state_dict': KNet_model.state_dict(),\n",
        "                  'optimizer_state_dict': KNet_Pipeline.optimizer.state_dict()\n",
        "                  }, '/content/drive/MyDrive/Project/models/pypower14/KNet_weights_partial'+str(r2[index])+'.pth')\n",
        "        [KNet_MSE_test_linear_arr, KNet_MSE_test_linear_avg, KNet_MSE_test_dB_avg, KNet_test, timming] = KNet_Pipeline.NNTest(N_T,\n",
        "                                                                                                                nl_test_input,\n",
        "                                                                                                                  nl_test_target)\n",
        "        nl_KNet_MSElist.append(KNet_MSE_test_dB_avg)\n",
        "\n",
        "    ######## GSP-KalmanNet ########\n",
        "    if GSP_KalmanNet:\n",
        "        sys_model = SystemModel(pypower_f, q[index], pypower_h, r[index], nl_T, nl_T_test, nl_m, nl_n, pypower_L, pypower_V, pypower_V_t)\n",
        "        sys_model.InitSequence(m1x_0.to(dev), m2x_0.to(dev))\n",
        "        print(\"GSP - KNet EKF with full model info\")\n",
        "        modelFolder = 'KNet' + '/'\n",
        "        gspKNet_Pipeline = Pipeline_EKF(strTime, \"GSP-KalmanNet\", \"GSP-KalmanNet\")\n",
        "        gspKNet_Pipeline.setssModel(sys_model)\n",
        "        gspKNet_model = GSPExtendedKalmanNetNN()\n",
        "        try:\n",
        "          checkpoint = torch.load('/content/drive/MyDrive/Project/models/pypower14/gspKNet_weights_f'+str(r2[index])+'.pth')\n",
        "          epochs = 60\n",
        "          print(\"from checkpoint\")\n",
        "        except:\n",
        "          checkpoint = 0\n",
        "          epochs = 60\n",
        "          gspKNet_model.apply(init_weights)\n",
        "          print(\"Training from zero\")\n",
        "        gspKNet_model.Build(sys_model)\n",
        "        gspKNet_Pipeline.setModel(gspKNet_model,checkpoint)\n",
        "        gspKNet_Pipeline.setTrainingParams(n_Epochs=epochs, n_Batch=40, learningRate=1e-5, weightDecay=0)\n",
        "        if epochs != 2:\n",
        "          gspKNet_Pipeline.NNTrain(N_E, nl_train_input.float(), nl_train_target.float(), N_CV, nl_cv_input.float(), nl_cv_target.float())\n",
        "          print(\"Saving model ----->\")\n",
        "          torch.save({\n",
        "                  'model_state_dict': gspKNet_model.state_dict(),\n",
        "                  'optimizer_state_dict': gspKNet_Pipeline.optimizer.state_dict()\n",
        "                  }, '/content/drive/MyDrive/Project/models/pypower14/gspKNet_weights_f'+str(r2[index])+'.pth')\n",
        "\n",
        "        [KNet_MSE_test_linear_arr, KNet_MSE_test_linear_avg, gspKNet_MSE_test_dB_avg, KNet_test, timming] = gspKNet_Pipeline.NNTest(N_T,\n",
        "                                                                                                                     nl_test_input,\n",
        "                                                                                                                     nl_test_target)\n",
        "        nl_GSP_KNet_MSElist.append(gspKNet_MSE_test_dB_avg)\n",
        "\n",
        "\n",
        "    ######### gsp-EKF ###########\n",
        "    if GSP_EKF:\n",
        "        mean = torch.mean(nl_test_target, dim=2)\n",
        "        # subtract the mean from the data\n",
        "        centered_data = nl_test_target - mean.unsqueeze(2)\n",
        "        # compute the covariance matrix of the data\n",
        "        covariance_matrix = torch.matmul(centered_data, centered_data.transpose(1, 2)) / (centered_data.size(2) - 1)\n",
        "        avg_covariance_matrix = torch.mean(covariance_matrix, dim=0)\n",
        "        # m2x_0 = cov_matrix+covariance_matrix\n",
        "        m2x_0 = covariance_matrix[0,:,:]\n",
        "        # m2x_0 = 0 * 0 * torch.zeros(nl_m, nl_m).to(dev)\n",
        "        epsilon = torch.eye(nl_m)*(1E-13)\n",
        "        equation = 13\n",
        "        # Print(nl_test_input.shape)\n",
        "        if torch.isnan(m2x_0).any():\n",
        "            print(\"############## Variable F contains NaN values.\")\n",
        "\n",
        "        sys_model = SystemModel(pypower_f_EKF, q[index].to(dev), pypower_h_EKF, r[index].to(dev), nl_T, nl_T_test, nl_m, nl_n, pypower_L.to(dev), pypower_V.to(dev), pypower_V_t.to(dev))\n",
        "        sys_model.InitSequence(m1x_0, m2x_0)\n",
        "\n",
        "        [MSE_finalize13, nl13_MSE_EKF_linear_arr, nl13_MSE_EKF_linear_avg, nl13_MSE_EKF_dB_avg, nl13_EKF_KG_array, nl13_EKF_out,\n",
        "        nl13_SNR] = EKFTest(sys_model, nl_test_input, nl_test_target, equation, 'NonLinear')\n",
        "        if torch.isnan(nl13_EKF_KG_array).any() or torch.isnan(nl13_MSE_EKF_linear_arr).any():\n",
        "            print(\"Output KG or MSE array contains NaN values!!!!!!!\")\n",
        "\n",
        "        # Diagonal-KG gsp-EKF\n",
        "        equation = 20\n",
        "        [MSE_finalize20, nl20_MSE_EKF_linear_arr, nl20_MSE_EKF_linear_avg, nl20_MSE_EKF_dB_avg, nl20_EKF_KG_array, nl20_EKF_out,\n",
        "        nl20_SNR] = EKFTest(sys_model, nl_test_input, nl_test_target, equation, 'NonLinear')\n",
        "        if torch.isnan(nl20_EKF_KG_array).any() or torch.isnan(nl20_MSE_EKF_linear_arr).any():\n",
        "            print(\"Output KG or MSE array contains NaN values!!!!!!!\")\n",
        "\n",
        "        nl_MSElist.append(nl13_MSE_EKF_dB_avg)\n",
        "        nl_MSElist_diag.append(nl20_MSE_EKF_dB_avg)\n",
        "\n",
        "    ######## EKF ###########\n",
        "    if EKF:\n",
        "        mean = torch.mean(nl_test_target, dim=2)\n",
        "        centered_data = nl_test_target - mean.unsqueeze(2)\n",
        "        covariance_matrix = torch.matmul(centered_data, centered_data.transpose(1, 2)) / (centered_data.size(2) - 1)\n",
        "        avg_covariance_matrix = torch.mean(covariance_matrix, dim=0)\n",
        "        # m2x_0 = cov_matrix+covariance_matrix\n",
        "        m2x_0 = covariance_matrix[0,:,:]\n",
        "        # m2x_0 = 0 * 0 * torch.zeros(nl_m, nl_m).to(dev)\n",
        "        # print(\"EKF Calculation\",m2x_0)\n",
        "        epsilon = torch.eye(nl_m)*(1E-13)\n",
        "        equation = 13\n",
        "        I = torch.eye(nl_m)\n",
        "        if torch.isnan(m2x_0).any():\n",
        "            print(\"############## Variable F contains NaN values.\")\n",
        "\n",
        "        sys_model = SystemModel(pypower_f_EKF, q[index].to(dev), pypower_h_EKF, r[index].to(dev), nl_T, nl_T_test, nl_m, nl_n, I.to(dev), I.to(dev), I.to(dev))\n",
        "        sys_model.InitSequence(m1x_0, m2x_0)\n",
        "\n",
        "        [MSE_finalize13, nl13_MSE_EKF_linear_arr, nl13_MSE_EKF_linear_avg, nl13_MSE_EKF_dB_avg, nl13_EKF_KG_array, nl13_EKF_out,\n",
        "        nl13_SNR] = EKFTest(sys_model, nl_test_input, nl_test_target, equation, 'NonLinear')\n",
        "        if torch.isnan(nl13_EKF_KG_array).any() or torch.isnan(nl13_MSE_EKF_linear_arr).any():\n",
        "            print(\"Output KG or MSE array contains NaN values!!!!!!!\")\n",
        "\n",
        "    # # Average\n",
        "    # MSE_test_linear_avg = torch.mean(MSE_test_linear_arr)\n",
        "    # MSE_test_dB_avg = 10 * torch.log10(MSE_test_linear_avg)\n",
        "    # mse = torch.mean((target - estimation)**2)\n",
        "    # print(\"Naive MSE \", MSE_test_dB_avg, \" [dB]\")\n",
        "    # Naive_MSE.append(MSE_test_dB_avg)\n",
        "    # # plt.figure()\n",
        "\n",
        "    # plt.plot(range(1,T), MSE_test_linear_arr[1:].cpu().detach().numpy(), label='Knet')\n",
        "    # # plt.plot(range(2, T+1), KF_MSE_finalize_dB[1:].cpu().detach().numpy(), label='KF')\n",
        "    # # plt.plot(range(2, T+1), MSE_finalize_gspKnet[1:].cpu().detach().numpy(), label='Knet GSP')\n",
        "    # # plt.plot(range(2, T+1), MSE_finalize13[1:].cpu().detach().numpy(), label='GSP-EKF-full')\n",
        "    # # plt.plot(range(2, T+1), MSE_finalize20[1:].cpu().detach().numpy(), label='GSP-EKF-diag')\n",
        "    # plt.xlabel('Iteration')\n",
        "    # plt.ylabel('MSE dB')\n",
        "    # plt.title(' MSE vs Iteration')\n",
        "    # plt.legend()\n",
        "    # plt.grid()\n",
        "    # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nl_MSElist_diag)\n",
        "print(nl_MSElist11)"
      ],
      "metadata": {
        "id": "g3ppEcCkQwOU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "29df3f76-de7c-4350-c06c-6f3f54386fed"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-5d066ac35589>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnl_MSElist_diag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnl_MSElist11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'nl_MSElist11' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B_1coUkSCCoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "HjtEWW4okVb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "11222d9c-025b-4e35-b4b1-4ab5977535a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-60-9e4aa01a7c5e>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  line1, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), EKF_pyp, color='red',  linewidth=1,\n",
            "<ipython-input-60-9e4aa01a7c5e>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  line2, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gsp_kalmanNet, color='blue',  linewidth=1,\n",
            "<ipython-input-60-9e4aa01a7c5e>:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  line3, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gspEKF_pyp, color='green',  linewidth=1,\n",
            "<ipython-input-60-9e4aa01a7c5e>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  line84, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), KalmanNet, color='orange',  linewidth=1,\n",
            "<ipython-input-60-9e4aa01a7c5e>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  line4, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gspEKF_partial, color='red', linestyle='dashed', linewidth=1,\n",
            "<ipython-input-60-9e4aa01a7c5e>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  line5, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), KalmanNet_partial, color='orange', linestyle='dashed', linewidth=1,\n",
            "<ipython-input-60-9e4aa01a7c5e>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  line6, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gsp_Knet_partial, color='blue', linestyle='dashed', linewidth=1,\n",
            "<ipython-input-60-9e4aa01a7c5e>:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  line7, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gspEKFdiag_partial, color='green', linestyle='dashed', linewidth=1,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "##########################Full ##########################\n",
        "gspEKF_pyp       = torch.tensor([(-28.0228), (-27.8972), (-26.8039), (-21.7252)]).cpu()\n",
        "EKF_pyp           = torch.tensor([(-28.0723), (-27.9465), (-26.8521), (-21.7763)]).cpu()\n",
        "gsp_kalmanNet = torch.tensor([-37.1011, -35.5429, -29.6012, -20.537]).cpu() #, -11.7079]\n",
        "KalmanNet = torch.tensor([(-37.9458),(-35.0206),(-28.7034),(-21.6987)]).cpu()\n",
        "\n",
        "######################## partial #######################\n",
        "gspEKFdiag_partial = torch.tensor([-23.5967,-23.5515,-23.0950,-20.0477]).cpu()\n",
        "gspEKF_partial = torch.tensor([-23.9520,-23.9035,-23.4389,-20.3371]).cpu()\n",
        "gsp_Knet_partial = torch.tensor([(-32.4118), (-30.7539), (-25.5338), (-20.0453)]).cpu()\n",
        "KalmanNet_partial = torch.tensor([(-31.5370),(-31.0717),(-26.0936),(-21.3560)]).cpu()\n",
        "\n",
        "\n",
        "r2 = torch.tensor([0.001,0.01,0.1,1])\n",
        "\n",
        "plt.figure()\n",
        "########### Full ############\n",
        "line1, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), EKF_pyp, color='red',  linewidth=1,\n",
        "                  marker='*', markerfacecolor='red', markersize=3, label=\"EKF\")\n",
        "line2, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gsp_kalmanNet, color='blue',  linewidth=1,\n",
        "                  marker='o', markerfacecolor='blue', markersize=3, label=\"GSP-KalmanNet\")\n",
        "line3, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gspEKF_pyp, color='green',  linewidth=1,\n",
        "             marker='*', markerfacecolor='green', markersize=3, label=\"GSP-EKF\")\n",
        "line84, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), KalmanNet, color='orange',  linewidth=1,\n",
        "             marker='*', markerfacecolor='green', markersize=3, label=\"KalmanNet\")\n",
        "\n",
        "########## Partial ##########\n",
        "line4, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gspEKF_partial, color='red', linestyle='dashed', linewidth=1,\n",
        "                  marker='*', markerfacecolor='red', markersize=3, label=\"partial EKF\")\n",
        "line5, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), KalmanNet_partial, color='orange', linestyle='dashed', linewidth=1,\n",
        "                  marker='*', markerfacecolor='red', markersize=3, label=\"partial KalmanNet\")\n",
        "line6, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gsp_Knet_partial, color='blue', linestyle='dashed', linewidth=1,\n",
        "                  marker='o', markerfacecolor='blue', markersize=3, label=\"partial GSP-KakmanNet\")\n",
        "line7, = plt.plot(10*torch.log10(torch.tensor(1/r2.cpu())), gspEKFdiag_partial, color='green', linestyle='dashed', linewidth=1,\n",
        "             marker='*', markerfacecolor='green', markersize=3, label=\"partial gspEKF\")\n",
        "\n",
        "leg = plt.legend(loc='upper right')\n",
        "plt.xlabel('1/r^2[dB]')\n",
        "plt.ylabel('MSE[dB]')\n",
        "plt.grid()\n",
        "plt.title('Power Grid Case14 - Gaussian noise')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMs9BdMv_nU7"
      },
      "source": [
        "# PyPower Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSOldTJr_rh9"
      },
      "outputs": [],
      "source": [
        "# # # seed = 0\n",
        "# # # torch.manual_seed(seed)\n",
        "# x0 = torch.tensor([0.8, 0.3, 0.45, 0.25, 0.6, 0.1, 0.75, 0.22, 0.12, 0.95,0.1,0.7,0.3,0.5])\n",
        "# l = torch.tensor([0.001, 0.01, 0.1, 1, 10])\n",
        "# r2 = [0.001, 0.01, 0.1, 1, 10]\n",
        "# r2 = l\n",
        "# vdB = -20  # ratio v=q2/r2\n",
        "# v = 10 ** (vdB / 10)\n",
        "# q2 = torch.mul(v, r2)\n",
        "# # q2 = [v*i for i in r2]\n",
        "# nl_T = 200\n",
        "# N_train = 1200\n",
        "# N_valid = 100\n",
        "# N_test = 100\n",
        "\n",
        "# for index in range(len(r2)-3):\n",
        "#     # print('r2 =', r2[index])\n",
        "#     # ## Validation data create\n",
        "#     # valid_X = torch.zeros(N_valid, pypower_n, nl_T)  # Tensor of m-by-T to contain the states thru time.\n",
        "#     # valid_Y = torch.zeros(N_valid, pypower_m, nl_T)  # Tensor of n-by-T to contain the observations thru time.\n",
        "#     # for n in range(N_valid):  # Train\n",
        "#     #     valid_X[n, :, 0] = x0\n",
        "#     #     valid_Y[n, :, 0] = pypower_h_EKF(x0)\n",
        "#     #     for i in range(1, nl_T):  # create another T states in time\n",
        "#     #         valid_X[n, :, i] = pypower_f_EKF(valid_X[n, :, i - 1]) + torch.randn(pypower_n) * q2[index] ** 0.5\n",
        "#     #         valid_Y[n, :, i] = pypower_h_EKF(valid_X[n, :, i])\n",
        "#     # valid_Y = valid_Y + torch.randn(valid_Y.shape) * r2[index] ** 0.5\n",
        "#     # if True in torch.isnan(valid_X):\n",
        "#     #   print('valid_X NANNNNN')\n",
        "#     # else:\n",
        "#     #   print('valid_X is good')\n",
        "#     # if True in torch.isnan(valid_Y):\n",
        "#     #   print('valid_Y NANNNNN')\n",
        "#     # else:\n",
        "#     #   print('valid_Y is good')\n",
        "#     # torch.save(valid_X.type(torch.DoubleTensor), '/content/drive/MyDrive/Project/data/pypower14f/valid_target_r2_' + str(l[index]) + '.pt')\n",
        "#     # torch.save(valid_Y.type(torch.DoubleTensor), '/content/drive/MyDrive/Project/data/pypower14f/valid_observation_r2_' + str(l[index]) + '.pt')\n",
        "#     # print(valid_X[5,:,:])\n",
        "\n",
        "#     ##### Train data create\n",
        "#     train_X = torch.zeros(N_train, pypower_n, nl_T)  # Tensor of m-by-T to contain the states thru time.\n",
        "#     train_Y = torch.zeros(N_train, pypower_m, nl_T)  # Tensor of n-by-T to contain the observations thru time.\n",
        "#     for n in range(N_train):  # Train\n",
        "#         if n % 100 == 0:\n",
        "#           print(\"iter\",n)\n",
        "#         # torch.manual_seed(seed)\n",
        "#         # torch.cuda.manual_seed(seed)\n",
        "#         train_X[n, :, 0] = x0\n",
        "#         train_X = train_X\n",
        "#         train_Y[n, :, 0] = pypower_h_EKF(x0)\n",
        "#         for i in range(1, nl_T):  # create another T states in time\n",
        "#             train_X[n, :, i] = pypower_f_EKF(train_X[n, :, i - 1]) + torch.randn(pypower_n) * q2[index] ** 0.5\n",
        "#             train_Y[n, :, i] = pypower_h_EKF(train_X[n, :, i])\n",
        "#             # torch.manual_seed(seed * i)  # Insert any integer\n",
        "#             # torch.cuda.manual_seed(seed * i)  # Insert any integer\n",
        "#     train_Y = train_Y + torch.randn(train_Y.shape)*r2[index]**0.5\n",
        "\n",
        "#     if True in torch.isnan(train_X):\n",
        "#       print('train_X NANNNNN')\n",
        "#     else:\n",
        "#       print('train_X is good')\n",
        "#     if True in torch.isnan(train_Y):\n",
        "#       print('train_Y NANNNNN')\n",
        "#     else:\n",
        "#       print('train_Y is good')\n",
        "#     torch.save(train_X.type(torch.DoubleTensor), '/content/drive/MyDrive/Project/data/pypower14f/train_target_r2_' + str(l[index]) + '.pt')\n",
        "#     torch.save(train_Y.type(torch.DoubleTensor), '/content/drive/MyDrive/Project/data/pypower14f/train_observation_r2_' + str(l[index]) + '.pt')\n",
        "\n",
        "#     ######### Test data create\n",
        "#     # loc = 0\n",
        "#     # # rand_r = torch.distributions.laplace.Laplace(0, r2[index])\n",
        "#     # # rand_q = torch.distributions.laplace.Laplace(0, q2[index])\n",
        "#     # test_X = torch.zeros(N_test, pypower_n, nl_T)  # Tensor of m-by-T to contain the states thru time.\n",
        "#     # test_Y = torch.zeros(N_test, pypower_m, nl_T)  # Tensor of n-by-T to contain the observations thru time.\n",
        "#     # for n in range(N_test):\n",
        "#     #     test_X[n, :, 0] = x0\n",
        "#     #     test_Y[n, :, 0] = pypower_h_EKF(x0)\n",
        "#     #     for i in range(1, nl_T):  # create another T states in time\n",
        "#     #         # test_X[n, :, i] = pypower_f_EKF(test_X[n, :, i - 1]) + torch.from_numpy(np.random.laplace(loc, q2[index], pypower_n)).cuda()  #torch.randn(pypower_n) * q2[index] ** 0.5\n",
        "#     #         test_X[n, :, i] = pypower_f_EKF(test_X[n, :, i - 1]) + torch.randn(pypower_n) * q2[index] ** 0.5\n",
        "#     #         test_Y[n, :, i] = pypower_h_EKF(test_X[n, :, i])\n",
        "#     # # test_Y[:,:,:] = test_Y[:,:,:] + torch.tensor(np.random.laplace(loc, r2[index], N_test*pypower_n*nl_T)).reshape((N_test, pypower_n, nl_T)) * r2[index] #torch.randn(test_Y[:,:,:].shape) * r2[index] ** 0.5\n",
        "#     # test_Y[:,:,:] = test_Y[:,:,:] + torch.randn(test_Y[:,:,:].shape) * r2[index] ** 0.5\n",
        "#     # torch.save(test_X.type(torch.DoubleTensor), '/content/drive/MyDrive/Project/data/pypower14f/test_target_r2_' + str(l[index]) + '.pt')\n",
        "#     # torch.save(test_Y.type(torch.DoubleTensor), '/content/drive/MyDrive/Project/data/pypower14f/test_observation_r2_' + str(l[index]) + '.pt')\n",
        "#     # if True in torch.isnan(test_X):\n",
        "#     #   print('NANNNNN')\n",
        "#     # else:\n",
        "#     #   print('target is good')\n",
        "#     # if True in torch.isnan(test_Y):\n",
        "#     #   print('NANNNNN')\n",
        "#     # else:\n",
        "#     #   print('target is good')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J07LBfK0VeLE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "S2hQOE5haUrV"
      ],
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "T4",
      "toc_visible": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}